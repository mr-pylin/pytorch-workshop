{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud83d\udcdd **Author:** Amirhossein Heydari - \ud83d\udce7 **Email:** amirhosseinheydari78@gmail.com - \ud83d\udccd **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set a seed for deterministic results\n",
        "random_state = 42\n",
        "torch.manual_seed(random_state)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check if cuda is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regular Dataset\n",
        "   - Regular datasets typically used in CNNs & MLPs are composed of independent data points\n",
        "   - Each data point is usually represented as a fixed-size vector (or tensor for images)\n",
        "   - **Notation**:\n",
        "      - $N$: Number of samples in the dataset.\n",
        "      - $\\mathbf{x}_i$: Input data point $i$, where $i \\in \\{1, 2, \\ldots, N\\}$.\n",
        "      - $\\mathbf{y}_i$: Label or target associated with input data $i$.\n",
        "   - **Formulation**:\n",
        "      - Dataset: $D=\\{(\\mathbf{x}_i, \\mathbf{y}_i)\\mid i = 1, 2, \\ldots, N\\}$\n",
        "      - Each $\\mathbf{x}_i \\in \u211d^M$, where $M$ is the dimensionality of the input feature vector\n",
        "   - **Example**: $D = \\{ (\\mathbf{x}_1, \\mathbf{y}_1), (\\mathbf{x}_2, \\mathbf{y}_2), (\\mathbf{x}_3, \\mathbf{y}_3) \\}$\n",
        "      - $\\mathbf{x}_1 = [1.0, 2.0], \\quad \\mathbf{y}_1 = 0$\n",
        "      - $\\mathbf{x}_2 = [2.5, 3.5], \\quad \\mathbf{y}_2 = 1$\n",
        "      - $\\mathbf{x}_3 = [0.5, 1.5], \\quad \\mathbf{y}_3 = 0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data: tensor([[1.1000, 2.1000]]), label: tensor([0])\n",
            "data: tensor([[2.5000, 3.5000]]), label: tensor([1])\n",
            "data: tensor([[0.5000, 1.5000]]), label: tensor([0])\n"
          ]
        }
      ],
      "source": [
        "class RegularDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = torch.tensor([[1.1, 2.1], [2.5, 3.5], [0.5, 1.5]], dtype=torch.float32)\n",
        "        self.labels = torch.tensor([0, 1, 0], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = RegularDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "# iterate through the dataset\n",
        "for data, label in dataloader:\n",
        "    print(f\"data: {data}, label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequential Dataset\n",
        "   - Sequential datasets used in RNNs are composed of sequences of data points.\n",
        "   - Each sequence represents a temporal or sequential relationship among the data points\n",
        "   - **Notation**:\n",
        "      - $N$: Number of sequences in the dataset.\n",
        "      - $T$: Length of each sequence.\n",
        "      - $\\mathbf{x}^t_i$: Input data point at time step $t$ in the sequence $i$, where $t \\in \\{1, 2, \\ldots, T\\}$ and $i \\in \\{1, 2, \\ldots, N\\}$\n",
        "      - $\\mathbf{y}_i$: Label or target associated with sequence $i$.\n",
        "   - **Formulation**:\n",
        "      - Dataset: $D = \\{ (\\mathbf{x}_i^1, \\mathbf{x}_i^2, \\ldots, \\mathbf{x}_i^T, \\mathbf{y}_i) \\mid i = 1, 2, \\ldots, N \\}$\n",
        "      - Each $\\mathbf{x}^t_i \\in \u211d^M$, where $M$ is the dimensionality of the input feature vector at each time step.\n",
        "   - **Example**: $D = \\{ (\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{y}_1), (\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{y}_2), (\\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{x}_5, \\mathbf{y}_3) \\}$\n",
        "      - $\\mathbf{x}_1 = [1.0, 0.0]$\n",
        "      - $\\mathbf{x}_2 = [0.5, 1.5]$\n",
        "      - $\\mathbf{x}_3 = [1.0, 2.0]$\n",
        "      - $\\mathbf{x}_4 = [2.0, 1.0]$\n",
        "      - $\\mathbf{x}_5 = [1.5, 0.5]$\n",
        "      - $\\mathbf{y}_1 = 0$\n",
        "      - $\\mathbf{y}_2 = 1$\n",
        "      - $\\mathbf{y}_3 = 0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sequence:\n",
            "tensor([[[1.0000, 0.0000],\n",
            "         [0.5000, 1.5000],\n",
            "         [1.0000, 2.0000]]])\n",
            "label: tensor([0])\n",
            "\n",
            "sequence:\n",
            "tensor([[[2.0000, 1.0000],\n",
            "         [1.5000, 0.5000],\n",
            "         [2.5000, 1.5000]]])\n",
            "label: tensor([1])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class SequentialDatasetWithoutOverlap(Dataset):\n",
        "    def __init__(self):\n",
        "        # original data points\n",
        "        self.data = torch.tensor([\n",
        "            [1.0, 0.0],\n",
        "            [0.5, 1.5],\n",
        "            [1.0, 2.0],\n",
        "            [2.0, 1.0],\n",
        "            [1.5, 0.5],\n",
        "            [2.5, 1.5]\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        # labels for each sequence\n",
        "        self.labels = torch.tensor([0, 1], dtype=torch.int64)\n",
        "\n",
        "        # sequence length\n",
        "        self.seq_length = 3\n",
        "\n",
        "    def __len__(self):\n",
        "        # number of sequences without overlap\n",
        "        return len(self.data) // self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # calculate the start index of the sequence\n",
        "        start_idx = idx * self.seq_length\n",
        "\n",
        "        # create a sequence of length seq_length\n",
        "        sequence = self.data[start_idx:start_idx + self.seq_length]\n",
        "        label = self.labels[idx]\n",
        "        return sequence, label\n",
        "\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = SequentialDatasetWithoutOverlap()\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "# iterate through the dataset\n",
        "for sequence, label in dataloader:\n",
        "    print(f\"sequence:\\n{sequence}\\nlabel: {label}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sequence:\n",
            "tensor([[[0.5000, 1.5000],\n",
            "         [1.0000, 2.0000],\n",
            "         [2.0000, 1.0000]]])\n",
            "label: tensor([1])\n",
            "\n",
            "sequence:\n",
            "tensor([[[1.0000, 2.0000],\n",
            "         [2.0000, 1.0000],\n",
            "         [1.5000, 0.5000]]])\n",
            "label: tensor([0])\n",
            "\n",
            "sequence:\n",
            "tensor([[[1.0000, 0.0000],\n",
            "         [0.5000, 1.5000],\n",
            "         [1.0000, 2.0000]]])\n",
            "label: tensor([0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class SequentialDatasetWithOverlap(Dataset):\n",
        "    def __init__(self):\n",
        "        # original data points\n",
        "        self.data = torch.tensor([\n",
        "            [1.0, 0.0],\n",
        "            [0.5, 1.5],\n",
        "            [1.0, 2.0],\n",
        "            [2.0, 1.0],\n",
        "            [1.5, 0.5]\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        # labels for each sequence\n",
        "        self.labels = torch.tensor([0, 1, 0], dtype=torch.int64)\n",
        "\n",
        "        # sequence length\n",
        "        self.seq_length = 3\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # create a sequence of length seq_length\n",
        "        sequence = self.data[idx:idx+self.seq_length]\n",
        "        label = self.labels[idx]\n",
        "        return sequence, label\n",
        "\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = SequentialDatasetWithOverlap()\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# iterate through the dataset\n",
        "for sequence, label in dataloader:\n",
        "    print(f\"sequence:\\n{sequence}\\nlabel: {label}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Types of sequence-to-sequence modeling configurations\n",
        "   - **One-to-One** (Single Input to Single Output):\n",
        "      - Simplest form of neural network where a single input is mapped to a single output\n",
        "      - Used in a standard feed-forward neural network (e.g. MLP or CNN based architectures)\n",
        "      - e.g. Image classification\n",
        "   - **One-to-Many** (Single Input to Sequence Output):\n",
        "      - A single input is processed by the RNN, which then produces a sequence of outputs over time.\n",
        "      - e.g. Image captioning (an image input resulting in a sequence of words).\n",
        "   - **Many-to-One** (Sequence Input to Single Output):\n",
        "      - The RNN processes each input in the sequence, and the final hidden state is used to produce the output\n",
        "      - e.g. Sentiment analysis (a sequence of words leading to a single sentiment label)\n",
        "   - **Many-to-Many** (Sequence Input to Sequence Output):\n",
        "      - A sequence of inputs leads to a sequence of outputs. This can be further divided into two subcategories:\n",
        "         - **Synchronized** Many-to-Many\n",
        "            - Each input in the sequence has a corresponding output\n",
        "            - The RNN processes a sequence of inputs, producing a corresponding output at each time step\n",
        "            - e.g. Video classification (each frame in a video results in a corresponding label)\n",
        "         - **Asynchronized** Many-to-Many\n",
        "            - The lengths of the input and output sequences can differ\n",
        "            - The RNN processes a sequence of inputs and generates a sequence of outputs which may have different lengths\n",
        "            - e.g. Machine translation (a sequence of words in one language translates to a sequence of words in another language)\n",
        "         \n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/original/seq-to-seq-modeling.svg\" alt=\"seq-to-seq-modeling.svg\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">sequence-to-sequence modeling</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Network Structure: Recurrent Neural Networks\n",
        "   - RNNs are specifically designed to handle sequential data, where the order of elements matters\n",
        "   - Unlike feedforward neural networks, RNNs possess a \"memory\" component to process information from previous inputs, influencing the current output\n",
        "   - Each step in the sequence is processed by the same network (shared weights), with information passed between steps\n",
        "   - RNNs can suffer from vanishing and exploding gradients, making training difficult for long sequences.\n",
        "\n",
        "**RNN Variants**:\n",
        "   - Vanilla RNN\n",
        "   - Long Short-Term Memory (LSTM)\n",
        "      - Improves upon the vanilla RNN by introducing gates to control information flow\n",
        "   - Gated Recurrent Units (GRU)\n",
        "      - Simplifies the LSTM architecture while maintaining performance\n",
        "\n",
        "**Usefull Links**:\n",
        "   - [karpathy.github.io/2015/05/21/rnn-effectiveness](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "   - [stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
        "   - [d2l.ai/chapter_recurrent-modern/deep-rnn.html](https://d2l.ai/chapter_recurrent-modern/deep-rnn.html)\n",
        "   - [towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Vanilla RNN\n",
        "   - **Notations**:\n",
        "      - $\\mathbf{x}_t$: input at time step $t$.\n",
        "      - $\\mathbf{h}_t$: Hidden state at time step $t$.\n",
        "      - $\\mathbf{y}_t$: Output at time step $t$.\n",
        "      - $\\mathbf{W}_{ih}$: Weight matrix for input to hidden\n",
        "      - $\\mathbf{W}_{hh}$: Weight matrix for hidden to hidden\n",
        "      - $\\mathbf{W}_{ho}$: Weight matrix for hidden to output\n",
        "      - $\\mathbf{b}_{ih}$: Bias for input to hidden\n",
        "      - $\\mathbf{b}_{hh}$: Bias for hidden to hidden\n",
        "      - $\\mathbf{b}_{ho}$: Bias for hidden to output\n",
        "      - $\\mathbf{\\sigma}$: Activation function (e.g., Tanh, Sigmoid, ReLU)\n",
        "      - $\\mathbf{g}$: Activation function for output (e.g., Softmax for classification)\n",
        "   - **Formulations**:\n",
        "      - Hidden State Calculation:\n",
        "      $$\\mathbf{h}_t = \\sigma(\\mathbf{W}_{ih} \\mathbf{x}_t + \\mathbf{b}_{ih} + \\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{b}_{hh}), \\quad \\mathbf{h}_0 = \\mathbf{0}$$\n",
        "      - Output Calculation:\n",
        "      $$\\mathbf{y}_t = g(\\mathbf{W}_{ho} \\mathbf{h}_t + \\mathbf{b}_{ho})$$\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/original/recurrent-neural-networks-1.svg\" alt=\"recurrent-neural-networks-1.svg\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">Vanilla Recurrent Neural Networks</figcaption>\n",
        "</figure>\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/original/recurrent-neural-networks-2.svg\" alt=\"recurrent-neural-networks-2.svg\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">Calculations</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
        "        super(VanillaRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # input to hidden connection weights\n",
        "        self.W_ih = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
        "        # input to hidden connection biases\n",
        "        self.b_ih = nn.Parameter(torch.randn(hidden_dim))\n",
        "\n",
        "        # hidden to hidden connection weights\n",
        "        self.W_hh = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "        # hidden to hidden connection biases\n",
        "        self.b_hh = nn.Parameter(torch.randn(hidden_dim))\n",
        "\n",
        "        # weights for hidden to output connection\n",
        "        self.W_ho = nn.Parameter(torch.randn(output_dim, hidden_dim))\n",
        "        # bias for output layer\n",
        "        self.b_ho = nn.Parameter(torch.randn(output_dim))\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
        "        hidden = torch.tanh(input @ self.W_ih.T + self.b_ih + hidden @ self.W_hh.T + self.b_hh)\n",
        "        output = hidden @ self.W_ho.T + self.b_ho\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
        "        # initialize the hidden state with zeros (h_0)\n",
        "        return torch.zeros(batch_size, self.hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.size()               : torch.Size([128, 5, 10])\n",
            "y.size()               : torch.Size([128])\n",
            "x.size() [first batch] : torch.Size([32, 5, 10])\n",
            "y.size() [first batch] : torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# parameters\n",
        "input_dim = 10\n",
        "hidden_dim = 20\n",
        "output_dim = 5\n",
        "num_data = 128\n",
        "sequence_length = 5\n",
        "batch_size = 32\n",
        "\n",
        "# generate synthetic dataset\n",
        "x = torch.randn(num_data, sequence_length, input_dim)\n",
        "y = torch.randn(num_data)\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = TensorDataset(x, y)\n",
        "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# log\n",
        "print(f\"x.size()               : {x.size()}\")\n",
        "print(f\"y.size()               : {y.size()}\")\n",
        "print(f\"x.size() [first batch] : {next(iter(trainsetloader))[0].size()}\")\n",
        "print(f\"y.size() [first batch] : {next(iter(trainsetloader))[1].size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VanillaRNN()"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize model\n",
        "rnn_1 = VanillaRNN(input_dim, hidden_dim, output_dim)\n",
        "rnn_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VanillaRNN                               [32, 5]                   745\n",
              "==========================================================================================\n",
              "Total params: 745\n",
              "Trainable params: 745\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.01\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(rnn_1, input_size=((batch_size, input_dim), (batch_size, hidden_dim)), device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch: 1/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 1/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 1/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 1/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 1/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# forward pass through the RNN\n",
        "for c, (x, y_true) in enumerate(trainsetloader):\n",
        "    # initialize hidden state\n",
        "    hidden = rnn_1.init_hidden(batch_size)\n",
        "\n",
        "    for i in range(sequence_length):\n",
        "        y_pred, hidden = rnn_1(x[:, i, :], hidden)\n",
        "        print(f\"batch: {c+1}/{len(trainsetloader)} | time step: {i+1} | hidden.size(): {hidden.size()} | output.size(): {y_pred.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combined Weights and Concatenated Input and Hidden\n",
        "   - Reformulate the Vanilla RNN by:\n",
        "      - Combining the input-to-hidden and hidden-to-hidden weights into a single weight matrix\n",
        "      - Concatenating the input and hidden states together\n",
        "   - **Notations**:\n",
        "      - $\\mathbf{x}_t$: Input at time step $t$.\n",
        "      - $\\mathbf{h}_t$: Hidden state at time step $t$.\n",
        "      - $\\mathbf{y}_t$: Output at time step $t$.\n",
        "      - $\\mathbf{W}$: Combined weight matrix\n",
        "      - $\\mathbf{b}$: Combined bias vector\n",
        "      - $\\mathbf{W}_{ho}$: Weight matrix for hidden to output\n",
        "      - $\\mathbf{b}_{ho}$: Bias for hidden to output\n",
        "      - $\\mathbf{\\sigma}$: Activation function (e.g., Tanh, Sigmoid, ReLU)\n",
        "      - $\\mathbf{g}$: Activation function for output (e.g., Softmax for classification)\n",
        "   - **Formulations**:\n",
        "      - Concatenation of Input and Hidden State:\n",
        "      $$\\mathbf{z}_t = [\\mathbf{x}_t; \\mathbf{h}_{t-1}]$$\n",
        "      - Hidden State Calculation:\n",
        "      $$\\mathbf{h}_t = \\sigma(\\mathbf{W} \\mathbf{z}_t + \\mathbf{b})$$\n",
        "      - Output Calculation:\n",
        "      $$\\mathbf{y}_t = g(\\mathbf{W}_{ho} \\mathbf{h}_t + \\mathbf{b}_{ho})$$\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/original/recurrent-neural-networks-3.svg\" alt=\"recurrent-neural-networks-3.svg\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">Combining Weights</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VanillaRNN2(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
        "        super(VanillaRNN2, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # combined weight matrix for input to hidden and hidden to hidden\n",
        "        self.W = nn.Parameter(torch.randn(hidden_dim, input_dim + hidden_dim))\n",
        "        self.b = nn.Parameter(torch.randn(hidden_dim))\n",
        "\n",
        "        # weights for hidden to output connection\n",
        "        self.W_ho = nn.Parameter(torch.randn(output_dim, hidden_dim))\n",
        "        self.b_ho = nn.Parameter(torch.randn(output_dim))\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
        "        combined = torch.cat((input, hidden), dim=1)  # concatenate input and hidden state\n",
        "        hidden = torch.tanh(combined @ self.W.T + self.b)\n",
        "        output = hidden @ self.W_ho.T + self.b_ho\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
        "        # initialize the hidden state with zeros (h_0)\n",
        "        return torch.zeros(batch_size, self.hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.size()               : torch.Size([128, 5, 10])\n",
            "y.size()               : torch.Size([128])\n",
            "x.size() [first batch] : torch.Size([32, 5, 10])\n",
            "y.size() [first batch] : torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# parameters\n",
        "input_dim = 10\n",
        "hidden_dim = 20\n",
        "output_dim = 5\n",
        "num_data = 128\n",
        "sequence_length = 5\n",
        "batch_size = 32\n",
        "\n",
        "# generate synthetic dataset\n",
        "x = torch.randn(num_data, sequence_length, input_dim)\n",
        "y = torch.randn(num_data)\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = TensorDataset(x, y)\n",
        "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# log\n",
        "print(f\"x.size()               : {x.size()}\")\n",
        "print(f\"y.size()               : {y.size()}\")\n",
        "print(f\"x.size() [first batch] : {next(iter(trainsetloader))[0].size()}\")\n",
        "print(f\"y.size() [first batch] : {next(iter(trainsetloader))[1].size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VanillaRNN2()"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize model\n",
        "rnn_2 = VanillaRNN2(input_dim, hidden_dim, output_dim)\n",
        "rnn_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VanillaRNN2                              [32, 5]                   725\n",
              "==========================================================================================\n",
              "Total params: 725\n",
              "Trainable params: 725\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.01\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(rnn_2, input_size=((batch_size, input_dim), hidden.size()), device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch: 1/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 1/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 1/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 1/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 1/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 1 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 2 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 3 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 4 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | time step: 5 | hidden.size(): torch.Size([32, 20]) | output.size(): torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# forward pass through the RNN\n",
        "for c, (x, y_true) in enumerate(trainsetloader):\n",
        "    # initialize hidden state\n",
        "    hidden = rnn_2.init_hidden(batch_size)\n",
        "\n",
        "    for i in range(sequence_length):\n",
        "        y_pred, hidden = rnn_2(x[:, i, :], hidden)\n",
        "        print(f\"batch: {c+1}/{len(trainsetloader)} | time step: {i+1} | hidden.size(): {hidden.size()} | output.size(): {y_pred.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep RNN\n",
        "   - A Deep RNN consists of multiple layers of RNN cells stacked on top of each other.\n",
        "   - Each layer processes the hidden states of the layer below as its input.\n",
        "   - The output of one layer is used as the input to the next layer.\n",
        "   - **Notations**:\n",
        "      - $\\mathbf{x}_t$: Input at time step $t$.\n",
        "      - $\\mathbf{h}^l_t$: Hidden state at time step $t$ in layer $l$.\n",
        "      - $\\mathbf{y}_t$: Output at time step $t$.\n",
        "      - $\\mathbf{W}^l$: Combined weight matrix for layer $l$\n",
        "      - $\\mathbf{b}^l$: Bias vector for layer $l$\n",
        "      - $\\mathbf{W}_{ho}$: Weight matrix for hidden to output\n",
        "      - $\\mathbf{b}_{ho}$: Bias for hidden to output\n",
        "      - $\\mathbf{\\sigma}$: Activation function (e.g., Tanh, Sigmoid, ReLU)\n",
        "      - $\\mathbf{g}$: Activation function for output (e.g., Softmax for classification)\n",
        "      - $\\mathbf{L}$: Number of layers\n",
        "   - **Formulations**:\n",
        "      - Concatenation of Input and Hidden State for Layer 1:\n",
        "      $$\\mathbf{z}_t^1 = [\\mathbf{x}_t; \\mathbf{h}_{t-1}^1]$$\n",
        "      - Hidden State Calculation for Layer 1:\n",
        "      $$\\mathbf{h}_t^1 = \\sigma(\\mathbf{W}^1 \\mathbf{z}_t^1 + \\mathbf{b}^1)$$\n",
        "      - Concatenation of Hidden States for Subsequent Layers:\n",
        "      $$\\mathbf{z}_t^l = [\\mathbf{h}_t^{l-1}; \\mathbf{h}_{t-1}^l] \\quad \\text{for} \\quad l = 2, \\ldots, L$$\n",
        "      - Hidden State Calculation for Subsequent Layers:\n",
        "      $$\\mathbf{h}_t^l = \\sigma(\\mathbf{W}^l \\mathbf{z}_t^l + \\mathbf{b}^l) \\quad \\text{for} \\quad l = 2, \\ldots, L$$\n",
        "      - Output Calculation:\n",
        "      $$\\mathbf{y}_t = g(\\mathbf{W}_{ho} \\mathbf{h}_t^L + \\mathbf{b}_{ho})$$\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/original/recurrent-neural-networks-4.svg\" alt=\"recurrent-neural-networks-4.svg\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">Deep Recurrent Neural Networks</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepRNN(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int) -> None:\n",
        "        super(DeepRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # define RNN layers\n",
        "        self.rnn_layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            if i == 0:\n",
        "                self.rnn_layers.append(nn.Linear(input_dim + hidden_dim, hidden_dim))\n",
        "            else:\n",
        "                self.rnn_layers.append(nn.Linear(hidden_dim + hidden_dim, hidden_dim))\n",
        "                \n",
        "        # define the output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
        "        combined_input = torch.cat((input, hidden[0]), dim=1)  # concatenate input and the first hidden state along the feature dimension\n",
        "        new_hidden = []\n",
        "\n",
        "        for i, rnn_layer in enumerate(self.rnn_layers):\n",
        "            hidden_state = torch.tanh(rnn_layer(combined_input))\n",
        "            new_hidden.append(hidden_state)\n",
        "            combined_input = torch.cat((hidden_state, hidden[i]), dim=1)  # concatenate the current hidden state with the previous one\n",
        "\n",
        "        # use the last hidden state for output\n",
        "        final_hidden = new_hidden[-1]\n",
        "        output = self.output_layer(final_hidden)\n",
        "        return output, torch.stack(new_hidden)\n",
        "\n",
        "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
        "        # initialize hidden state with zeros for each layer and batch\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters\n",
        "input_dim = 10\n",
        "hidden_dim = 20\n",
        "output_dim = 5\n",
        "num_layers = 3\n",
        "num_data = 128\n",
        "sequence_length = 5\n",
        "batch_size = 32\n",
        "\n",
        "# generate synthetic dataset\n",
        "x = torch.randn(num_data, sequence_length, input_dim)\n",
        "y = torch.randn(num_data)\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = TensorDataset(x, y)\n",
        "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeepRNN(\n",
              "  (rnn_layers): ModuleList(\n",
              "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
              "    (1-2): 2 x Linear(in_features=40, out_features=20, bias=True)\n",
              "  )\n",
              "  (output_layer): Linear(in_features=20, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize model\n",
        "deep_rnn = DeepRNN(input_dim, hidden_dim, output_dim, num_layers)\n",
        "deep_rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "DeepRNN                                  [32, 5]                   --\n",
              "\u251c\u2500ModuleList: 1-1                        --                        --\n",
              "\u2502    \u2514\u2500Linear: 2-1                       [32, 20]                  620\n",
              "\u2502    \u2514\u2500Linear: 2-2                       [32, 20]                  820\n",
              "\u2502    \u2514\u2500Linear: 2-3                       [32, 20]                  820\n",
              "\u251c\u2500Linear: 1-2                            [32, 5]                   105\n",
              "==========================================================================================\n",
              "Total params: 2,365\n",
              "Trainable params: 2,365\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.08\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 0.02\n",
              "Params size (MB): 0.01\n",
              "Estimated Total Size (MB): 0.04\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(deep_rnn, input_size=((batch_size, input_dim), (num_layers, batch_size, hidden_dim)), device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 20])\n",
            "Batch: 1/4 | Time step: 1 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 1/4 | Time step: 2 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 1/4 | Time step: 3 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 1/4 | Time step: 4 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 1/4 | Time step: 5 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "torch.Size([3, 32, 20])\n",
            "Batch: 2/4 | Time step: 1 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 2/4 | Time step: 2 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 2/4 | Time step: 3 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 2/4 | Time step: 4 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 2/4 | Time step: 5 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "torch.Size([3, 32, 20])\n",
            "Batch: 3/4 | Time step: 1 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 3/4 | Time step: 2 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 3/4 | Time step: 3 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 3/4 | Time step: 4 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 3/4 | Time step: 5 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "torch.Size([3, 32, 20])\n",
            "Batch: 4/4 | Time step: 1 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 4/4 | Time step: 2 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 4/4 | Time step: 3 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 4/4 | Time step: 4 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n",
            "Batch: 4/4 | Time step: 5 | hidden.size(): torch.Size([3, 32, 20]) | output.size(): torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# forward pass through the RNN\n",
        "for c, (x, y_true) in enumerate(trainsetloader):\n",
        "    # initialize hidden state for each batch\n",
        "    hidden = deep_rnn.init_hidden(batch_size)\n",
        "    print(hidden.size())\n",
        "    \n",
        "    for i in range(sequence_length):\n",
        "        y_pred, hidden = deep_rnn(x[:, i, :], hidden)\n",
        "        print(f\"Batch: {c+1}/{len(trainsetloader)} | Time step: {i+1} | hidden.size(): {hidden.size()} | output.size(): {y_pred.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN using PyTorch\n",
        "   - [pytorch.org/docs/stable/generated/torch.nn.RNN.html](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int = 1) -> None:\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # forward propagate RNN\n",
        "        # out : output of the RNN layer for each sequence in a batch for each time step.\n",
        "        # _   : the final hidden state (often denoted as hn) of the RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # decode the hidden state of the last time step [seq-to-one modeling]\n",
        "        # :  -> selects all elements along the first dimension (typically batch size).\n",
        "        # -1 -> selects the last element along the second dimension (which represents the sequence length)\n",
        "        # :  -> selects all elements along the third dimension (feature dimension)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters\n",
        "input_dim = 10\n",
        "hidden_dim = 20\n",
        "output_dim = 5\n",
        "num_layers = 1\n",
        "num_data = 128\n",
        "sequence_length = 5\n",
        "batch_size = 32\n",
        "\n",
        "# generate synthetic dataset\n",
        "x = torch.randn(num_data, sequence_length, input_dim)\n",
        "y = torch.randn(num_data, output_dim)\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = TensorDataset(x, y)\n",
        "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (rnn): RNN(10, 20, batch_first=True)\n",
              "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize model\n",
        "rnn_3 = RNN(input_dim, hidden_dim, output_dim, num_layers)\n",
        "rnn_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "RNN                                      [32, 5]                   --\n",
              "\u251c\u2500RNN: 1-1                               [32, 5, 20]               640\n",
              "\u251c\u2500Linear: 1-2                            [32, 5]                   105\n",
              "==========================================================================================\n",
              "Total params: 745\n",
              "Trainable params: 745\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.11\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 0.03\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.04\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(rnn_3, input_size=(batch_size, *x.size()[1:]), device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch: 1/4 | output.size(): torch.Size([32, 5])\n",
            "batch: 2/4 | output.size(): torch.Size([32, 5])\n",
            "batch: 3/4 | output.size(): torch.Size([32, 5])\n",
            "batch: 4/4 | output.size(): torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# forward pass through the RNN\n",
        "for c, (x, y_true) in enumerate(trainsetloader):\n",
        "    y_pred = rnn_3(x)\n",
        "    print(f\"batch: {c+1}/{len(trainsetloader)} | output.size(): {y_pred.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Long Short-Term Memory (LSTM)\n",
        "   - A type of recurrent neural network (RNN) aimed at dealing with the vanishing gradient problem present in traditional RNNs.\n",
        "   - It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\".\n",
        "   - It is based on the [Long Short-term Memory](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) paper, Developed in 1997 by [Sepp Hochreiter](https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en) and [J\u00fcrgen Schmidhuber](https://scholar.google.com/citations?user=gLnCTgIAAAAJ&hl=en).\n",
        "   - [pytorch.org/docs/stable/generated/torch.nn.LSTM.html](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "   - **Notations**:\n",
        "      - $\\mathbf{x}_t$: Input vector at time step $t$\n",
        "      - $\\mathbf{h}_t$: Hidden state vector at time step $t$\n",
        "      - $\\mathbf{c}_t$: Cell state vector at time step $t$\n",
        "      - $\\mathbf{W}_f$: Weight matrix for the forget gate\n",
        "      - $\\mathbf{W}_i$: Weight matrix for the input gate\n",
        "      - $\\mathbf{W}_c$: Weight matrix for the candidate cell state\n",
        "      - $\\mathbf{W}_o$: Weight matrix for the output gate\n",
        "      - $\\mathbf{U}_f$: Weight matrix for the forget gate (recurrent)\n",
        "      - $\\mathbf{U}_i$: Weight matrix for the input gate (recurrent)\n",
        "      - $\\mathbf{U}_c$: Weight matrix for the candidate cell state (recurrent)\n",
        "      - $\\mathbf{U}_o$: Weight matrix for the output gate (recurrent)\n",
        "      - $\\mathbf{b}_f$: Bias vector for the forget gate\n",
        "      - $\\mathbf{b}_i$: Bias vector for the input gate\n",
        "      - $\\mathbf{b}_c$: Bias vector for the candidate cell state\n",
        "      - $\\mathbf{b}_o$: Bias vector for the output gate\n",
        "   - **Formulations**:\n",
        "      - Forget Gate\n",
        "      $$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\mathbf{x}_t + \\mathbf{U}_f \\mathbf{h}_{t-1} + \\mathbf{b}_f)$$\n",
        "      - Input Gate\n",
        "      $$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\mathbf{x}_t + \\mathbf{U}_i \\mathbf{h}_{t-1} + \\mathbf{b}_i)$$\n",
        "      - Candidate Cell State\n",
        "      $$\\tilde{\\mathbf{c}}_t = \\tanh(\\mathbf{W}_c \\mathbf{x}_t + \\mathbf{U}_c \\mathbf{h}_{t-1} + \\mathbf{b}_c)$$\n",
        "      - Cell State\n",
        "      $$\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t$$\n",
        "      - Output Gate\n",
        "      $$\\mathbf{o}_t = \\sigma(\\mathbf{W}_o \\mathbf{x}_t + \\mathbf{U}_o \\mathbf{h}_{t-1} + \\mathbf{b}_o)$$\n",
        "      - Hidden State\n",
        "      $$\\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)$$\n",
        "   - **Notes**:\n",
        "      - The lack of `Weights` in the `Cell State`, allows the long-term memories to flow through a series of unrolled units without causing the gradient to explode or vanish.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/original/recurrent-neural-networks-5.svg\" alt=\"recurrent-neural-networks-5.svg\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">Long Short-Term Memory (LSTM)</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int) -> None:\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters\n",
        "input_dim = 10\n",
        "hidden_dim = 20\n",
        "output_dim = 5\n",
        "num_layers = 2\n",
        "num_data = 128\n",
        "sequence_length = 5\n",
        "batch_size = 32\n",
        "\n",
        "# generate synthetic dataset\n",
        "x = torch.randn(num_data, sequence_length, input_dim)\n",
        "y = torch.randn(num_data, output_dim)\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = TensorDataset(x, y)\n",
        "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(10, 20, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize model\n",
        "lstm = LSTMModel(input_dim, hidden_dim, output_dim, num_layers)\n",
        "lstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "LSTMModel                                [32, 5]                   --\n",
              "\u251c\u2500LSTM: 1-1                              [32, 5, 20]               5,920\n",
              "\u251c\u2500Linear: 1-2                            [32, 5]                   105\n",
              "==========================================================================================\n",
              "Total params: 6,025\n",
              "Trainable params: 6,025\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.95\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 0.03\n",
              "Params size (MB): 0.02\n",
              "Estimated Total Size (MB): 0.06\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(lstm, input_size=(batch_size, *x.size()[1:]), device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch: 1/4 | Output Size: torch.Size([32, 5])\n",
            "Batch: 2/4 | Output Size: torch.Size([32, 5])\n",
            "Batch: 3/4 | Output Size: torch.Size([32, 5])\n",
            "Batch: 4/4 | Output Size: torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# forward pass through the LSTM\n",
        "for c, (x_batch, y_true) in enumerate(trainsetloader):\n",
        "    y_pred = lstm(x_batch)\n",
        "    print(f\"Batch: {c+1}/{len(trainsetloader)} | Output Size: {y_pred.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gated Recurrent Units (GRU)\n",
        "   - A gating mechanism in recurrent neural networks, introduced in 2014 by [Kyunghyun](https://dblp.uni-trier.de/search/author?author=Kyunghyun%20Cho).\n",
        "   - It is based on the [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555) paper.\n",
        "   - Similar to LSTM but lacks a `context vector` or `output gate`, resulting in fewer parameters than LSTM.\n",
        "   - [pytorch.org/docs/stable/generated/torch.nn.GRU.html](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "   - **Notations**:\n",
        "      - $\\mathbf{x}_t$: Input vector at time step $t$\n",
        "      - $\\mathbf{h}_t$: Hidden state vector at time step $t$\n",
        "      - $\\mathbf{c}_t$: Concatenated input and hidden state vector at time step $t$\n",
        "      - $\\mathbf{W}_z$: Weight matrix for the update gate\n",
        "      - $\\mathbf{W}_r$: Weight matrix for the reset gate\n",
        "      - $\\mathbf{W}_h$: Weight matrix for the candidate hidden state\n",
        "      - $\\mathbf{b}_z$: Bias vector for the update gate\n",
        "      - $\\mathbf{b}_r$: Bias vector for the reset gate\n",
        "      - $\\mathbf{b}_h$: Bias vector for the candidate hidden state\n",
        "      - $\\mathbf{z}_t$: Update gate vector at time step $t$\n",
        "      - $\\mathbf{r}_t$: Reset gate vector at time step $t$\n",
        "      - $\\mathbf{\\tilde{h}}_t$: Candidate hidden state vector at time step $t$\n",
        "   - **Formulations**:\n",
        "      - Concatenated Input and Hidden State:\n",
        "      $$\\mathbf{c}_t = [\\mathbf{x}_t; \\mathbf{h}_{t-1}]$$\n",
        "      - Reset Gate:\n",
        "      $$\\mathbf{r}_t = \\sigma(\\mathbf{W}_r \\mathbf{c}_t + \\mathbf{b}_r)$$\n",
        "      - Update Gate:\n",
        "      $$\\mathbf{z}_t = \\sigma(\\mathbf{W}_z \\mathbf{c}_t + \\mathbf{b}_z)$$\n",
        "      - Candidate Hidden State:\n",
        "      $$\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{W}_h [\\mathbf{x}_t; (\\mathbf{r}_t \\odot \\mathbf{h}_{t-1})] + \\mathbf{b}_h)$$\n",
        "      - Hidden State:\n",
        "      $$\\mathbf{h}_t = (1 - \\mathbf{z}_t) \\odot \\mathbf{h}_{t-1} + \\mathbf{z}_t \\odot \\tilde{\\mathbf{h}}_t$$\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/original/recurrent-neural-networks-6.svg\" alt=\"recurrent-neural-networks-6.svg\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">Gated Recurrent Units (GRU)</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int) -> None:\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters\n",
        "input_dim = 10\n",
        "hidden_dim = 20\n",
        "output_dim = 5\n",
        "num_layers = 2\n",
        "num_data = 128\n",
        "sequence_length = 5\n",
        "batch_size = 32\n",
        "\n",
        "# generate synthetic dataset\n",
        "x = torch.randn(num_data, sequence_length, input_dim)\n",
        "y = torch.randn(num_data, output_dim)\n",
        "\n",
        "# create dataset and dataloader\n",
        "dataset = TensorDataset(x, y)\n",
        "trainsetloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GRUModel(\n",
              "  (gru): GRU(10, 20, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize model\n",
        "gru = GRUModel(input_dim, hidden_dim, output_dim, num_layers)\n",
        "gru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "GRUModel                                 [32, 5]                   --\n",
              "\u251c\u2500GRU: 1-1                               [32, 5, 20]               4,440\n",
              "\u251c\u2500Linear: 1-2                            [32, 5]                   105\n",
              "==========================================================================================\n",
              "Total params: 4,545\n",
              "Trainable params: 4,545\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.71\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 0.03\n",
              "Params size (MB): 0.02\n",
              "Estimated Total Size (MB): 0.05\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(gru, input_size=(batch_size, *x.size()[1:]), device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch: 1/4 | Output Size: torch.Size([32, 5])\n",
            "Batch: 2/4 | Output Size: torch.Size([32, 5])\n",
            "Batch: 3/4 | Output Size: torch.Size([32, 5])\n",
            "Batch: 4/4 | Output Size: torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# forward pass through the GRU\n",
        "for c, (x_batch, y_true) in enumerate(trainsetloader):\n",
        "    y_pred = gru(x_batch)\n",
        "    print(f\"Batch: {c+1}/{len(trainsetloader)} | Output Size: {y_pred.size()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "author_name": "Amirhossein Heydari",
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}