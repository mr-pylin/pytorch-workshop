{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** amirhosseinheydari78@gmail.com - üìç **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "   - torchvision models:\n",
    "      - class\n",
    "         - brings in the model class directly\n",
    "         - Allows more control and customization since you are dealing directly with the class. You can override methods, customize initialization, etc.\n",
    "      - function\n",
    "         - This import brings in a function that returns an instance of the model\n",
    "         - Easier and quicker to use, especially for standard models\n",
    "   - [pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torchvision.models import VGG, vgg11, vgg13, vgg16, vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet\n",
    "   - Developed in 2014 by [Karen Simonyan](https://dblp.uni-trier.de/search/author?author=Karen%20Simonyan) and [Andrew Zisserman](https://dblp.uni-trier.de/pid/z/AndrewZisserman.html?q=Andrew%20Zisserman) from the Visual Geometry Group ([VGG](https://www.robots.ox.ac.uk/~vgg/index.html)) at the University of [Oxford](https://www.ox.ac.uk/).\n",
    "   - It is based on the [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) paper\n",
    "   - It was trained on the [ImageNet](https://www.image-net.org/) dataset (first resized to 256x256 then center cropped to 224x224) [[ImageNet viewer](https://navigu.net/#imagenet)]\n",
    "   - Known for its simple and uniform architecture, using small `3x3` convolutional filters consistently throughout the network\n",
    "   - It comes in several variants, primarily `VGG11`, `VGG13`, `VGG16` and `VGG19`, indicating the total number of layers\n",
    "   - The `runner-up` of the ImageNet Large Scale Visual Recognition Challenge ([ILSVRC](https://image-net.org/challenges/LSVRC/2014/)) in 2014\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../../../assets/images/original/cnn/architectures/vgg16.svg\" alt=\"vgg16-architecture.svg\" style=\"width: 100%;\">\n",
    "    <figcaption>VGG16 Architecture</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom VGGNet\n",
    "   - `Softmax` is missing due to internal implementation of `LogSoftmax` in the `CrossEntropyLoss` function.\n",
    "   - there is an extension of VGGNet which also contains `nn.BatchNorm2d` before `nn.ReLU` in the `feature_extractor` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVGGNet(nn.Module):\n",
    "    def __init__(self, feature_layers: list, num_classes: int = 1000) -> None:\n",
    "        super(CustomVGGNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(*self._make_layers(feature_layers))\n",
    "\n",
    "        # 512x7x7 -> 512x7x7\n",
    "        # trainable params: 0\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
    "\n",
    "        # flatten : 512x7x7 -> 25088\n",
    "        # 25088 -> 1000\n",
    "        self.classifier = nn.Sequential(\n",
    "\n",
    "            # 25088 -> 4096\n",
    "            # trainable params: (25088 + 1) * 4096 = 102,764,544\n",
    "            nn.Linear(25088, 4096),\n",
    "\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: 0\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: 0\n",
    "            nn.Dropout(),\n",
    "\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: (4096 + 1) * 4096 = 16,781,312\n",
    "            nn.Linear(4096, 4096),\n",
    "\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: 0\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: 0\n",
    "            nn.Dropout(),\n",
    "\n",
    "            # 4096 -> 1000\n",
    "            # trainable params: (4096 + 1) * 1000 = 4,097,000\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # feature extractor\n",
    "        x = self.features(x)\n",
    "\n",
    "        # adaptive average pooling\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        # flatten : 512x7x7 -> 25088\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # classifier\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layers(self, cfg: list) -> list:\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_1 = CustomVGGNet(\n",
    "    feature_layers=[64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    num_classes=1000\n",
    ")\n",
    "\n",
    "vgg11_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg11_1, (1, 3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg13_1 = CustomVGGNet(\n",
    "    feature_layers=[64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    num_classes=1000\n",
    ")\n",
    "\n",
    "vgg13_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg13_1, (1, 3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_1 = CustomVGGNet(\n",
    "    feature_layers=[64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    num_classes=1000\n",
    ")\n",
    "\n",
    "vgg16_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg16_1, (1, 3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_1 = CustomVGGNet(\n",
    "    feature_layers=[64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    num_classes=1000\n",
    ")\n",
    "\n",
    "vgg19_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg19_1, (1, 3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch VGGNet\n",
    "   - All VGGNet variants available in PyTorch: [pytorch.org/vision/main/models/vgg.html](https://pytorch.org/vision/main/models/vgg.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_2 = vgg11()\n",
    "vgg11_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg11_2, (1, 3, 227, 227), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg13_2 = vgg13()\n",
    "vgg13_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg13_2, (1, 3, 227, 227), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_2 = vgg16()\n",
    "vgg16_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg16_2, (1, 3, 227, 227), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_2 = vgg19()\n",
    "vgg19_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg19_2, (1, 3, 227, 227), device='cpu')"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
