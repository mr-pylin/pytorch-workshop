{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch`\n",
    "   - Direct access to the underlying C++ implementation.\n",
    "   - Optimized for performance.\n",
    "   - Typically used internally by PyTorch.\n",
    "   - Not commonly used directly in user code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import relu, sigmoid, sign, softmax, tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.nn`\n",
    "   - Creates a ReLU module.\n",
    "   - Can be used as a layer in a neural network.\n",
    "   - Has parameters (e.g., `inplace`) for additional control.\n",
    "   - Suitable for building complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ELU, GELU, LeakyReLU, LogSigmoid, LogSoftmax, Mish, ReLU, Sigmoid, SiLU, Softmax, Softplus, Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.nn.functional`\n",
    "   - Functional API for applying ReLU.\n",
    "   - More flexible than torch.nn.ReLU for custom operations.\n",
    "   - Often used directly in model forward passes.\n",
    "   - Provides more control over the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import elu, gelu, leaky_relu, logsigmoid, log_softmax, mish, relu, sigmoid, silu, softmax, softplus, tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "   - Activation functions are used to introduce non-linearity into the neural network\n",
    "   - Without an activation function, a neural network would behave like a linear regression model, no matter how many layers it has\n",
    "   - [pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "   - [pytorch.org/docs/stable/nn.html#non-linear-activations-other](https://pytorch.org/docs/stable/nn.html#non-linear-activations-other)\n",
    "   - [pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)\n",
    "\n",
    "**Note**:\n",
    "   - Using Python functions is not a correct implementation of an activation function for Pytorch\n",
    "   - The correct implementation is covered in the future notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.0000,  -9.9800,  -9.9600,  ...,   9.9600,   9.9800,  10.0000])\n"
     ]
    }
   ],
   "source": [
    "# domain [-10, +10]\n",
    "x = torch.linspace(-10, +10, 1001)\n",
    "\n",
    "# log\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, linear_func(x))\n",
    "plt.title(\"Linear\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "   - Historically used for `binary classification`, but less common now due to [vanishing gradient](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484) issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, sigmoid_func(x))\n",
    "plt.title(\"Sigmoid\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent (Tanh)\n",
    "   - Similar to `sigmoid` but centered around 0, used in [recurrent neural networks (RNNs)](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) and older architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    exp_x = torch.exp(x)\n",
    "    exp_neg_x = torch.exp(-x)\n",
    "    return (exp_x - exp_neg_x) / (exp_x + exp_neg_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, tanh_func(x))\n",
    "plt.title(\"Hyperbolic Tangent (Tanh)\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "   - Smooth approximation of `ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(1 + torch.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, softplus_func(x))\n",
    "plt.title(\"Softplus\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogSigmoid\n",
    "   - Logarithm of `sigmoid`, less common but used in specific applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsigmoid_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.log(1 / (1 + torch.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, logsigmoid_func(x))\n",
    "plt.title(\"LogSigmoid\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    "   - Most commonly used, computationally efficient, but suffers from the [dying ReLU](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks) ([vanishing gradient](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)) problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.max(x, torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, relu_func(x))\n",
    "plt.title(\"Rectified Linear Unit (ReLU)\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeakyReLU\n",
    "   - Addresses the `dying ReLU` problem by allowing a small, non-zero gradient for negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_func(x: torch.Tensor, negative_slope: float = 0.2) -> torch.Tensor:\n",
    "    return torch.max(x, negative_slope * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, leaky_relu_func(x))\n",
    "plt.title(\"LeakyReLU\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Linear Unit (ELU)\n",
    "   - Similar to `LeakyReLU` but uses an exponential function for negative inputs, often providing better performance than `ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_func(x: torch.Tensor, alpha: int = 1.0) -> torch.Tensor:\n",
    "    return torch.where(x > 0, x, alpha * (torch.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, elu_func(x))\n",
    "plt.title(\"Exponential Linear Unit (ELU)\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Linear Unit (SiLU)\n",
    "   - Combines ReLU-like behavior with a smooth curve, often yielding better results than `ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, silu_func(x))\n",
    "plt.title(\"Sigmoid Linear Unit (SiLU)\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mish\n",
    "   - Self-regularized activation function, generally performs better than `ReLU` and its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x * torch.tanh(torch.nn.functional.softplus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, mish_func(x))\n",
    "plt.title(\"Mish\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "   - Used for `multi-class classification`, outputs probabilities [[mutually exclusive](https://en.wikipedia.org/wiki/Softmax_function)] for each class, often used `internally` in `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_func(x: torch.Tensor, dim=None) -> torch.Tensor:\n",
    "    if dim is None:\n",
    "        dim = len(x.shape) - 1\n",
    "    exp_x = torch.exp(x - x.max(dim=dim, keepdim=True).values)\n",
    "    return exp_x / exp_x.sum(dim=dim, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, softmax_func(x))\n",
    "plt.title(\"Softmax\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-0.05, 0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogSoftmax\n",
    "   - Logarithm of softmax, often used in `NLLLoss`.\n",
    "   - Reducing the risk of numerical issues and ensuring more reliable calculations rather than `Softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax_func(x: torch.Tensor, dim=None) -> torch.Tensor:\n",
    "    if dim is None:\n",
    "        dim = len(x.shape) - 1\n",
    "    softmax_x = torch.nn.functional.softmax(x, dim=dim)\n",
    "    return torch.log(softmax_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, logsoftmax_func(x))\n",
    "plt.title(\"LogSoftmax\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-25, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Error Linear Units (GeLU)\n",
    "   - Approximates the expected value of `ReLU` with a Gaussian input, often used in `transformer-based` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 2.0 ** 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, gelu_func(x))\n",
    "plt.title(\"Gaussian Error Linear Units (GeLU)\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(12, 8), layout='compressed')\n",
    "fig.suptitle(\"Activation Functions\")\n",
    "axs[0, 0].plot(x, relu_func(x))\n",
    "axs[0, 0].set(title='Rectified Linear Unit (ReLU)', xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[0, 1].plot(x, leaky_relu(x))\n",
    "axs[0, 1].set(title='LeakyReLU', xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[0, 2].plot(x, elu_func(x))\n",
    "axs[0, 2].set(title='Exponential Linear Unit (ELU)', xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[0, 3].plot(x, silu_func(x))\n",
    "axs[0, 3].set(title='Sigmoid Linear Unit (SiLU)', xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[1, 0].plot(x, mish_func(x))\n",
    "axs[1, 0].set(title='Mish', xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[1, 1].plot(x, sigmoid_func(x))\n",
    "axs[1, 1].set(title='Sigmoid', xlim=[-10, 10], ylim=[-4, 4])\n",
    "axs[1, 2].plot(x, tanh_func(x))\n",
    "axs[1, 2].set(title='Hyperbolic Tangent (Tanh)', xlim=[-10, 10], ylim=[-4, 4])\n",
    "axs[1, 3].plot(x, softplus_func(x))\n",
    "axs[1, 3].set(title='Softplus', xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[2, 0].plot(x, logsigmoid_func(x))\n",
    "axs[2, 0].set(title='LogSigmoid', xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[2, 1].plot(x, softmax_func(x))\n",
    "axs[2, 1].set(title='Softmax', xlim=[-10, 10], ylim=[-0.05, 0.05])\n",
    "axs[2, 2].plot(x, logsoftmax_func(x))\n",
    "axs[2, 2].set(title='LogSoftmax', xlim=[-10, 10], ylim=[-25, 0])\n",
    "axs[2, 3].plot(x, gelu_func(x))\n",
    "axs[2, 3].set(title='Gaussian Error Linear Units (GeLU)', xlim=[-10, 10], ylim=[-10, 10])\n",
    "for ax in fig.axes:\n",
    "    ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Functions\n",
    "   - Threshold functions are a simpler type of activation function primarily used in the early development of neural networks\n",
    "   - These functions decide whether a neuron should be activated or not based on whether the input surpasses a certain threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.where(x >= 0, torch.ones_like(x), torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, step_func(x))\n",
    "plt.title(\"Step\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_func(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.where(x > 0, torch.ones_like(x), torch.where(x < 0, torch.ones_like(x) * -1, torch.zeros_like(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, sign_func(x))\n",
    "plt.title(\"Sign\")\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Threshold Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), layout='compressed')\n",
    "fig.suptitle(\"Threshold Functions\")\n",
    "axs[0].plot(x, step_func(x))\n",
    "axs[0].grid(True)\n",
    "axs[0].set(title='step', xlim=[-10, 10], ylim=[-2, 2])\n",
    "axs[1].plot(x, sign_func(x))\n",
    "axs[1].grid(True)\n",
    "axs[1].set(title='sign', xlim=[-10, 10], ylim=[-2, 2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
