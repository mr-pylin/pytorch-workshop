{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìù **Author:** Amirhossein Heydari - üìß **Email:** amirhosseinheydari78@gmail.com - üìç **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set a seed for deterministic results\n",
        "random_state = 42\n",
        "torch.manual_seed(random_state)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check if cuda is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multilayer Perceptron\n",
        "   - A [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (aka Fully-Connected Network or Dense Network) is a class of feedforward artificial neural networks\n",
        "   - An MLP consists of at least three layers of nodes: an input layer, one or more hidden layers, and an output layer\n",
        "\n",
        "**Key Characteristics**:\n",
        "   - Fully Connected: Every node (neuron) in one layer is connected to every node in the next layer.\n",
        "   - Non-Linear [Activation](https://en.wikipedia.org/wiki/Activation_function): Each neuron applies a non-linear activation function, allowing the network to model complex patterns.\n",
        "   - [Feedforward](https://en.wikipedia.org/wiki/Feedforward_neural_network): Data moves in one direction, from input to output, with no cycles or loops.\n",
        "\n",
        "**Basic Architecture**:\n",
        "   - Input Layer: Receives the input features. The number of neurons here equals the number of features in the input data.\n",
        "   - Hidden Layers: Each hidden layer contains a set of neurons that apply weighted sums and activation functions. The number of neurons and layers is a [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_%28machine_learning%29).\n",
        "   - Output Layer: Produces the final output, which could be a single value ([regression](https://en.wikipedia.org/wiki/Regression_analysis)) or a set of values ([classification](https://en.wikipedia.org/wiki/Classification)).\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/SVGs/multi-layer-perceptron.svg\" alt=\"multi-layer-perceptron.svg\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">Multi-Layer-Perceptron (aka fully connected layers)</figcaption>\n",
        "</figure>\n",
        "\n",
        "<table style=\"margin-left:auto;margin-right:auto;text-align:center;\">\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th colspan=\"2\">hidden<sub>1</sub> parameters</th>\n",
        "      <th colspan=\"2\">hidden<sub>2</sub> parameters</th>\n",
        "      <th colspan=\"2\">logits parameters</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Weights</td>\n",
        "      <td>Biases</td>\n",
        "      <td>Weights</td>\n",
        "      <td>Biases</td>\n",
        "      <td>Weights</td>\n",
        "      <td>Biases</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>A √ó B</td>\n",
        "      <td>B</td>\n",
        "      <td>B √ó C</td>\n",
        "      <td>C</td>\n",
        "      <td>C √ó D</td>\n",
        "      <td>D</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "  <tfoot>\n",
        "    <tr>\n",
        "      <td colspan=\"2\">(A + 1) √ó B</td>\n",
        "      <td colspan=\"2\">(B + 1) √ó C</td>\n",
        "      <td colspan=\"2\">(C + 1) √ó D</td>\n",
        "    </tr>\n",
        "  </tfoot>\n",
        "</table>\n",
        "\n",
        "**Activation Functions**:\n",
        "   - [Activation functions](./04_activation-functions.ipynb) introduce non-linearity into the network, enabling it to learn and model complex data.\n",
        "   - Common activation functions include:\n",
        "      - Sigmoid: $\\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}}$ (good for binary classification)\n",
        "      - Tanh: $\\text{Tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ (outputs range from -1 to 1)\n",
        "      - ReLU (Rectified Linear Unit): $\\text{ReLU}(x) = \\max(0, x)$ (popular in deep learning)\n",
        "\n",
        "**Training an MLP**:\n",
        "   - Forward Pass: Calculate the output using the current weights and biases.\n",
        "   - Loss Calculation: Compute the loss using a [loss function](./05_loss-functions.ipynb), such as Mean Squared Error (MSE) for regression or Cross-Entropy for classification.\n",
        "   - Backward Pass (Backpropagation): Calculate the gradient of the loss function with respect to each weight and bias.\n",
        "   - Weight Update: Update the weights and biases using an optimization algorithm like Gradient Descent or Adam.\n",
        "\n",
        "**Limitations of MLPs**:\n",
        "   - Scalability: MLPs with many layers and neurons require significant computational resources.\n",
        "   - [Vanishing Gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem): In deep networks, gradients can become very small, making training difficult.\n",
        "   - Data Efficiency: MLPs generally require a large amount of data to perform well.\n",
        "\n",
        "**MLPs vs. Other Architectures**:\n",
        "   - MLPs vs. [CNNs (Convolutional Neural Networks)](./12_convolutional-neural-networks.ipynb): CNNs are better suited for image data because they can capture spatial hierarchies, while MLPs are more general-purpose.\n",
        "   - MLPs vs. [RNNs (Recurrent Neural Networks)](./18_recurrent-neural-networks.ipynb): RNNs are used for sequential data (e.g., time series, language modeling) because they can handle temporal dependencies.\n",
        "\n",
        "**Notes**:\n",
        "   - loss function : \n",
        "      - multi-class classification : `torch.nn.CrossEntropyLoss` = `torch.nn.LogSoftmax` + `torch.nn.NLLLoss`\n",
        "      - [pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "      - [pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)\n",
        "   - activation function for the last layer:\n",
        "      - when using `torch.nn.CrossEntropyLoss` as a loss function, the output layer doesn't need an activation function\n",
        "      - `torch.nn.CrossEntropyLoss` calculates `torch.nn.LogSoftmax` and `torch.nn.NLLLoss` internally.\n",
        "      - [pytorch.org/docs/stable/generated/torch.nn.Softmax.html](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
        "      - [pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html)\n",
        "   - `torch.nn.Linear`\n",
        "      - Weights\n",
        "         - Initialized based on a scheme similar to Xavier/Glorot initialization\n",
        "         - Uniform Distribution [default]: $W \\sim \\mathcal{U}\\left(-{gain}\\times\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, {gain}\\times\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right)$\n",
        "         - Normal Distribution: $W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\\right)$\n",
        "      - Biases:\n",
        "         - Initialized to zero\n",
        "      - [pytorch.org/docs/stable/nn.init.html](https://pytorch.org/docs/stable/nn.init.html)\n",
        "      - Paper: [Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010).](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
        "\n",
        "**Playground**:\n",
        "   - [deeperplayground.org](https://deeperplayground.org/)\n",
        "   - [alexlenail.me/NN-SVG](https://alexlenail.me/NN-SVG/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward Propagation Using Linear Algebra\n",
        "   - Layer 1 (First Hidden Layer)\n",
        "      - Input: $x \\in ‚Ñù^d$, where $d$ is the number of input features.\n",
        "      - Weights: $W^{(1)} \\in ‚Ñù^{h_1 \\times d}$, where $h_1$‚Äã is the number of neurons in the first hidden layer.\n",
        "      - Biases: $b^{(1)} \\in ‚Ñù^{h_1}$.\n",
        "      - The transformation for the first hidden layer is:\n",
        "      $$\\mathbf{z}^{(1)} = \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)})$$\n",
        "\n",
        "   - For each subsequent layer $l$, where $l = \\{2, 3, \\ldots, ùêø ‚àí 1\\}$\n",
        "      - Input from the previous layer: $z^{(l-1)} \\in ‚Ñù^{h_{l-1}}$.\n",
        "      - Weights: $W^{(l)} \\in ‚Ñù^{h_l \\times h_{l-1}}$, where $h_l$‚Äã is the number of neurons in the $l$-th hidden layer.\n",
        "      - Biases: $b^{(1)} \\in ‚Ñù^{h_l}$.\n",
        "      - The transformation for each hidden layer is:\n",
        "      $$\\mathbf{z}^{(l)} = \\sigma(\\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)})$$\n",
        "\n",
        "   - Output Layer\n",
        "      - Weights: $W^{(L)} \\in ‚Ñù^{o \\times h_{L-1}}$, where $o$ is the number of output neurons.\n",
        "      - Biases: $b^{(L)} \\in ‚Ñù^{o}$.\n",
        "      - The transformation for the output is:\n",
        "      $$\\mathbf{\\hat{y}} = \\sigma_L(\\mathbf{W}^{(L)} \\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        # initialize weights and biases for the first hidden layer\n",
        "        self.W1 = nn.Parameter(torch.randn(hidden_size1, input_size))\n",
        "        self.b1 = nn.Parameter(torch.randn(hidden_size1))\n",
        "        \n",
        "        # initialize weights and biases for the second hidden layer\n",
        "        self.W2 = nn.Parameter(torch.randn(hidden_size2, hidden_size1))\n",
        "        self.b2 = nn.Parameter(torch.randn(hidden_size2))\n",
        "        \n",
        "        # initialize weights and biases for the output layer\n",
        "        self.W3 = nn.Parameter(torch.randn(output_size, hidden_size2))\n",
        "        self.b3 = nn.Parameter(torch.randn(output_size))\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        self.z1 = x @ self.W1.T + self.b1\n",
        "        self.a1 = F.relu(self.z1)\n",
        "        \n",
        "        self.z2 = self.a1 @ self.W2.T + self.b2\n",
        "        self.a2 = F.relu(self.z2)\n",
        "        \n",
        "        self.z3 = self.a2 @ self.W3.T + self.b3\n",
        "        return self.z3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP()"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example input\n",
        "batch_size = 3\n",
        "x = torch.randn(batch_size, 10)\n",
        "y = torch.randn(batch_size, 2)\n",
        "\n",
        "# initialize the MLP\n",
        "input_size = 10   # number of input features\n",
        "hidden_size1 = 5  # number of neurons in the first hidden layer\n",
        "hidden_size2 = 3  # number of neurons in the second hidden layer\n",
        "output_size = 2   # number of output neurons (e.g., for binary classification)\n",
        "\n",
        "model_1 = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
        "model_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MLP                                      [3, 2]                    81\n",
              "==========================================================================================\n",
              "Total params: 81\n",
              "Trainable params: 81\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model_1, input_size=(x.size()), device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_pred:\n",
            "tensor([[ 5.2491,  6.4449],\n",
            "        [-0.6936,  0.9967],\n",
            "        [-0.0099,  1.6234]])\n"
          ]
        }
      ],
      "source": [
        "# perform forward propagation\n",
        "with torch.no_grad():\n",
        "    y_pred = model_1.forward(x)\n",
        "\n",
        "# log\n",
        "print(f\"y_pred:\\n{y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Computation and Backpropagation\n",
        "   - Compute the Loss:\n",
        "   $$\\mathcal{L}(\\mathbf{\\hat{y}}, \\mathbf{y})$$\n",
        "   - Backpropagation\n",
        "      - Compute the gradient of the loss with respect to the output layer weights and biases:\n",
        "      $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(L)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(L)}} \\cdot \\frac{\\partial \\mathbf{z}^{(L)}}{\\partial \\mathbf{W}^{(L)}}$$\n",
        "      - Compute gradients for the weights and biases of each preceding layer:\n",
        "      $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(l)}} \\cdot \\frac{\\partial \\mathbf{z}^{(l)}}{\\partial \\mathbf{W}^{(l)}}$$\n",
        "   - Update the Parameters using a gradient-based optimization algorithm like Gradient Descent or Adam:\n",
        "   $$\\mathbf{W}^{(l)} = \\mathbf{W}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward(self, x: torch.Tensor, y: torch.Tensor, learning_rate: float):\n",
        "    # compute the loss (Mean Squared Error - MSE)\n",
        "    # loss = (1/N) * sum((z3 - y)^2) over all batch samples\n",
        "    loss = torch.mean((self.z3 - y) ** 2)\n",
        "\n",
        "    # compute the gradient of the loss with respect to z3 (output layer pre-activation)\n",
        "    # this is the local gradient for the loss function with respect to z3\n",
        "    # d(loss)/d(z3) = 2 * (z3 - y) / N\n",
        "    loss_grad = 2 * (self.z3 - y) / y.size(0)\n",
        "\n",
        "    # compute the gradient of the loss with respect to W3 (weights between hidden layer 2 and output layer)\n",
        "    # d(loss)/d(W3) = d(loss)/d(z3) * d(z3)/d(W3)\n",
        "    # d(z3)/d(W3) = a2^T (activation of hidden layer 2)\n",
        "    # grad_W3 = (loss_grad)^T * a2\n",
        "    grad_W3 = torch.matmul(loss_grad.T, self.a2)\n",
        "\n",
        "    # compute the gradient of the loss with respect to b3 (biases of the output layer)\n",
        "    # d(loss)/d(b3) = d(loss)/d(z3) * d(z3)/d(b3)\n",
        "    # d(z3)/d(b3) = 1 (bias gradient accumulates over the batch dimension)\n",
        "    # grad_b3 = sum(loss_grad) across batch dimension\n",
        "    grad_b3 = torch.sum(loss_grad, dim=0)\n",
        "\n",
        "    # backpropagate the gradient to the second hidden layer (w.r.t. a2)\n",
        "    # compute the gradient of the loss with respect to a2 (activation of hidden layer 2)\n",
        "    # d(loss)/d(a2) = d(loss)/d(z3) * d(z3)/d(a2)\n",
        "    # d(z3)/d(a2) = W3 (weights between hidden layer 2 and output layer)\n",
        "    grad_a2 = torch.matmul(loss_grad, self.W3)\n",
        "\n",
        "    # compute the gradient of the loss with respect to z2 (pre-activation of hidden layer 2)\n",
        "    # this is the local gradient for ReLU at the second hidden layer\n",
        "    # d(z2)/d(a2) = ReLU'(z2) (element-wise derivative of ReLU)\n",
        "    # grad_z2 = grad_a2 * ReLU'(z2) (ReLU'(z2) is 1 where z2 > 0, else 0)\n",
        "    grad_z2 = grad_a2 * (self.a2 > 0).float()\n",
        "\n",
        "    # compute the gradient of the loss with respect to W2 (weights between hidden layer 1 and hidden layer 2)\n",
        "    # d(loss)/d(W2) = d(loss)/d(z2) * d(z2)/d(W2)\n",
        "    # d(z2)/d(W2) = a1^T (activation of hidden layer 1)\n",
        "    # grad_W2 = (grad_z2)^T * a1\n",
        "    grad_W2 = torch.matmul(grad_z2.T, self.a1)\n",
        "\n",
        "    # compute the gradient of the loss with respect to b2 (biases of hidden layer 2)\n",
        "    # d(loss)/d(b2) = d(loss)/d(z2) * d(z2)/d(b2)\n",
        "    # d(z2)/d(b2) = 1 (bias gradient accumulates over the batch dimension)\n",
        "    # grad_b2 = sum(grad_z2) across batch dimension\n",
        "    grad_b2 = torch.sum(grad_z2, dim=0)\n",
        "\n",
        "    # backpropagate the gradient to the first hidden layer (w.r.t. a1)\n",
        "    # compute the gradient of the loss with respect to a1 (activation of hidden layer 1)\n",
        "    # d(loss)/d(a1) = d(loss)/d(z2) * d(z2)/d(a1)\n",
        "    # d(z2)/d(a1) = W2 (weights between hidden layer 1 and hidden layer 2)\n",
        "    grad_a1 = torch.matmul(grad_z2, self.W2)\n",
        "\n",
        "    # compute the gradient of the loss with respect to z1 (pre-activation of hidden layer 1)\n",
        "    # this is the local gradient for ReLU at the first hidden layer\n",
        "    # d(z1)/d(a1) = ReLU'(z1) (element-wise derivative of ReLU)\n",
        "    # grad_z1 = grad_a1 * ReLU'(z1) (ReLU'(z1) is 1 where z1 > 0, else 0)\n",
        "    grad_z1 = grad_a1 * (self.a1 > 0).float()\n",
        "\n",
        "    # compute the gradient of the loss with respect to W1 (weights between input layer and hidden layer 1)\n",
        "    # d(loss)/d(W1) = d(loss)/d(z1) * d(z1)/d(W1)\n",
        "    # d(z1)/d(W1) = x^T (input features)\n",
        "    # grad_W1 = (grad_z1)^T * x\n",
        "    grad_W1 = torch.matmul(grad_z1.T, x)\n",
        "\n",
        "    # compute the gradient of the loss with respect to b1 (biases of hidden layer 1)\n",
        "    # d(loss)/d(b1) = d(loss)/d(z1) * d(z1)/d(b1)\n",
        "    # d(z1)/d(b1) = 1 (bias gradient accumulates over the batch dimension)\n",
        "    # grad_b1 = sum(grad_z1) across batch dimension\n",
        "    grad_b1 = torch.sum(grad_z1, dim=0)\n",
        "\n",
        "    # update parameters using gradients (Gradient Descent step)\n",
        "    with torch.no_grad():\n",
        "        self.W1 -= learning_rate * grad_W1\n",
        "        self.b1 -= learning_rate * grad_b1\n",
        "        self.W2 -= learning_rate * grad_W2\n",
        "        self.b2 -= learning_rate * grad_b2\n",
        "        self.W3 -= learning_rate * grad_W3\n",
        "        self.b3 -= learning_rate * grad_b3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "MLP.backward = backward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MLP                                      [3, 2]                    81\n",
              "==========================================================================================\n",
              "Total params: 81\n",
              "Trainable params: 81\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example input\n",
        "batch_size = 3\n",
        "x = torch.randn(batch_size, 10)\n",
        "y = torch.randn(batch_size, 2)\n",
        "\n",
        "# initialize the MLP\n",
        "input_size = 10   # Number of input features\n",
        "hidden_size1 = 5  # Number of neurons in the first hidden layer\n",
        "hidden_size2 = 3  # Number of neurons in the second hidden layer\n",
        "output_size = 2   # Number of output neurons\n",
        "\n",
        "model_2 = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
        "summary(model_2, input_size= x.size(), device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_true:\n",
            "tensor([[ 0.6131, -1.0648],\n",
            "        [ 0.1055,  1.9739],\n",
            "        [ 1.0703, -1.7379]])\n",
            "\n",
            "output before backpropagation:\n",
            "tensor([[-1.6613,  6.9142],\n",
            "        [-0.9228,  5.4186],\n",
            "        [-1.0267,  5.6290]])\n",
            "\n",
            "output after backpropagation:\n",
            "tensor([[0.8538, 1.7682],\n",
            "        [0.3796, 2.6229],\n",
            "        [0.8538, 1.7682]])\n"
          ]
        }
      ],
      "source": [
        "# perform forward propagation\n",
        "with torch.no_grad():\n",
        "    y_pred_1 = model_2.forward(x)\n",
        "\n",
        "# Perform backward propagation and update weights\n",
        "learning_rate = 0.01\n",
        "model_2.backward(x, y, learning_rate)\n",
        "\n",
        "# Perform forward propagation again to see updated output\n",
        "with torch.no_grad():\n",
        "    y_pred_2 = model_2.forward(x)\n",
        "\n",
        "# log\n",
        "print(f\"y_true:\\n{y}\\n\")\n",
        "print(f\"output before backpropagation:\\n{y_pred_1}\\n\")\n",
        "print(f\"output after backpropagation:\\n{y_pred_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multilayer Perceptron Using PyTorch\n",
        "   - Refer to this [notebook](./projects/00_multi-layer-perceptron.ipynb) for a comprehensive example on the MLP concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(MLP2, self).__init__()\n",
        "        # define layers using nn.Linear\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)    # first hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # second hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)   # output layer\n",
        "        \n",
        "        # define activation function (ReLU)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward pass through the network\n",
        "        x = self.relu(self.fc1(x))  # first hidden layer with ReLU\n",
        "        x = self.relu(self.fc2(x))  # second hidden layer with ReLU\n",
        "        x = self.fc3(x)             # output layer (no activation here)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example input\n",
        "batch_size = 3\n",
        "x = torch.randn(batch_size, 10)\n",
        "y = torch.randn(batch_size, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLP2(\n",
              "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
              "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
              "  (fc3): Linear(in_features=3, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize the MLP\n",
        "input_size = 10   # number of input features\n",
        "hidden_size1 = 5  # number of neurons in the first hidden layer\n",
        "hidden_size2 = 3  # number of neurons in the second hidden layer\n",
        "output_size = 2   # number of output neurons\n",
        "\n",
        "model_3 = MLP2(input_size, hidden_size1, hidden_size2, output_size)\n",
        "model_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MLP2                                     [3, 2]                    --\n",
              "‚îú‚îÄLinear: 1-1                            [3, 5]                    55\n",
              "‚îú‚îÄReLU: 1-2                              [3, 5]                    --\n",
              "‚îú‚îÄLinear: 1-3                            [3, 3]                    18\n",
              "‚îú‚îÄReLU: 1-4                              [3, 3]                    --\n",
              "‚îú‚îÄLinear: 1-5                            [3, 2]                    8\n",
              "==========================================================================================\n",
              "Total params: 81\n",
              "Trainable params: 81\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model_3, input_size= x.size(), device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  10/100  ->  Loss: 1.0662\n",
            "epoch  20/100  ->  Loss: 0.8673\n",
            "epoch  30/100  ->  Loss: 0.7438\n",
            "epoch  40/100  ->  Loss: 0.6574\n",
            "epoch  50/100  ->  Loss: 0.5912\n",
            "epoch  60/100  ->  Loss: 0.5368\n",
            "epoch  70/100  ->  Loss: 0.4932\n",
            "epoch  80/100  ->  Loss: 0.4588\n",
            "epoch  90/100  ->  Loss: 0.4289\n",
            "epoch 100/100  ->  Loss: 0.4014\n"
          ]
        }
      ],
      "source": [
        "# define a loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# define an optimizer (e.g., SGD)\n",
        "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.01)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 100  # Number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # forward pass\n",
        "    output = model_3(x)\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = criterion(output, y)\n",
        "    \n",
        "    # perform backward propagation automatically\n",
        "    loss.backward()\n",
        "    \n",
        "    # update the weights & zero the gradients\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # log\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'epoch {epoch+1:3}/{num_epochs}  ->  Loss: {loss.item():.4f}')"
      ]
    }
  ],
  "metadata": {
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "author_name": "Amirhossein Heydari",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
