{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** amirhosseinheydari78@gmail.com - üìç **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for deterministic results\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "   - A [**Multilayer Perceptron (MLP)**](https://en.wikipedia.org/wiki/Multilayer_perceptron) is a type of feedforward artificial neural network, also known as a **Fully-Connected Network** or **Dense Network**.\n",
    "   - It consists of at least three layers of nodes: an **input layer**, one or more **hidden layers**, and an **output layer**.\n",
    "\n",
    "üß¨ **Key Characteristics**:\n",
    "   - **Fully Connected**: Every node (neuron) in one layer is connected to every node in the next layer.\n",
    "   - **Non-Linear [Activations](./utils/activation-functions.ipynb)**: Each neuron applies a non-linear activation function, enabling the network to model complex patterns.\n",
    "   - **[Feedforward](https://en.wikipedia.org/wiki/Feedforward_neural_network)**: Data flows in a single direction, from input to output, with no cycles or loops.\n",
    "\n",
    "üèõÔ∏è **Basic Architecture**:\n",
    "   - **Input Layer**: Receives input features. The number of neurons equals the number of features in the dataset.\n",
    "   - **Hidden Layers**: These layers contain neurons that compute weighted sums and apply activation functions.\n",
    "   - **Output Layer**: Produces the final output, which could be a single value or a set of values for different tasks e.g. [**regression**](https://en.wikipedia.org/wiki/Regression_analysis), and [**classification**](https://en.wikipedia.org/wiki/Classification).\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../assets/images/original/mlp/multi-layer-perceptrons.svg\" alt=\"multi-layer-perceptrons.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Multi-Layer-Perceptron (aka fully connected layers)</figcaption>\n",
    "</figure>\n",
    "\n",
    "<table style=\"margin: 0 auto; text-align:center;\">\n",
    "   <thead>\n",
    "      <tr>\n",
    "         <th colspan=\"2\">hidden<sub>1</sub> parameters</th>\n",
    "         <th colspan=\"2\">hidden<sub>2</sub> parameters</th>\n",
    "         <th colspan=\"2\">logits parameters</th>\n",
    "      </tr>\n",
    "   </thead>\n",
    "   <tbody>\n",
    "      <tr>\n",
    "         <td>Weights</td>\n",
    "         <td>Biases</td>\n",
    "         <td>Weights</td>\n",
    "         <td>Biases</td>\n",
    "         <td>Weights</td>\n",
    "         <td>Biases</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td>A √ó B</td>\n",
    "         <td>B</td>\n",
    "         <td>B √ó C</td>\n",
    "         <td>C</td>\n",
    "         <td>C √ó D</td>\n",
    "         <td>D</td>\n",
    "      </tr>\n",
    "   </tbody>\n",
    "   <tfoot>\n",
    "      <tr>\n",
    "         <td colspan=\"2\">(A + 1) √ó B</td>\n",
    "         <td colspan=\"2\">(B + 1) √ó C</td>\n",
    "         <td colspan=\"2\">(C + 1) √ó D</td>\n",
    "      </tr>\n",
    "   </tfoot>\n",
    "</table>\n",
    "\n",
    "üìâ **Limitations of MLPs**:\n",
    "   - **Fixed Input and Output Sizes**:\n",
    "      - MLPs require a fixed size for both input and output, making them less flexible for tasks involving variable-length sequences.\n",
    "   - **Lack of Temporal Awareness**:\n",
    "      - MLPs do not inherently handle temporal data well.\n",
    "      - They treat each input independently, which means they can't capture the temporal dependencies in sequential data.\n",
    "   - **Scalability Issues**:\n",
    "      - As the size of the input data grows, the number of parameters in an MLP increases significantly, leading to higher computational costs and potential **overfitting**.\n",
    "   - **Stateless Nature**:\n",
    "      - MLPs learn a fixed function approximation and do not maintain any state between inputs, which limits their ability to model dynamic processes.\n",
    "\n",
    "‚öîÔ∏è **MLPs vs. Other Architectures**:\n",
    "   - MLPs vs. [CNNs (Convolutional Neural Networks)](./08-convolutional-neural-networks.ipynb): CNNs are better suited for image data because they can capture spatial hierarchies, while MLPs are more general-purpose.\n",
    "   - MLPs vs. [RNNs (Recurrent Neural Networks)](./12-recurrent-neural-networks.ipynb): RNNs are used for sequential data (e.g., time series, language modeling) because they can handle temporal dependencies.\n",
    "\n",
    "üõ†Ô∏è **Weight and Bias Initialization**:\n",
    "   - **Weight**\n",
    "      - Weights are initialized using the Kaiming (He) initialization by default, which is suitable for layers using ReLU activation functions.\n",
    "      -  the weights are initialized from a uniform distribution with a range based on the number of input and output units.\n",
    "      $$W \\sim \\mathcal{U}\\left(-{gain}\\times\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, {gain}\\times\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right)$$\n",
    "   - **Bias**:\n",
    "      - Biases are initialized to zero by default.\n",
    "   - More Details about Initialization: [hyperparameters.ipynb](./utils/hyperparameters.ipynb)\n",
    "\n",
    "üõù **Playgrounds**:\n",
    "   - [deeperplayground.org](https://deeperplayground.org/)\n",
    "   - [alexlenail.me/NN-SVG](https://alexlenail.me/NN-SVG/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation Using Linear Algebra\n",
    "   - **Layer 1 (First Hidden Layer)**\n",
    "      - **Input**: $x \\in ‚Ñù^d$, where $d$ is the number of input features.\n",
    "      - **Weights**: $W^{(1)} \\in ‚Ñù^{h_1 \\times d}$, where $h_1$‚Äã is the number of neurons in the first hidden layer.\n",
    "      - **Biases**: $b^{(1)} \\in ‚Ñù^{h_1}$.\n",
    "      - The transformation for the first hidden layer is:\n",
    "      $$\\mathbf{z}^{(1)} = \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)})$$\n",
    "\n",
    "   - **For each subsequent layer** $l$, where $l = \\{2, 3, \\ldots, L ‚àí 1\\}$\n",
    "      - **Input** from the previous layer: $z^{(l-1)} \\in ‚Ñù^{h_{l-1}}$.\n",
    "      - **Weights**: $W^{(l)} \\in ‚Ñù^{h_l \\times h_{l-1}}$, where $h_l$‚Äã is the number of neurons in the $l$-th hidden layer.\n",
    "      - **Biases**: $b^{(1)} \\in ‚Ñù^{h_l}$.\n",
    "      - The transformation for each hidden layer is:\n",
    "      $$\\mathbf{z}^{(l)} = \\sigma(\\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "   - **Output Layer**\n",
    "      - **Weights**: $W^{(L)} \\in ‚Ñù^{o \\times h_{L-1}}$, where $o$ is the number of output neurons.\n",
    "      - **Biases**: $b^{(L)} \\in ‚Ñù^{o}$.\n",
    "      - The transformation for the output is:\n",
    "      $$\\mathbf{\\hat{y}} = \\sigma_L(\\mathbf{W}^{(L)} \\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # initialize weights and biases for the first hidden layer\n",
    "        self.W1 = nn.Parameter(torch.randn(hidden_size1, input_size))\n",
    "        self.b1 = nn.Parameter(torch.randn(hidden_size1))\n",
    "        \n",
    "        # initialize weights and biases for the second hidden layer\n",
    "        self.W2 = nn.Parameter(torch.randn(hidden_size2, hidden_size1))\n",
    "        self.b2 = nn.Parameter(torch.randn(hidden_size2))\n",
    "        \n",
    "        # initialize weights and biases for the output layer\n",
    "        self.W3 = nn.Parameter(torch.randn(output_size, hidden_size2))\n",
    "        self.b3 = nn.Parameter(torch.randn(output_size))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.z1 = x @ self.W1.T + self.b1\n",
    "        self.a1 = F.relu(self.z1)\n",
    "        \n",
    "        self.z2 = self.a1 @ self.W2.T + self.b2\n",
    "        self.a2 = F.relu(self.z2)\n",
    "        \n",
    "        self.z3 = self.a2 @ self.W3.T + self.b3\n",
    "        return self.z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example input\n",
    "batch_size = 3\n",
    "x = torch.randn(batch_size, 10)\n",
    "y = torch.randn(batch_size, 2)\n",
    "\n",
    "# initialize the MLP\n",
    "input_size = 10   # number of input features\n",
    "hidden_size1 = 5  # number of neurons in the first hidden layer\n",
    "hidden_size2 = 3  # number of neurons in the second hidden layer\n",
    "output_size = 2   # number of output neurons (e.g., for binary classification)\n",
    "\n",
    "model_1 = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MLP                                      [3, 2]                    81\n",
       "==========================================================================================\n",
       "Total params: 81\n",
       "Trainable params: 81\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_1, input_size=(x.size()), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:\n",
      "tensor([[ 5.2491,  6.4449],\n",
      "        [-0.6936,  0.9967],\n",
      "        [-0.0099,  1.6234]])\n"
     ]
    }
   ],
   "source": [
    "# perform forward propagation\n",
    "with torch.no_grad():\n",
    "    y_pred = model_1.forward(x)\n",
    "\n",
    "# log\n",
    "print(f\"y_pred:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation and Backpropagation\n",
    "   - **Compute the Loss**:\n",
    "   $$\\mathcal{L}(\\mathbf{\\hat{y}}, \\mathbf{y})$$\n",
    "   - **Backpropagation**\n",
    "      - Compute the gradient of the loss with respect to the output layer weights and biases:\n",
    "      $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(L)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(L)}} \\cdot \\frac{\\partial \\mathbf{z}^{(L)}}{\\partial \\mathbf{W}^{(L)}}$$\n",
    "      - Compute gradients for the weights and biases of each preceding layer:\n",
    "      $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(l)}} \\cdot \\frac{\\partial \\mathbf{z}^{(l)}}{\\partial \\mathbf{W}^{(l)}}$$\n",
    "   - **Update the Parameters**\n",
    "      - using a gradient-based optimization algorithm like Gradient Descent or Adam:\n",
    "   $$\\mathbf{W}^{(l)} = \\mathbf{W}^{(l)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(l)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, x: torch.Tensor, y: torch.Tensor, learning_rate: float):\n",
    "    # compute the loss (Mean Squared Error - MSE)\n",
    "    # loss = (1/N) * sum((z3 - y)^2) over all batch samples\n",
    "    loss = torch.mean((self.z3 - y) ** 2)\n",
    "\n",
    "    # compute the gradient of the loss with respect to z3 (output layer pre-activation)\n",
    "    # this is the local gradient for the loss function with respect to z3\n",
    "    # d(loss)/d(z3) = 2 * (z3 - y) / N\n",
    "    loss_grad = 2 * (self.z3 - y) / y.size(0)\n",
    "\n",
    "    # compute the gradient of the loss with respect to W3 (weights between hidden layer 2 and output layer)\n",
    "    # d(loss)/d(W3) = d(loss)/d(z3) * d(z3)/d(W3)\n",
    "    # d(z3)/d(W3) = a2^T (activation of hidden layer 2)\n",
    "    # grad_W3 = (loss_grad)^T * a2\n",
    "    grad_W3 = torch.matmul(loss_grad.T, self.a2)\n",
    "\n",
    "    # compute the gradient of the loss with respect to b3 (biases of the output layer)\n",
    "    # d(loss)/d(b3) = d(loss)/d(z3) * d(z3)/d(b3)\n",
    "    # d(z3)/d(b3) = 1 (bias gradient accumulates over the batch dimension)\n",
    "    # grad_b3 = sum(loss_grad) across batch dimension\n",
    "    grad_b3 = torch.sum(loss_grad, dim=0)\n",
    "\n",
    "    # backpropagate the gradient to the second hidden layer (w.r.t. a2)\n",
    "    # compute the gradient of the loss with respect to a2 (activation of hidden layer 2)\n",
    "    # d(loss)/d(a2) = d(loss)/d(z3) * d(z3)/d(a2)\n",
    "    # d(z3)/d(a2) = W3 (weights between hidden layer 2 and output layer)\n",
    "    grad_a2 = torch.matmul(loss_grad, self.W3)\n",
    "\n",
    "    # compute the gradient of the loss with respect to z2 (pre-activation of hidden layer 2)\n",
    "    # this is the local gradient for ReLU at the second hidden layer\n",
    "    # d(z2)/d(a2) = ReLU'(z2) (element-wise derivative of ReLU)\n",
    "    # grad_z2 = grad_a2 * ReLU'(z2) (ReLU'(z2) is 1 where z2 > 0, else 0)\n",
    "    grad_z2 = grad_a2 * (self.a2 > 0).float()\n",
    "\n",
    "    # compute the gradient of the loss with respect to W2 (weights between hidden layer 1 and hidden layer 2)\n",
    "    # d(loss)/d(W2) = d(loss)/d(z2) * d(z2)/d(W2)\n",
    "    # d(z2)/d(W2) = a1^T (activation of hidden layer 1)\n",
    "    # grad_W2 = (grad_z2)^T * a1\n",
    "    grad_W2 = torch.matmul(grad_z2.T, self.a1)\n",
    "\n",
    "    # compute the gradient of the loss with respect to b2 (biases of hidden layer 2)\n",
    "    # d(loss)/d(b2) = d(loss)/d(z2) * d(z2)/d(b2)\n",
    "    # d(z2)/d(b2) = 1 (bias gradient accumulates over the batch dimension)\n",
    "    # grad_b2 = sum(grad_z2) across batch dimension\n",
    "    grad_b2 = torch.sum(grad_z2, dim=0)\n",
    "\n",
    "    # backpropagate the gradient to the first hidden layer (w.r.t. a1)\n",
    "    # compute the gradient of the loss with respect to a1 (activation of hidden layer 1)\n",
    "    # d(loss)/d(a1) = d(loss)/d(z2) * d(z2)/d(a1)\n",
    "    # d(z2)/d(a1) = W2 (weights between hidden layer 1 and hidden layer 2)\n",
    "    grad_a1 = torch.matmul(grad_z2, self.W2)\n",
    "\n",
    "    # compute the gradient of the loss with respect to z1 (pre-activation of hidden layer 1)\n",
    "    # this is the local gradient for ReLU at the first hidden layer\n",
    "    # d(z1)/d(a1) = ReLU'(z1) (element-wise derivative of ReLU)\n",
    "    # grad_z1 = grad_a1 * ReLU'(z1) (ReLU'(z1) is 1 where z1 > 0, else 0)\n",
    "    grad_z1 = grad_a1 * (self.a1 > 0).float()\n",
    "\n",
    "    # compute the gradient of the loss with respect to W1 (weights between input layer and hidden layer 1)\n",
    "    # d(loss)/d(W1) = d(loss)/d(z1) * d(z1)/d(W1)\n",
    "    # d(z1)/d(W1) = x^T (input features)\n",
    "    # grad_W1 = (grad_z1)^T * x\n",
    "    grad_W1 = torch.matmul(grad_z1.T, x)\n",
    "\n",
    "    # compute the gradient of the loss with respect to b1 (biases of hidden layer 1)\n",
    "    # d(loss)/d(b1) = d(loss)/d(z1) * d(z1)/d(b1)\n",
    "    # d(z1)/d(b1) = 1 (bias gradient accumulates over the batch dimension)\n",
    "    # grad_b1 = sum(grad_z1) across batch dimension\n",
    "    grad_b1 = torch.sum(grad_z1, dim=0)\n",
    "\n",
    "    # update parameters using gradients (Gradient Descent step)\n",
    "    with torch.no_grad():\n",
    "        self.W1 -= learning_rate * grad_W1\n",
    "        self.b1 -= learning_rate * grad_b1\n",
    "        self.W2 -= learning_rate * grad_W2\n",
    "        self.b2 -= learning_rate * grad_b2\n",
    "        self.W3 -= learning_rate * grad_W3\n",
    "        self.b3 -= learning_rate * grad_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MLP                                      [3, 2]                    81\n",
       "==========================================================================================\n",
       "Total params: 81\n",
       "Trainable params: 81\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example input\n",
    "batch_size = 3\n",
    "x = torch.randn(batch_size, 10)\n",
    "y = torch.randn(batch_size, 2)\n",
    "\n",
    "# initialize the MLP\n",
    "input_size = 10   # Number of input features\n",
    "hidden_size1 = 5  # Number of neurons in the first hidden layer\n",
    "hidden_size2 = 3  # Number of neurons in the second hidden layer\n",
    "output_size = 2   # Number of output neurons\n",
    "\n",
    "model_2 = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "summary(model_2, input_size= x.size(), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true:\n",
      "tensor([[ 0.6131, -1.0648],\n",
      "        [ 0.1055,  1.9739],\n",
      "        [ 1.0703, -1.7379]])\n",
      "\n",
      "output before backpropagation:\n",
      "tensor([[-1.6613,  6.9142],\n",
      "        [-0.9228,  5.4186],\n",
      "        [-1.0267,  5.6290]])\n",
      "\n",
      "output after backpropagation:\n",
      "tensor([[0.8538, 1.7682],\n",
      "        [0.3796, 2.6229],\n",
      "        [0.8538, 1.7682]])\n"
     ]
    }
   ],
   "source": [
    "# perform forward propagation\n",
    "with torch.no_grad():\n",
    "    y_pred_1 = model_2.forward(x)\n",
    "\n",
    "# Perform backward propagation and update weights\n",
    "learning_rate = 0.01\n",
    "model_2.backward(x, y, learning_rate)\n",
    "\n",
    "# Perform forward propagation again to see updated output\n",
    "with torch.no_grad():\n",
    "    y_pred_2 = model_2.forward(x)\n",
    "\n",
    "# log\n",
    "print(f\"y_true:\\n{y}\\n\")\n",
    "print(f\"output before backpropagation:\\n{y_pred_1}\\n\")\n",
    "print(f\"output after backpropagation:\\n{y_pred_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Using PyTorch\n",
    "   - Refer to this [**notebook**](./projects/01-multi-layer-perceptrons.ipynb) for a comprehensive example on the MLP concept.\n",
    "\n",
    "üìö **Tutorials**:\n",
    "   - Neural Networks: [pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial)\n",
    "   - Training a Classifier: [pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP2, self).__init__()\n",
    "        # define layers using nn.Linear\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)    # first hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # second hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)   # output layer\n",
    "        \n",
    "        # define activation function (ReLU)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward pass through the network\n",
    "        x = self.relu(self.fc1(x))  # first hidden layer with ReLU\n",
    "        x = self.relu(self.fc2(x))  # second hidden layer with ReLU\n",
    "        x = self.fc3(x)             # output layer (no activation here)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input\n",
    "batch_size = 3\n",
    "x = torch.randn(batch_size, 10)\n",
    "y = torch.randn(batch_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP2(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (fc3): Linear(in_features=3, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the MLP\n",
    "input_size = 10   # number of input features\n",
    "hidden_size1 = 5  # number of neurons in the first hidden layer\n",
    "hidden_size2 = 3  # number of neurons in the second hidden layer\n",
    "output_size = 2   # number of output neurons\n",
    "\n",
    "model_3 = MLP2(input_size, hidden_size1, hidden_size2, output_size)\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MLP2                                     [3, 2]                    --\n",
       "‚îú‚îÄLinear: 1-1                            [3, 5]                    55\n",
       "‚îú‚îÄReLU: 1-2                              [3, 5]                    --\n",
       "‚îú‚îÄLinear: 1-3                            [3, 3]                    18\n",
       "‚îú‚îÄReLU: 1-4                              [3, 3]                    --\n",
       "‚îú‚îÄLinear: 1-5                            [3, 2]                    8\n",
       "==========================================================================================\n",
       "Total params: 81\n",
       "Trainable params: 81\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_3, input_size= x.size(), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  10/100  ->  Loss: 1.0662\n",
      "epoch  20/100  ->  Loss: 0.8673\n",
      "epoch  30/100  ->  Loss: 0.7438\n",
      "epoch  40/100  ->  Loss: 0.6574\n",
      "epoch  50/100  ->  Loss: 0.5912\n",
      "epoch  60/100  ->  Loss: 0.5368\n",
      "epoch  70/100  ->  Loss: 0.4932\n",
      "epoch  80/100  ->  Loss: 0.4588\n",
      "epoch  90/100  ->  Loss: 0.4289\n",
      "epoch 100/100  ->  Loss: 0.4014\n"
     ]
    }
   ],
   "source": [
    "# define a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# define an optimizer (e.g., SGD)\n",
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 100  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # forward pass\n",
    "    output = model_3(x)\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = criterion(output, y)\n",
    "    \n",
    "    # perform backward propagation automatically\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights & zero the gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # log\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'epoch {epoch+1:3}/{num_epochs}  ->  Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
