{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** amirhosseinheydari78@gmail.com - üìç **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for deterministic results\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function to Plot Decision Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X: np.ndarray, y: np.ndarray, classifier, resolution: float = 0.01) -> None:\n",
    "    # setup marker generator and color map\n",
    "    markers = (\"o\", \"s\")\n",
    "    colors = (\"red\", \"blue\")\n",
    "    cmap = ListedColormap(colors)\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\n",
    "    y_pred = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T).reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, y_pred, alpha=0.2, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(\n",
    "            x=X[y == cl, 0],\n",
    "            y=X[y == cl, 1],\n",
    "            alpha=0.8,\n",
    "            c=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            label=f\"Class {cl}\",\n",
    "            edgecolor=\"black\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Iris Dataset\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../assets/images/third_party/01_08.png\" alt=\"01_08.png\" style=\"width: 50%;\">\n",
    "    <figcaption style=\"text-align: center;\">¬©Ô∏è Image: <a href= \"https://github.com/rasbt/machine-learning-book/blob/main/ch01/figures/01_08.png\">Machine Learning with PyTorch and Scikit-Learn</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris dataset as a pandas data-frame\n",
    "iris_df = pd.read_csv(\n",
    "    r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/iris/dataset.csv\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# log\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique labels\n",
    "unique_classes = iris_df.iloc[:, -1].unique()\n",
    "\n",
    "# number of data per label\n",
    "num_data_per_class = iris_df.iloc[:, -1].value_counts()\n",
    "\n",
    "# log\n",
    "print(f\"Unique labels: {unique_classes}\")\n",
    "print(f\"Number of data per label: {num_data_per_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need species: {'Iris-setosa', 'Iris-versicolor'}\n",
    "filtered_iris_df = iris_df[iris_df.iloc[:, -1].isin([\"Iris-setosa\", \"Iris-versicolor\"])]\n",
    "\n",
    "# select only the sepal length(first column) and petal length(third column)\n",
    "filtered_iris_df = filtered_iris_df.iloc[:, [0, 2, -1]]\n",
    "\n",
    "# split features and labels\n",
    "X = filtered_iris_df.iloc[:, [0, 1]].values\n",
    "y = filtered_iris_df.iloc[:, [2]].values.squeeze()\n",
    "\n",
    "# convert labels into numbers : {'Iris-setosa':0, 'Iris-versicolor':1}\n",
    "y = np.where(y == \"Iris-setosa\", 0, 1)\n",
    "\n",
    "# log\n",
    "print(f\"X.shape : {X.shape}\")\n",
    "print(f\"X.dtype : {X.dtype}\")\n",
    "print(f\"y.shape : {y.shape}\")\n",
    "print(f\"y.dtype : {y.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.scatter(X[:50, 0], X[:50, 1], color=\"red\", marker=\"o\", label=\"Iris-setosa:0\")\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1], color=\"blue\", marker=\"s\", label=\"Iris-versicolor:1\")\n",
    "plt.xlabel(\"Sepal length [cm]\")\n",
    "plt.ylabel(\"Petal length [cm]\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "   - The Perceptron is a type of **binary linear classifier** introduced by [**Frank Rosenblatt**](https://en.wikipedia.org/wiki/Frank_Rosenblatt) in 1958.\n",
    "   - It's the simplest form of a neural network, consisting of a single layer of input neurons connected to a single output neuron.\n",
    "\n",
    "üî¨ **Formulations**:\n",
    "      $$\n",
    "      \\hat{y} = \\begin{cases} \n",
    "      1 & \\text{if} \\; \\sum w_i x_i + b \\ge 0 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "      \\end{cases}\n",
    "      $$\n",
    "\n",
    "  - $w_i$: Weights for input $x_i$\n",
    "  - $b$: Bias term\n",
    "\n",
    "üìù **Paper**: [THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../assets/images/original/perceptron/perceptron-2.svg\" alt=\"perceptron-2.svg\" style=\"width: 80%;\">\n",
    "    <figcaption style=\"text-align: center;\">A Perceptron</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "**Training Rule (Rosenblatt's Rule)**:\n",
    "$$\n",
    "w_{i+1} \\leftarrow w_i + \\eta \\cdot (y_{\\text{true}} - y_{\\text{pred}}) \\cdot x_i\n",
    "$$\n",
    "$$\n",
    "b_{i+1} \\leftarrow b_i + \\eta \\cdot (y_{\\text{true}} - y_{\\text{pred}})\n",
    "$$\n",
    "   - $\\eta$: Learning rate\n",
    "   - $y_{true}$: Actual class label\n",
    "   - $y_{pred}$: Predicted class label\n",
    "\n",
    "‚úçÔ∏è **Notes**:\n",
    "   - The original perceptron algorithm as described in the **Frank Rosenblatt**'s paper, there are some differences from what is written in this notebook:\n",
    "     - The labels are typically $-1$ and $+1$, instead of $0$ and $+1$.\n",
    "     - It did not use a **learning rate** hyperparameter.\n",
    "     - The threshold function in the original perceptron is a **sign** function rather than **step** function.\n",
    "     - Bias is referred to as $\\theta$, not $b$. $\\theta$ played the same role as the bias term but in the **threshold function**.\n",
    "   -  The **gradient descent** approach is **not** directly applicable to the **learning rule** of the **original perceptron** algorithm.\n",
    "   - The following code is adapted from the book *[Machine Learning with PyTorch and Scikit-Learn](https://github.com/rasbt/machine-learning-book)* with **modifications** made to fit the requirements of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, eta: float = 0.01, epochs: int = 50, seed: int = seed) -> None:\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        # initialize weights and bias following a normal distribution with a deterministic seed\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        self.w_ = rng.normal(loc=0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = rng.normal(loc=0, scale=0.01, size=1)\n",
    "\n",
    "        # to collect errors per epoch\n",
    "        self.errors = []\n",
    "\n",
    "        # train loop\n",
    "        for epoch in range(self.epochs):\n",
    "            errors = 0\n",
    "            for x, y_true in zip(X, y):\n",
    "\n",
    "                # output of the perceptron\n",
    "                y_pred = self.predict(x)\n",
    "\n",
    "                # update w_ and b_\n",
    "                update_step = self.eta * (y_true - y_pred)\n",
    "                self.w_ += update_step * x\n",
    "                self.b_ += update_step\n",
    "\n",
    "                # count number of updates in the current epoch\n",
    "                if update_step != 0:\n",
    "                    errors += 1\n",
    "\n",
    "            self.errors.append(errors)\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        output = np.dot(x, self.w_) + self.b_\n",
    "        unit_function = np.where(output >= 0, 1, 0)\n",
    "        return unit_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a perceptron\n",
    "perceptron = Perceptron(eta=0.1, epochs=10)\n",
    "\n",
    "# fit dataset to the model\n",
    "perceptron.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot errors per epoch\n",
    "plt.plot(perceptron.errors, marker=\"o\")\n",
    "plt.xticks(range(10))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Number of updates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary\n",
    "plot_decision_regions(X, y, classifier=perceptron)\n",
    "plt.xlabel(\"Sepal length [cm]\")\n",
    "plt.ylabel(\"Petal length [cm]\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Linear Neurons (AdaLiNe)\n",
    "   - The Adaline, developed by [Bernard Widrow](https://en.wikipedia.org/wiki/Bernard_Widrow) and [Ted Hoff](https://en.wikipedia.org/wiki/Marcian_Hoff) in 1960.\n",
    "   - It is a refinement of the perceptron, but it uses a different cost function and activation.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../assets/images/original/perceptron/adaline.svg\" alt=\"adaline.svg\" style=\"width: 80%;\">\n",
    "    <figcaption style=\"text-align: center;\">Adaptive Linear Neurons</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "**Training Rule (Widrow-Hoff Rule)**:\n",
    "   - Mean Squared Error (MSE) Loss:\n",
    "   $$\n",
    "   L(\\mathbf{w}, b) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b \\right)^2\n",
    "   $$\n",
    "   - Weights update:\n",
    "   $$\n",
    "   \\Delta \\mathbf{w} = -\\eta \\nabla_{w}L(\\mathbf{w}, b) = \\eta \\frac{1}{n} \\sum_{i=1}^{n} \\left( y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b \\right) \\mathbf{x}^{(i)}\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{w_{i+1}} \\leftarrow \\mathbf{w_i} + \\Delta \\mathbf{w}\n",
    "   $$\n",
    "   - Bias update:\n",
    "   $$\n",
    "   \\Delta b = -\\eta \\nabla_{b}L(\\mathbf{w}, b) = \\eta \\frac{1}{n} \\sum_{i=1}^{n} \\left( y^{(i)} - \\mathbf{w}^T \\mathbf{x}^{(i)} - b \\right)\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{b_{i+1}} \\leftarrow \\mathbf{b_i} + \\Delta b\n",
    "   $$\n",
    "\n",
    "‚úçÔ∏è Note:\n",
    "   - The following code is adapted from the book *[Machine Learning with PyTorch and Scikit-Learn](https://github.com/rasbt/machine-learning-book)* with **modifications** made to fit the requirements of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaLiNe:\n",
    "    def __init__(self, eta: float = 0.01, epochs: int = 50, seed: int = seed) -> None:\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        # initialize weights and bias following a normal distribution with a deterministic seed\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        self.w_ = rng.normal(loc=0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = rng.normal(loc=0, scale=0.01, size=1)\n",
    "\n",
    "        # to collect losses per epoch\n",
    "        self.losses_ = []\n",
    "\n",
    "        # training loop\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # output of the adaline (before passing to threshold function)\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "\n",
    "            # update w_ and b_\n",
    "            errors = y - output\n",
    "            self.w_ += self.eta * X.T.dot(errors) / X.shape[0]\n",
    "            self.b_ += self.eta * errors.mean()\n",
    "\n",
    "            # calculate loss function (MSE in this case)\n",
    "            loss = (errors**2).mean() / 2\n",
    "            self.losses_.append(loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def activation(self, X):\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# initialize several adaline using different learning rates and standardizing inputs\n",
    "adaline_1 = AdaLiNe(epochs=num_epochs, eta=0.1).fit(X, y)\n",
    "adaline_2 = AdaLiNe(epochs=num_epochs, eta=0.002).fit(X, y)\n",
    "\n",
    "# standardize inputs to have mean=0 and std=1\n",
    "X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "adaline_3 = AdaLiNe(epochs=num_epochs, eta=0.5).fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 4))\n",
    "ax[0].plot(range(1, len(adaline_1.losses_) + 1), np.log10(adaline_1.losses_), marker=\"o\")\n",
    "ax[0].set(\n",
    "    title=f\"Adaline - Learning rate {adaline_1.eta}\",\n",
    "    xlabel=\"Epochs\",\n",
    "    ylabel=\"log(Mean squared error)\",\n",
    "    xticks=range(num_epochs + 1),\n",
    ")\n",
    "ax[1].plot(range(1, len(adaline_2.losses_) + 1), adaline_2.losses_, marker=\"o\")\n",
    "ax[1].set(\n",
    "    title=f\"Adaline - Learning rate {adaline_2.eta}\",\n",
    "    xlabel=\"Epochs\",\n",
    "    ylabel=\"Mean squared error\",\n",
    "    xticks=range(num_epochs + 1),\n",
    ")\n",
    "ax[2].plot(range(1, len(adaline_3.losses_) + 1), adaline_3.losses_, marker=\"o\")\n",
    "ax[2].set(\n",
    "    title=f\"Adaline - Learning rate {adaline_3.eta} + Standardize Input\",\n",
    "    xlabel=\"Epochs\",\n",
    "    ylabel=\"Mean squared error\",\n",
    "    xticks=range(num_epochs + 1),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision region\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6), layout=\"compressed\")\n",
    "plt.sca(axes[0])\n",
    "plot_decision_regions(X, y, adaline_1)\n",
    "plt.title(\"Adaline - Learning rate 0.1\")\n",
    "plt.legend()\n",
    "plt.sca(axes[1])\n",
    "plot_decision_regions(X, y, adaline_2)\n",
    "plt.title(\"Adaline - Learning rate 0.0001\")\n",
    "plt.legend()\n",
    "plt.sca(axes[2])\n",
    "plot_decision_regions(X_std, y, adaline_3)\n",
    "plt.title(\"Adaline - Learning rate 0.5 + Standardize\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
