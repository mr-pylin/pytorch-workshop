{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“ **Author:** Amirhossein Heydari - ðŸ“§ **Email:** amirhosseinheydari78@gmail.com - ðŸ“ **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for deterministic results\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# log\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using Word2Vec [Skip-Grams method]\n",
    "ðŸŒŸ **Example**:\n",
    "   - Sentence: **The quick brown fox jumps over the lazy dog.**\n",
    "   - **WINDOW_SIZE: 2**\n",
    "      - For \"The\" (center word), context words are [\"quick\"].\n",
    "      - For \"quick\" (center word), context words are [\"The\", \"brown\"].\n",
    "      - For \"brown\" (center word), context words are [\"quick\", \"The\", \"fox\", \"jumps\"].\n",
    "      - For \"fox\" (center word), context words are [\"brown\", \"quick\", \"jumps\"].\n",
    "      - For \"jumps\" (center word), context words are [\"fox\", \"brown\", \"over\"].\n",
    "      - For \"over\" (center word), context words are [\"jumps\", \"the\"].\n",
    "      - For \"the\" (center word), context words are [\"over\"].\n",
    "   - **NEGATIVE_SAMPLES: 5**\n",
    "      - If we are predicting \"brown\" and the context words are [\"quick\", \"The\", \"fox\", \"jumps\"].\n",
    "      - we will also randomly select 5 words from the rest of the vocabulary (e.g., \"lazy\", \"dog\", etc.) and treat them as negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "WINDOW_SIZE = 2\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.01\n",
    "NEGATIVE_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", cache_dir=\"../../datasets/\")\n",
    "train_text = dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and build vocabulary\n",
    "tokenized_sentences = [sentence.split() for sentence in train_text if sentence.strip()]\n",
    "words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "vocab = Counter(words)\n",
    "\n",
    "# log\n",
    "print(f\"vocabulary size             : {len(vocab)}\")\n",
    "print(f\"top 5 most  frequent tokens : {vocab.most_common(5)}\")\n",
    "print(f\"top 5 least frequent tokens : {sorted(vocab.items(), key=lambda x: x[1])[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rare words\n",
    "min_freq = 5\n",
    "vocab = {word: freq for word, freq in vocab.items() if freq >= min_freq}\n",
    "word2idx = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# log\n",
    "print(f\"vocabulary size             : {vocab_size}\")\n",
    "print(f\"top 5 most  frequent tokens : {sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:5]}\")\n",
    "print(f\"top 5 least frequent tokens : {sorted(vocab.items(), key=lambda x: x[1])[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate skip-grams with negative sampling\n",
    "def generate_skip_grams(\n",
    "    words: list[str], word2idx: dict[str, int], window_size: int, vocab: dict[str, int]\n",
    ") -> list[tuple[int, int]]:\n",
    "    pairs = []\n",
    "    for idx, word in enumerate(words):\n",
    "        if word not in word2idx:\n",
    "            continue\n",
    "        center_idx = word2idx[word]\n",
    "        context_range = range(max(0, idx - window_size), min(len(words), idx + window_size + 1))\n",
    "        for context_idx in context_range:\n",
    "            if context_idx == idx:\n",
    "                continue\n",
    "            context_word = words[context_idx]\n",
    "            if context_word in word2idx:\n",
    "                pairs.append((center_idx, word2idx[context_word]))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# generate skip-grams\n",
    "skip_grams = generate_skip_grams(words, word2idx, WINDOW_SIZE, vocab)\n",
    "\n",
    "# log\n",
    "print(f\"total number of skip-grams         : {len(skip_grams)}\")\n",
    "print(f\"sequence of words [1000 to 1006]   : {words[1000:1007]}\")\n",
    "print(\"{word:idx} for the above sequence] :\", {w: word2idx[w] for w in words[1000:1007]})\n",
    "print(\n",
    "    f\"skip-gram pairs for above sequence : {[skip_grams[skip_grams.index((401, 200)) + i] for i in range(WINDOW_SIZE)]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, skip_grams, vocab_size, neg_samples):\n",
    "        self.skip_grams = skip_grams\n",
    "        self.vocab_size = vocab_size\n",
    "        self.neg_samples = neg_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skip_grams)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.skip_grams[idx]\n",
    "        negatives = torch.multinomial(torch.tensor([1.0] * self.vocab_size), self.neg_samples, replacement=True)\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long), negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(skip_grams, vocab_size, NEGATIVE_SAMPLES)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "num_batches = len(data_loader)\n",
    "\n",
    "# log\n",
    "for i in range(WINDOW_SIZE * 2):\n",
    "    temp_sample = dataset[skip_grams.index((401, 200)) + i]\n",
    "    print(temp_sample)\n",
    "    print(f\"  negative samples:\")\n",
    "    for j in range(NEGATIVE_SAMPLES):\n",
    "        print(\"    \", {temp_sample[2][j].item(): idx2word[temp_sample[2][j].item()]})\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define and Initialize the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, center, context, negatives):\n",
    "        center_embed = self.embeddings(center)\n",
    "        context_embed = self.context_embeddings(context)\n",
    "        neg_embed = self.context_embeddings(negatives)\n",
    "\n",
    "        # positive scores\n",
    "        positive_score = torch.sum(center_embed * context_embed, dim=1)\n",
    "        positive_loss = -torch.log(torch.sigmoid(positive_score + 1e-9))\n",
    "\n",
    "        # negative scores\n",
    "        negative_score = torch.bmm(neg_embed, center_embed.unsqueeze(2)).squeeze()\n",
    "        negative_loss = -torch.sum(torch.log(torch.sigmoid(-negative_score + 1e-9)), dim=1)\n",
    "\n",
    "        return (positive_loss + negative_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vocab_size, EMBEDDING_DIM).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model,\n",
    "    input_data=(\n",
    "        (\n",
    "            torch.randint(0, vocab_size, (BATCH_SIZE,)).to(device),\n",
    "            torch.randint(0, vocab_size, (BATCH_SIZE,)).to(device),\n",
    "            torch.randint(0, vocab_size, (BATCH_SIZE, NEGATIVE_SAMPLES)).to(device),\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # log\n",
    "    print(f\"epoch {epoch + 1:>0{len(str(EPOCHS))}}/{EPOCHS}\")\n",
    "\n",
    "    for i, (center, context, negatives) in enumerate(data_loader):\n",
    "        # Move data to the correct device\n",
    "        center, context, negatives = center.to(device), context.to(device), negatives.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center, context, negatives)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # log\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"  iteration {i + 1:>0{len(str(num_batches))}}/{num_batches}  |  loss: {loss.item():.4f}  |  total loss: {total_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    # log\n",
    "    print(f\"epoch {epoch+1:>0{len(str(EPOCHS))}}/{EPOCHS}  |  total loss: {total_loss:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embeddings.weight.data\n",
    "torch.save(embeddings, \"../../assets/embeddings/word2vec_embeddings.pt\")\n",
    "\n",
    "# log\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize word embeddings using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings: torch.Tensor, idx2word: dict[int, str], num_points: int = 100) -> None:\n",
    "    tsne = TSNE(n_components=2, random_state=seed)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings[:num_points].cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for idx, (x, y) in enumerate(reduced_embeddings):\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x, y, idx2word[idx], fontsize=9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(embeddings, idx2word)"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
