{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud83d\udcdd **Author:** Amirhossein Heydari - \ud83d\udce7 **Email:** amirhosseinheydari78@gmail.com - \ud83d\udccd **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.nn import BCELoss, CrossEntropyLoss, L1Loss, MSELoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear & Logistic regression\n",
        "<div>\n",
        "    <figure style=\"text-align: center;\">\n",
        "        <img src=\"../assets/images/svgs/linear-regression.svg\" alt=\"linear-regression.svg\" style=\"width: 100%;\">\n",
        "        <figcaption style=\"text-align: center;\">Linear Regression Model</figcaption>\n",
        "    </figure>\n",
        "    <br>\n",
        "    <br>\n",
        "    <figure style=\"text-align: center;\">\n",
        "        <img src=\"../assets/images/svgs/logistic-regression.svg\" alt=\"logistic-regression.svg\" style=\"width: 100%;\">\n",
        "        <figcaption style=\"text-align: center;\">Logistic Regression Model</figcaption>\n",
        "    </figure>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss Function\n",
        "   - A function that quantifies the difference between the predicted output of a model and the true target values\n",
        "   - It serves as a measure of how well (or poorly) the model's predictions align with the actual outcomes, guiding the optimization process during training\n",
        "   - Available Loss Functions in PyTorch: [pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"../assets/images/third_party/loss-function.png\" alt=\"loss-function.png\" style=\"width: 100%;\">\n",
        "    <figcaption style=\"text-align: center;\">\u00a9\ufe0f Image: <a href= \"https://www.offconvex.org/2016/03/22/saddlepoints\">offconvex.org/2016/03/22/saddlepoints</a></figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression tasks\n",
        "1. [Mean Absolute Error](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) (`torch.nn.L1Loss`)\n",
        "   - AKA L1 norm\n",
        "\n",
        "   - Robust to outliers BUT does not provide gradients for large errors, leading to slower convergence\n",
        "\n",
        "   - $L(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^{N} |\\hat{y}_i - y_i|$\n",
        "\n",
        "2. [Mean Squared Error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) (`torch.nn.MSELoss`)\n",
        "   - AKA L2 norm\n",
        "\n",
        "   - Sensitive to outliers because it penalizes large errors quadratically (due to squaring)\n",
        "\n",
        "   - $L(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$\n",
        "\n",
        "3. [Huber](https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html) (`torch.nn.HuberLoss`)\n",
        "   - It serves as a compromise between `Mean Absolute Error` and `Mean Squared Error`\n",
        "\n",
        "   - It acts `quadratic` for small errors (similar to MSE) and `linear` for large errors (similar to MAE)\n",
        "\n",
        "   - $\n",
        "L(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\begin{cases}\n",
        "\\frac{1}{2} (\\hat{y}_i - y_i)^2, & \\text{if } |\\hat{y}_i - y_i| \\leq \\delta \\\\\n",
        "\\delta (|\\hat{y}_i - y_i| - \\frac{1}{2} \\delta), & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$\n",
        "4. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification tasks\n",
        "1. Binary Classification\n",
        "   1. [Binary Cross-Entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) (`torch.nn.BCELoss`)\n",
        "      - Expects a tensor of probabilities and a tensor of binary labels\n",
        "\n",
        "      - Directly applying sigmoid to logits can lead to numerical instability (issues of overflow and underflow in computational systems)\n",
        "\n",
        "      - $L(\\hat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_{i} \\log(\\hat{y}_{i}) + (1 - y_{i}) \\log(1 - \\hat{y}_{i})]$\n",
        "\n",
        "   2. [Binary Cross-Entropy with Logits](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) (`torch.nn.BCEWithLogitsLoss`)\n",
        "      - Expects logits (raw predictions) and a tensor of binary labels\n",
        "\n",
        "      - Implicitly combines `torch.nn.Sigmoid` & `torch.nn.BCELoss` together\n",
        "\n",
        "      - It reduces the number of operations required compared to applying sigmoid and BCELoss separately\n",
        "\n",
        "      - $L(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\max(\\hat{y}_i, 0) - \\hat{y}_i \\cdot y_i + \\log(1 + \\exp(-|\\hat{y}_i|)) \\right]$\n",
        "\n",
        "   3. ...\n",
        "\n",
        "2. Multiclass Classification\n",
        "   1. [Negative Log Likelihood](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) (`torch.nn.NLLLoss`)\n",
        "      - Expects a tensor of probabilities and a tensor of binary labels.\n",
        "\n",
        "      - Directly applying LogSoftmax to logits can lead to numerical instability (issues of overflow and underflow in computational systems)\n",
        "\n",
        "      - $L(\\hat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_{iy_i})$\n",
        "\n",
        "   2. [Cross-Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (`torch.nn.CrossEntropyLoss`)\n",
        "      - Expects logits (raw predictions) and a tensor of binary labels\n",
        "\n",
        "      - Implicitly combines `torch.nn.LogSoftmax` & `torch.nn.NLLLoss` together\n",
        "\n",
        "      - It reduces the number of operations required compared to applying sigmoid and BCELoss separately\n",
        "      \n",
        "      - $L(\\hat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left(\\frac{\\exp(\\hat{y}_{iy_i})}{\\sum_{k=1}^{C} \\exp(\\hat{y}_{ik})}\\right)$\n",
        "\n",
        "   3. ...\n",
        "\n",
        "3. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BCELoss vs MSELoss\n",
        "   - BCELoss is more sensitive to the amount of error (grows faster if the distance between `y_true` & `y_pred` is high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_true: tensor([0., 0., 0.])\n",
            "y_pred: tensor([0.5000, 0.7500, 1.0000])\n",
            "--------------------------------------------------\n",
            "MSELoss [per sample]: tensor([0.2500, 0.5625, 0.9999])\n",
            "MSELoss             : 0.60414\n",
            "BCELoss [per sample]: tensor([0.6931, 1.3863, 9.9996])\n",
            "BCELoss             : 4.02635\n"
          ]
        }
      ],
      "source": [
        "# we have 3 samples for a binary classification\n",
        "y_true = torch.tensor([[0], [0], [0]], dtype=torch.float32)\n",
        "\n",
        "# output of model_1\n",
        "output = torch.tensor([[0], [1.09864], [10]], dtype=torch.float32)\n",
        "y_pred = torch.sigmoid(output)\n",
        "\n",
        "mse_1 = MSELoss(reduction='none')(y_pred, y_true).squeeze()\n",
        "mse_2 = MSELoss()(y_pred, y_true)\n",
        "bce_1 = BCELoss(reduction='none')(y_pred, y_true).squeeze()\n",
        "bce_2 = BCELoss()(y_pred, y_true)\n",
        "\n",
        "# log\n",
        "print(f\"y_true: {y_true.squeeze()}\")\n",
        "print(f\"y_pred: {y_pred.squeeze()}\")\n",
        "print('-' * 50)\n",
        "print(f\"MSELoss [per sample]: {mse_1}\")\n",
        "print(f\"MSELoss             : {mse_2:.5f}\")\n",
        "print(f\"BCELoss [per sample]: {bce_1}\")\n",
        "print(f\"BCELoss             : {bce_2:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "y_true = torch.zeros(size=(100, 1))\n",
        "y_pred = torch.sigmoid(torch.linspace(-10, +10, 100).reshape(-1, 1))\n",
        "bce_loss = BCELoss(reduction='none')(y_pred, y_true)\n",
        "mse_loss = MSELoss(reduction='none')(y_pred, y_true)\n",
        "\n",
        "plt.plot(y_pred, bce_loss, label='BCELoss')\n",
        "plt.plot(y_pred, mse_loss, label='MSELoss')\n",
        "plt.title(f\"y_true = {y_true[0, 0]}   |   {y_pred.min().round()} <= y_pred <= {y_pred.max().round()}\")\n",
        "plt.xlabel(\"y_pred\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "author_name": "Amirhossein Heydari",
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}