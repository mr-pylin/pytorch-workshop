{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** amirhosseinheydari78@gmail.com - üìç **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for deterministic results\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set print options to increase line width\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding\n",
    "   - To plug words into a Neural Network, we need a way to turn the **words** into **numbers**.\n",
    "   - One-hot encoding is a simple representation of **categorical** data in **binary vectors**.\n",
    "   - Each unique **word** (or **token**) in a vocabulary is assigned a **unique index**.\n",
    "   - The vector representation consists of **all zeros** except for a **`1`** at the position of the **word's index**.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/original/we/one-hot-embedding.svg\" alt=\"one-hot-embedding.svg\" style=\"width: 80%;\">\n",
    "    <figcaption style=\"text-align: center;\">One-Hot Embedding</figcaption>\n",
    "</figure>\n",
    "\n",
    "üìú **Properties**:\n",
    "   - **Dimensionality**\n",
    "      - The dimension of the vector equals the size of the vocabulary.\n",
    "      -  For example, if there are 10,000 words, each vector is 10,000-dimensional.\n",
    "   - **Sparsity**\n",
    "      - Most values in the vector are 0, leading to high memory consumption and computational inefficiency.\n",
    "   - **No Semantic Relationships**\n",
    "      - Vectors for words like **\"cat\"** and **\"dog\"** are as dissimilar as **\"cat\"** and **\"table\"**, even though **\"cat\"** and **\"dog\"** are **semantically related**.\n",
    "\n",
    "üìâ **Limitations**:\n",
    "   - **Inefficient** for large vocabularies due to high dimensionality.\n",
    "   - Encodes words **independently** without considering their **meaning** or **relationships**.\n",
    "   - **Sparse** vectors can lead to **poor** performance in machine learning models, especially for **large datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating One-Hot Vectors for a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple: [1.0, 0.0, 0.0, 0.0]\n",
      "banana: [0.0, 1.0, 0.0, 0.0]\n",
      "school: [0.0, 0.0, 1.0, 0.0]\n",
      "date: [0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# define the vocabulary\n",
    "vocabulary = [\"apple\", \"banana\", \"school\", \"date\"]\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# create and store one-hot vectors in a dictionary\n",
    "one_hot_vectors = {vocabulary[idx]: one_hot_vector for idx, one_hot_vector in enumerate(torch.eye(vocab_size))}\n",
    "\n",
    "# display the one-hot vectors\n",
    "for word, vector in one_hot_vectors.items():\n",
    "    print(f\"{word}: {vector.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using One-Hot Vectors in a Simple Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute similarity (cosine similarity)\n",
    "def calculate_similarity(vec1, vec2):\n",
    "    return torch.cosine_similarity(vec1.unsqueeze(dim=0), vec2.unsqueeze(dim=0))\n",
    "\n",
    "# create a 2D tensor for similarity values directly\n",
    "similarity_values = torch.zeros((vocab_size, vocab_size))\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        similarity_values[i, j] = calculate_similarity(one_hot_vectors[vocabulary[i]], one_hot_vectors[vocabulary[j]])\n",
    "\n",
    "# plot the heatmap\n",
    "plt.figure()\n",
    "sns.heatmap(similarity_values, annot=True, fmt=\".1f\", xticklabels=vocabulary, yticklabels=vocabulary, cmap=\"Blues\")\n",
    "plt.title(\"Word Similarity Heatmap\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency-Based Word Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count Vectorization\n",
    "   - Represents each word as a **vector** with a **dimensionality** equal to the **vocabulary size**.\n",
    "   - Each entry in the vector corresponds to the **number of times** the word appears in a **document** or **corpus**.\n",
    "   - For the sentence **\"I love AI, AI loves me\"** with a vocabulary of `{I, love, AI, loves, me}`, the count vectors are:\n",
    "      - `I : [1, 0, 0, 0, 0]`\n",
    "      - `AI: [0, 0, 2, 0, 0]`\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/original/we/frequency-count-vectorization.svg\" alt=\"frequency-count-vectorization.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Frequency-Based: Count Vectorization</figcaption>\n",
    "</figure>\n",
    "\n",
    "üìà **Advantages**:\n",
    "   - Simple and easy to implement\n",
    "\n",
    "üìâ **Disadvantages**:\n",
    "   - Most entries are **zero** for large vocabularies.\n",
    "   - Fails to capture **relationships** between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The cat sat on the mat\",             # Document 1\n",
    "    \"The dog sat on the mat\",             # Document 2\n",
    "    \"The cat and the dog played outside\"  # Document 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and convert to lowercase\n",
    "def preprocess(corpus: list) -> list:\n",
    "    return [doc.lower().split() for doc in corpus]\n",
    "\n",
    "# count vectorization\n",
    "def count_vectorization(corpus: list) ->dict[str, list[int]]:\n",
    "    docs = preprocess(corpus)\n",
    "    unique_vocabs = set(word for doc in docs for word in doc)\n",
    "    word_counts = {word: [doc.count(word) for doc in docs] for word in unique_vocabs}\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorization values:\n",
      "  mat     : [1, 1, 0]\n",
      "  the     : [2, 2, 2]\n",
      "  cat     : [1, 0, 1]\n",
      "  on      : [1, 1, 0]\n",
      "  played  : [0, 0, 1]\n",
      "  dog     : [0, 1, 1]\n",
      "  outside : [0, 0, 1]\n",
      "  sat     : [1, 1, 0]\n",
      "  and     : [0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# pre-process the raw corpus\n",
    "pp_corpus = preprocess(corpus)\n",
    "len_max_word = len(max(pp_corpus, key=len))\n",
    "\n",
    "# calculate Count Vectorization for the entire corpus\n",
    "word_counts = count_vectorization(corpus)\n",
    "\n",
    "# log\n",
    "print(\"Count Vectorization values:\")\n",
    "for term, counts in word_counts.items():\n",
    "    print(f\"  {term:{len_max_word + 1}}: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "   - Improves on **raw counts** by assigning **weights** to words based on their **importance** in the document relative to the entire corpus.\n",
    "   - **TF (Term Frequency)**: Frequency of word $t$ in document $d$.\n",
    "   - **IDF (Inverse Document Frequency)**: Importance of word $t$ across all documents.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/original/we/frequency-tf-idf.svg\" alt=\"frequency-tf-idf.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Frequency-Based: TF-IDF</figcaption>\n",
    "</figure>\n",
    "\n",
    "üî¨ **Formula**:\n",
    "   1. **Term Frequency (TF)**\n",
    "      $$\\text{TF}(t, d) = \\frac{\\text{Count of term t in document d}}{\\text{Total terms in document d}}$$\n",
    "   1. **Inverse Document Frequency (IDF)**\n",
    "      $$\\text{IDF}(t) = \\log \\left( \\frac{N + 1}{\\text{DF}(t) + 1} \\right)$$\n",
    "   1. **Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
    "      $$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "where $N$ is the total number of documents, and $\\text{DF}(t)$ is the number of documents containing $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The cat sat on the mat\",             # Document 1\n",
    "    \"The dog sat on the mat\",             # Document 2\n",
    "    \"The cat and the dog played outside\"  # Document 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and convert to lowercase\n",
    "def preprocess(corpus: list) -> list:\n",
    "    return [doc.lower().split() for doc in corpus]\n",
    "\n",
    "# calculate Term Frequency (TF)\n",
    "def term_frequency(doc: str, term: str) -> float:\n",
    "    term_count = doc.count(term)\n",
    "    return term_count / len(doc)\n",
    "\n",
    "# calculate Inverse Document Frequency (IDF)\n",
    "def inverse_document_frequency(corpus: list, term: str) -> float:\n",
    "    num_docs_with_term = sum(1 for doc in corpus if term in doc)\n",
    "    return math.log((len(corpus) + 1) / (num_docs_with_term + 1))\n",
    "\n",
    "# calculate TF-IDF for each term in the corpus\n",
    "def calculate_tf_idf(corpus: list)  -> tuple[dict[str, list[float]], dict[str, float], dict[str, torch.Tensor]]:\n",
    "    terms = set(word for doc in corpus for word in doc)  # get unique terms from all documents\n",
    "    tf_values = {}\n",
    "    idf_values = {}\n",
    "    tf_idf_values = {}\n",
    "    \n",
    "    # calculate TF and IDF for all terms\n",
    "    for term in terms:\n",
    "        tf_values[term] = [term_frequency(doc, term) for doc in corpus]\n",
    "        idf_values[term] = inverse_document_frequency(corpus, term)\n",
    "        tf_idf_values[term] = torch.tensor(tf_values[term]) * idf_values[term]\n",
    "    \n",
    "    return tf_values, idf_values, tf_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency (TF) values:\n",
      "  mat     : ['0.1667', '0.1667', '0.0000']\n",
      "  the     : ['0.3333', '0.3333', '0.2857']\n",
      "  cat     : ['0.1667', '0.0000', '0.1429']\n",
      "  on      : ['0.1667', '0.1667', '0.0000']\n",
      "  played  : ['0.0000', '0.0000', '0.1429']\n",
      "  dog     : ['0.0000', '0.1667', '0.1429']\n",
      "  outside : ['0.0000', '0.0000', '0.1429']\n",
      "  sat     : ['0.1667', '0.1667', '0.0000']\n",
      "  and     : ['0.0000', '0.0000', '0.1429']\n",
      "\n",
      "Inverse Document Frequency (IDF) values:\n",
      "  mat     : 0.2877\n",
      "  the     : 0.0000\n",
      "  cat     : 0.2877\n",
      "  on      : 0.2877\n",
      "  played  : 0.6931\n",
      "  dog     : 0.2877\n",
      "  outside : 0.6931\n",
      "  sat     : 0.2877\n",
      "  and     : 0.6931\n",
      "\n",
      "TF-IDF values:\n",
      "  mat     : ['0.0479', '0.0479', '0.0000']\n",
      "  the     : ['0.0000', '0.0000', '0.0000']\n",
      "  cat     : ['0.0479', '0.0000', '0.0411']\n",
      "  on      : ['0.0479', '0.0479', '0.0000']\n",
      "  played  : ['0.0000', '0.0000', '0.0990']\n",
      "  dog     : ['0.0000', '0.0479', '0.0411']\n",
      "  outside : ['0.0000', '0.0000', '0.0990']\n",
      "  sat     : ['0.0479', '0.0479', '0.0000']\n",
      "  and     : ['0.0000', '0.0000', '0.0990']\n"
     ]
    }
   ],
   "source": [
    "# pre-process the raw corpus\n",
    "pp_corpus = preprocess(corpus)\n",
    "len_max_word = len(max(pp_corpus, key=len))\n",
    "\n",
    "# calculate TF, IDF, and TF-IDF for the entire corpus\n",
    "tf_values, idf_values, tf_idf_values = calculate_tf_idf(pp_corpus)\n",
    "\n",
    "# log\n",
    "print(\"Term Frequency (TF) values:\")\n",
    "for term, tf in tf_values.items():\n",
    "    print(f\"  {term:{len_max_word + 1}}: {[f'{t:.4f}' for t in tf]}\")\n",
    "\n",
    "print(\"\\nInverse Document Frequency (IDF) values:\")\n",
    "for term, idf in idf_values.items():\n",
    "    print(f\"  {term:{len_max_word + 1}}: {idf:.4f}\")\n",
    "\n",
    "print(\"\\nTF-IDF values:\")\n",
    "for term, tf_idf in tf_idf_values.items():\n",
    "    print(f\"  {term:{len_max_word + 1}}: {[f'{t:.4f}' for t in tf_idf.tolist()]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Initialization\n",
    "   - It maps **words** into **dense vectors of fixed size** (initialized with random values), capturing **semantic** meanings and **relationships** in **various contexts**.\n",
    "   - Unlike one-hot encoding, embeddings are **continuous-valued** and **compact representations**.\n",
    "   - The vectors are updated using some downstream task, like a classification task, so that words with similar meanings would be closer together in the vector space.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/original/we/word-embedding.svg\" alt=\"word-embedding.svg\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">Traditional Word Embedding using Neural Networks</figcaption>\n",
    "</figure>\n",
    "\n",
    "üìú **Properties**:\n",
    "   - **Low Dimensionality**\n",
    "      - Vectors typically have dimensions like **50**, **100**, or **300**, **regardless of vocabulary size**.\n",
    "   - **Semantic Relationships**\n",
    "      - Words with similar **meanings** or **contexts** have similar **vector representations**.\n",
    "      - Example: **\"king\"** and **\"queen\"** might have vectors that are close in the **embedding space**.\n",
    "   - **Learned Representations**\n",
    "      - Embeddings are **learned from data**, capturing nuanced meanings based on word co-occurrences.\n",
    "\n",
    "üìà **Advantages**:\n",
    "   - **Lower** **memory** and **computational** requirements compared to one-hot encoding.\n",
    "   - Captures **semantic** relationships and contextual nuances.\n",
    "   - Pre-trained embeddings (like [**GloVe**](https://nlp.stanford.edu/projects/glove/) or [**FastText**](https://fasttext.cc/)) can be used across multiple tasks.\n",
    "\n",
    "üìâ **Limitations**:\n",
    "   - Embeddings may not generalize well if the training data is biased or limited\n",
    "   - Pre-trained embeddings struggle with unseen words, though methods like FastText address this by considering subword information.\n",
    "      - **GloVe & Word2Vec**\n",
    "         - These embeddings are **static** and **tied** directly to the words in the training corpus.\n",
    "         - If a word **does not appear** in the training corpus, the model **cannot** generate an **embedding** for it.\n",
    "         - Because they rely on **co-occurrence statistics** (or local context), and if a word is absent from the training data, the model has no data to create a meaningful representation.\n",
    "         - This results in a failure to handle **out-of-vocabulary (OOV)** words.\n",
    "      - **FastText**\n",
    "         - It represents each word as a bag of character n-grams (subword units).\n",
    "         - For example, the word \"playing\" could be broken down into subword units like:\n",
    "            - \"pla\", \"lay\", \"ayi\", \"yin\", \"ing\" (and also prefixes and suffixes like \"pl\", \"ing\", etc.)\n",
    "            - This is subword-level information that captures word morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"cats like to chase mice\",           # Document 1\n",
    "    \"dogs like to chase cats\",           # Document 2\n",
    "    \"mice like to chase dogs\",           # Document 3\n",
    "    \"cats and dogs are pets\",            # Document 4\n",
    "    \"mice are small and quick\",          # Document 5\n",
    "    \"dogs are loyal and friendly\",       # Document 6\n",
    "    \"cats are independent and curious\",  # Document 7\n",
    "    \"mice are often found in fields\",    # Document 8\n",
    "    \"pets bring joy to their owners\",    # Document 9\n",
    "    \"dogs and cats can be friends\",      # Document 10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the corpus and build a vocabulary\n",
    "vocabulary = {word for sentence in corpus for word in sentence.split()}\n",
    "vocab_size = len(vocabulary)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# hyperparameters\n",
    "embedding_dim = 10  # each word will be represented by 10 numbers\n",
    "context_size = 2    # number of context words to predict the next word\n",
    "learning_rate = 0.001\n",
    "epochs = 120\n",
    "\n",
    "# prepare training data (pairs of context words and target word)\n",
    "context_words = []\n",
    "target_words = []\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for i in range(context_size, len(words)):\n",
    "\n",
    "        # context = previous 'context_size' words\n",
    "        context = words[i - context_size:i]\n",
    "        target = words[i]\n",
    "        \n",
    "        # convert words to indices\n",
    "        context_indices = [word_to_idx[word] for word in context]\n",
    "        target_index = word_to_idx[target]\n",
    "        \n",
    "        context_words.append(context_indices)\n",
    "        target_words.append(target_index)\n",
    "\n",
    "# convert to tensors\n",
    "context_tensor = torch.tensor(context_words, dtype=torch.long)\n",
    "target_tensor = torch.tensor(target_words, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordPredictionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(NextWordPredictionModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim * context_size, vocab_size)\n",
    "    \n",
    "    def forward(self, context):\n",
    "        embedded = self.embeddings(context)  # embedding layer [lookup table]\n",
    "        embedded = embedded.view(1, -1)      # flatten the embeddings\n",
    "        output = self.fc(embedded)           # fully connected layer\n",
    "        return output\n",
    "\n",
    "# instantiate the model, loss function, and optimizer\n",
    "model = NextWordPredictionModel(vocab_size, embedding_dim)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/120 -> Loss: 106.17169380187988\n",
      "Epoch 011/120 -> Loss: 97.98147189617157\n",
      "Epoch 021/120 -> Loss: 90.42929065227509\n",
      "Epoch 031/120 -> Loss: 83.52974367141724\n",
      "Epoch 041/120 -> Loss: 77.28132420778275\n",
      "Epoch 051/120 -> Loss: 71.66597467660904\n",
      "Epoch 061/120 -> Loss: 66.64803922176361\n",
      "Epoch 071/120 -> Loss: 62.175538301467896\n",
      "Epoch 081/120 -> Loss: 58.18662166595459\n",
      "Epoch 091/120 -> Loss: 54.6183876991272\n",
      "Epoch 101/120 -> Loss: 51.413461446762085\n",
      "Epoch 111/120 -> Loss: 48.52277076244354\n",
      "Epoch 120/120 -> Loss: 46.156003057956696\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(context_tensor)):\n",
    "        context = context_tensor[i]  # x\n",
    "        target = target_tensor[i]    # y_true\n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        output = model(context)\n",
    "        \n",
    "        # backward\n",
    "        loss = loss_fn(output, target.unsqueeze(dim=0))\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # store loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # log\n",
    "    if epoch % 10 == 0 or (epoch + 1) == epochs:\n",
    "        print(f\"Epoch {epoch + 1:03}/{epochs} -> Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: dogs        , Embedding: tensor([ 1.9137,  1.4560,  0.8759, -2.1305,  0.7160, -1.2763, -0.0435, -1.6225, -0.7886,  1.6386])\n",
      "Word: joy         , Embedding: tensor([-0.3999, -1.4019, -0.7463, -0.5884, -0.7627,  0.7427,  1.6566, -0.1580, -0.4732,  0.4605])\n",
      "Word: found       , Embedding: tensor([-0.7651,  1.0983,  0.8069,  1.7188,  1.2894,  1.3102,  0.5997,  1.3287, -0.2605,  0.0381])\n",
      "Word: curious     , Embedding: tensor([-0.2516,  0.8599, -1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057, -0.7746])\n",
      "Word: are         , Embedding: tensor([-1.6074,  1.0477, -0.8846, -0.6043, -1.3506,  2.2575, -1.2085, -0.4120, -1.0048, -0.7248])\n",
      "Word: quick       , Embedding: tensor([ 0.0780,  0.5258, -0.4880,  1.1914, -0.8140, -0.7360, -1.4032,  0.0360, -0.0635,  0.6756])\n",
      "Word: be          , Embedding: tensor([-0.1146,  1.8441, -1.1611,  1.4067,  1.4701,  0.8668,  2.2431,  0.5206,  0.3355, -0.1853])\n",
      "Word: their       , Embedding: tensor([-1.0755,  1.3009, -0.1542,  0.5065,  0.0569,  0.4010,  0.5616, -0.6415, -2.2097, -0.7480])\n",
      "Word: owners      , Embedding: tensor([ 0.0109, -0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,  0.7440, -0.4816])\n",
      "Word: friends     , Embedding: tensor([-1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835, -2.5095,  0.4880,  0.7846,  0.0286])\n",
      "Word: cats        , Embedding: tensor([ 0.5734,  0.5587,  1.0665, -0.5391, -0.1917,  0.7223,  0.4121,  0.1865,  0.2621,  1.2949])\n",
      "Word: like        , Embedding: tensor([ 0.0585, -0.3956, -1.5531, -0.1609, -0.6007,  0.4951,  0.8656,  0.1237, -0.4140,  0.6391])\n",
      "Word: mice        , Embedding: tensor([-0.0224,  0.2026,  0.1301,  0.7960,  1.1003,  0.3281,  0.7282,  0.4090,  1.9364,  1.0183])\n",
      "Word: pets        , Embedding: tensor([-1.4538, -1.1401, -0.1386,  1.6369,  0.6585,  0.5749,  1.1404,  0.0053, -1.8281,  0.9476])\n",
      "Word: can         , Embedding: tensor([-0.3993,  1.0505, -0.6914,  0.6383, -0.9917,  0.9602,  1.6650,  1.4508,  0.3037, -0.2323])\n",
      "Word: in          , Embedding: tensor([-0.7259,  0.0860,  0.3404,  0.9919, -0.4625,  1.6135, -2.4994, -0.4306, -1.2051,  0.8152])\n",
      "Word: bring       , Embedding: tensor([-1.9245,  0.2381,  0.0221, -0.3563,  0.3187, -0.7455,  0.1790, -1.0896, -1.5990,  1.3731])\n",
      "Word: often       , Embedding: tensor([ 1.2769,  0.0425, -1.5867,  0.7423,  0.8137,  2.0240,  0.0314,  0.1225, -0.8005, -0.2171])\n",
      "Word: to          , Embedding: tensor([-0.9349, -1.7018, -1.1493, -0.5829, -0.5085, -1.5664, -2.0005,  0.1710,  1.0682, -0.5565])\n",
      "Word: small       , Embedding: tensor([ 0.7352,  0.7171,  1.8006, -0.9366,  0.9800, -0.3408, -1.1749,  0.3749,  0.4966,  1.3698])\n",
      "Word: independent , Embedding: tensor([ 0.5582,  2.1229, -0.5278, -0.9309,  0.1894,  1.0709,  1.2915,  0.4341, -0.8132, -1.0438])\n",
      "Word: friendly    , Embedding: tensor([-0.4949, -0.5923,  0.1543,  0.4408, -0.1483, -2.3184, -0.3980,  1.0805, -1.7809,  1.5080])\n",
      "Word: chase       , Embedding: tensor([ 0.3186, -0.4710,  1.0931,  1.7103, -0.0197,  1.6489,  0.1441, -1.0767, -0.5490,  0.0793])\n",
      "Word: and         , Embedding: tensor([ 0.4740,  2.0402, -0.0123, -0.8336, -2.0881, -1.0798,  0.0321,  0.0195,  0.1929,  0.4176])\n",
      "Word: fields      , Embedding: tensor([-0.9291,  0.2762, -0.5389,  0.4626, -1.4790,  0.4323, -0.1250,  0.7821,  0.5635,  1.8582])\n",
      "Word: loyal       , Embedding: tensor([ 1.0905, -0.8531,  1.3012,  0.2725, -1.9701,  0.0146, -1.4297, -1.9089, -0.1609,  0.7722])\n"
     ]
    }
   ],
   "source": [
    "len_max_word = len(max(vocabulary, key=len))\n",
    "\n",
    "# extract the learned word embeddings\n",
    "word_embeddings = model.embeddings.weight.data\n",
    "\n",
    "# display word embeddings for each word in the vocabulary\n",
    "for idx, word in idx_to_word.items():\n",
    "    print(f\"Word: {word:{len_max_word + 1}}, Embedding: {word_embeddings[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute similarity (cosine similarity)\n",
    "def calculate_similarity(vec1, vec2):\n",
    "    return torch.cosine_similarity(vec1.unsqueeze(dim=0), vec2.unsqueeze(dim=0))\n",
    "\n",
    "# create a 2D tensor for similarity values directly\n",
    "similarity_values = torch.zeros((vocab_size, vocab_size))\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        similarity_values[i, j] = calculate_similarity(word_embeddings[i], word_embeddings[j])\n",
    "\n",
    "# plot the heatmap\n",
    "plt.figure(figsize=(24, 18))\n",
    "sns.heatmap(similarity_values, annot=True, fmt=\".1f\", xticklabels=vocabulary, yticklabels=vocabulary, cmap=\"Blues\")\n",
    "plt.title(\"Word Similarity Heatmap\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensionality of word embeddings\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "# plot the embeddings\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, word in enumerate(idx_to_word.values()):\n",
    "    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1])\n",
    "    plt.annotate(word, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))\n",
    "\n",
    "plt.title(\"Word Embeddings Visualization\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec\n",
    "   - A popular method for learning word embeddings.\n",
    "   - It was introduced in the paper [**Efficient Estimation of Word Representations in Vector Space**](https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/4_TF_supervised/notes_slides/1301.3781.pdf) by [*Tomas Mikolov*](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en&oi=sra) in 2013.\n",
    "\n",
    "üèõÔ∏è **Two Main Architectures of Word2Vec**:\n",
    "   - **Skip-gram**\n",
    "      - Predicts the **context words** (surrounding words) given a **target word**.\n",
    "      - Objective: Maximize the probability of context words appearing given the target word.\n",
    "      <figure style=\"text-align: center;\">\n",
    "         <img src=\"../../assets/images/original/we/word2vec-skipgram.svg\" alt=\"word2vec-skipgram.svg\" style=\"width: 100%;\">\n",
    "         <figcaption style=\"text-align: center;\">Word2Vec using Skip-Gram method</figcaption>\n",
    "      </figure>\n",
    "\n",
    "   - **CBOW (Continuous Bag of Words)**\n",
    "      - Predicts a **target word** based on its surrounding **context words**.\n",
    "      - Objective: Maximize the probability of a target word given the context.\n",
    "      <figure style=\"text-align: center;\">\n",
    "         <img src=\"../../assets/images/original/we/word2vec-cbow.svg\" alt=\"word2vec-cbow.svg\" style=\"width: 100%;\">\n",
    "         <figcaption style=\"text-align: center;\">Word2Vec using CBOW method</figcaption>\n",
    "      </figure>\n",
    "\n",
    "üìà **Advantages**:\n",
    "   - It provides better word embedding representations compared to traditional vanilla word embeddings\n",
    "   - Optimized using techniques like **negative sampling** or **hierarchical softmax** to improve efficiency at training stage.\n",
    "      - **Negative Sampling**\n",
    "         - Focuses on distinguishing observed (**positive**) word-context pairs from randomly sampled (**negative**) pairs (**around 2-20 pairs**).\n",
    "\n",
    "üìâ **Limitations**:\n",
    "   - **Static Embeddings**\n",
    "      - Produces the **same** vector for a word, regardless of its **context**.\n",
    "      - For example, **\"bark\"** has the same embedding whether referring to the **outer covering of a tree** or the **sound a dog makes**.\n",
    "   - **OOV Words**\n",
    "      - Struggles with **unseen** words in the test set unless extended methods like **FastText** are used.\n",
    "   - These limitations are overcome by **contextual embeddings** (e.g., [**BERT**](https://github.com/google-research/bert), [**GPT-3**](https://github.com/openai/gpt-3))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GloVe (Global Vectors for Word Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FastText (Subword Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ELMo (Embeddings from Language Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BERT (Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Matrix Factorization (Latent Semantic Analysis - LSA)\n",
    "   - **Matrix factorization** is a technique used in **Latent Semantic Analysis (LSA)** to uncover hidden structures in large text corpora.\n",
    "   - It aims to capture the **semantic relationships** between words and documents by decomposing a large **term-document matrix** into a smaller **latent semantic space**.\n",
    "   - It uses **singular value decomposition (SVD)** to reduce the dimensionality of the term-document matrix, revealing patterns and associations between terms and documents.\n",
    "\n",
    "üìú **Properties**:\n",
    "   - **Dimensionality Reduction**\n",
    "      - The technique reduces the **high-dimensional** term-document matrix into a **lower-dimensional representation** while retaining key semantic information.\n",
    "   - **Latent Semantic Structure**\n",
    "      - LSA uncovers **hidden relationships** and **topics** within the data that are not immediately visible in the raw term-document matrix.\n",
    "   - **Singular Value Decomposition (SVD)**\n",
    "      - The matrix is factorized into three matrices: **U**, **Œ£**, and **V**, where:\n",
    "         - **U**: Term matrix (words),\n",
    "         - **Œ£**: Singular values (importance),\n",
    "         - **V**: Document matrix.\n",
    "\n",
    "üìà **Advantages**:\n",
    "   - **Captures Synonymy**\n",
    "      - LSA can recognize words with similar meanings even if they don't appear together frequently in the documents.\n",
    "      - Example: **\"car\"** and **\"automobile\"** might be clustered together in the **latent space** despite not often appearing in the same context.\n",
    "   - **Dimensionality Reduction**\n",
    "      - LSA simplifies the data while preserving key semantic information, reducing both **memory usage** and **computation time**.\n",
    "   - **Discover Topics**\n",
    "      - It can uncover **latent topics** within the corpus, grouping similar documents and terms based on their underlying meaning.\n",
    "   - **No Need for Labeling**\n",
    "      - Unlike supervised learning, LSA does not require labeled data to identify these patterns.\n",
    "\n",
    "üìâ **Limitations**:\n",
    "   - **Does Not Handle Polysemy Well**\n",
    "      - Words with multiple meanings may be grouped together despite having different contexts. For example, **\"bank\"** (riverbank vs. financial institution) could be treated the same.\n",
    "   - **Sparse Matrices**\n",
    "      - LSA relies on large, sparse term-document matrices, which can be **computationally expensive** to create and process.\n",
    "   - **Requires Sufficient Data**\n",
    "      - The quality of the latent semantic space depends heavily on the amount and diversity of the training data.\n",
    "   - **Interpretability Issues**\n",
    "      - The topics or relationships discovered by LSA may be difficult to **interpret** due to the complexity of the latent space and the SVD transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love programming in Python\",               # Document 1\n",
    "    \"Python is a great programming language\",     # Document 2\n",
    "    \"I enjoy data science and machine learning\",  # Document 3\n",
    "    \"Data science is amazing with Python\",        # Document 4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix Shape (term-document matrix): (4, 11)\n",
      "Transformed Matrix Shape (after LSA): (4, 2)\n",
      "\n",
      "Transformed Matrix (LSA representation):\n",
      "[[ 0.69609115 -0.46351513]\n",
      " [ 0.67453765 -0.48738522]\n",
      " [ 0.43283672  0.74122971]\n",
      " [ 0.66586887  0.49645966]]\n",
      "\n",
      "Top terms for each LSA component:\n",
      "Component 1:\n",
      "  python\n",
      "  programming\n",
      "  love\n",
      "\n",
      "Component 2:\n",
      "  data\n",
      "  science\n",
      "  machine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorize the corpus using TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # removing stop words in English : \"the\", \"and\", \"is\", \"in\", \"at\", \"on\", \"of\", \"to\", \"a\", \"for\", etc.\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# apply Latent Semantic Analysis (LSA) using Truncated SVD\n",
    "# n_components is the number of latent semantic dimensions you want to reduce to\n",
    "lsa = TruncatedSVD(n_components=2)\n",
    "X_lsa = lsa.fit_transform(X)\n",
    "\n",
    "# log\n",
    "print(f\"Original Matrix Shape (term-document matrix): {X.shape}\")\n",
    "print(f\"Transformed Matrix Shape (after LSA): {X_lsa.shape}\")\n",
    "print(f\"\\nTransformed Matrix (LSA representation):\\n{X_lsa}\")\n",
    "\n",
    "# examine the topics (terms most related to each component)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "print(\"\\nTop terms for each LSA component:\")\n",
    "for i, topic in enumerate(lsa.components_):\n",
    "    print(f\"Component {i+1}:\")\n",
    "    terms_indices = topic.argsort()[:-4:-1]  # get the top 3 terms for this component\n",
    "    for index in terms_indices:\n",
    "        print(f\"  {terms[index]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the heatmap\n",
    "similarity_matrix = np.dot(X_lsa, X_lsa.T)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarity_matrix, annot=True, cmap='YlGnBu', fmt='.2f', linewidths=0.5, xticklabels=['Doc 1', 'Doc 2', 'Doc 3', 'Doc 4'], yticklabels=['Doc 1', 'Doc 2', 'Doc 3', 'Doc 4'])\n",
    "plt.title(\"Document Similarity Matrix in LSA Space\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
