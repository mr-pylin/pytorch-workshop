{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** amirhosseinheydari78@gmail.com - üìç **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import BCELoss, CrossEntropyLoss, L1Loss, MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "   - A function that quantifies the difference between the predicted output of a model and the true target values\n",
    "   - It serves as a measure of how well (or poorly) the model's predictions align with the actual outcomes, guiding the optimization process during training\n",
    "   - Available Loss Functions in PyTorch: [pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../../assets/images/third_party/loss-function.png\" alt=\"loss-function.png\" style=\"width: 100%;\">\n",
    "    <figcaption style=\"text-align: center;\">¬©Ô∏è Image: <a href= \"https://www.offconvex.org/2016/03/22/saddlepoints\">offconvex.org/2016/03/22/saddlepoints</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression tasks\n",
    "1. [Mean Absolute Error](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) (`torch.nn.L1Loss`)\n",
    "   - Measures the mean absolute error (MAE) between each element in the input and target tensors (aka L1 norm)\n",
    "   - Robust to outliers BUT does not provide gradients for large errors, leading to slower convergence\n",
    "   - **Formula**:\n",
    "      - $\\text{L1Loss} = \\frac{1}{N} \\sum_{i=1}^{N} |\\hat{y}_i - y_i|$\n",
    "   - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $\\hat{y}_i$: Predicted value for the $i_{th}$ sample\n",
    "      - $y_i$: True value for the $i_{th}$ sample\n",
    "\n",
    "1. [Mean Squared Error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) (`torch.nn.MSELoss`)\n",
    "   - Measures the mean squared error (MSE) between each element in the input and target tensors (aka L2 norm)\n",
    "   - Sensitive to outliers because it penalizes large errors quadratically (due to squaring)\n",
    "   - **Formula**:\n",
    "      - $\\text{MSELoss} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$\n",
    "   - **Notations**:\n",
    "      - \\(N\\): Number of samples\n",
    "      - \\(\\hat{y}_i\\): Predicted value for the \\(i\\)-th sample\n",
    "      - \\(y_i\\): True value for the \\(i\\)-th sample\n",
    "\n",
    "1. [Huber](https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html) (`torch.nn.HuberLoss`)\n",
    "   - Combines the best properties of MAE and MSE losses by being less sensitive to outliers than MSE and more stable than MAE\n",
    "   - It acts `quadratic` for small errors (similar to MSE) and `linear` for large errors (similar to MAE)\n",
    "   - **Formula**:\n",
    "      - $\n",
    "        \\text{HuberLoss} = \\frac{1}{N} \\sum_{i=1}^{N} \n",
    "        \\begin{cases} \n",
    "        0.5 (\\hat{y}_i - y_i)^2 & \\text{if } |\\hat{y}_i - y_i| < \\delta \\\\\n",
    "        \\delta \\cdot (|\\hat{y}_i - y_i| - 0.5 \\cdot \\delta) & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "        $\n",
    "   - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $\\hat{y}_i$: Predicted value for the $i_{th}$ sample\n",
    "      - $y_i$: True value for the $i_{th}$ sample\n",
    "      - $\\delta$: Threshold parameter\n",
    "\n",
    "1. [Smooth L1](https://pytorch.org/docs/main/generated/torch.nn.SmoothL1Loss.html) (`torch.nn.SmoothL1Loss`)\n",
    "   - It provides a smooth transition between quadratic and linear behavior, unlike Huber Loss which has a sharp cutoff\n",
    "   - Often used in object detection tasks\n",
    "   - **Formula**:\n",
    "      - $\n",
    "        \\text{SmoothL1Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \n",
    "        \\begin{cases} \n",
    "        0.5 \\cdot \\frac{(\\hat{y}_i - y_i)^2}{\\beta} & \\text{if } |\\hat{y}_i - y_i| < \\beta \\\\\n",
    "        |\\hat{y}_i - y_i| - 0.5 \\cdot \\beta & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "        $\n",
    "   - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $\\hat{y}_i$: Predicted value for the \\hat{y}_i sample\n",
    "      - $y_i$: True value for the \\hat{y}_i sample\n",
    "      - $\\beta$: Threshold parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification tasks\n",
    "1. **Binary Classification**\n",
    "   1. [Binary Cross-Entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) (`torch.nn.BCELoss`)\n",
    "      - Measures the binary cross-entropy loss between the target and the input probabilities (`Sigmoid`).\n",
    "      - Penalizes incorrect predictions more heavily, especially when the predicted probability is far from the actual class.\n",
    "      - **Formula**:\n",
    "         - $\\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]$\n",
    "      - **Notations**:\n",
    "         - $N$: Number of samples\n",
    "         - $y_i$: True label for the $i_{th}$ sample (0 or 1)\n",
    "         - $p_i$: Predicted probability for the $i_{th}$ sample\n",
    "   \n",
    "   1. [Binary Cross-Entropy with Logits](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) (`torch.nn.BCEWithLogitsLoss`)\n",
    "      - Measures the binary cross-entropy loss between the target and the input logits.\n",
    "      - Combines a `torch.nn.Sigmoid` and `torch.nn.BCELoss` in one single class.\n",
    "      - More numerically stable than using a plain sigmoid followed by a binary cross-entropy loss.\n",
    "      - **Formula**:\n",
    "         - $\\text{BCEWithLogits} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\sigma(\\hat{y}_i)) + (1 - y_i) \\log(1 - \\sigma(\\hat{y}_i)) \\right]$\n",
    "      - **Notations**:\n",
    "         - $N$: Number of samples\n",
    "         - $y_i$: True label for the $i_{th}$ sample (0 or 1)\n",
    "         - $\\hat{y}_i$: Logit (raw model output) for the $i_{th}$ sample\n",
    "         - $\\sigma(\\hat{y}_i)$: Sigmoid function applied to $\\hat{y}_i$, i.e., $\\sigma(\\hat{y}_i) = \\frac{1}{1 + e^{- \\hat{y}_i}}$\n",
    "   \n",
    "   1. [Soft Margin](https://pytorch.org/docs/main/generated/torch.nn.SoftMarginLoss.html) (`torch.nn.SoftMarginLoss`)\n",
    "      - Measures the logistic loss between the target and the input.\n",
    "      - Expects target values to be either 1 or -1.\n",
    "      - **Formula**:\n",
    "         - $\\text{SoftMarginLoss} = \\frac{1}{N} \\sum_{i=1}^{N} \\log(1 + \\exp(-y_i \\hat{y}_i))$\n",
    "      - **Notations**:\n",
    "         - $N$: Number of samples\n",
    "         - $y_i$: True label for the $i_{th}$ sample (1 or -1)\n",
    "         - $\\hat{y}_i$: Logit (raw model output) for the $i_{th}$ sample\n",
    "\n",
    "1. **Multiclass Classification**\n",
    "   1. [Negative Log Likelihood](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) (`torch.nn.NLLLoss`)\n",
    "      - Measures the negative log likelihood loss between the target and the input log-probabilities (`LogSoftmax`).\n",
    "      - Directly applying `LogSoftmax` to logits can lead to numerical instability (issues of overflow and underflow in computational systems).\n",
    "      - **Formula**:\n",
    "         - $\\text{NLLLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(\\hat{y}_{i, y_i})$\n",
    "      - **Notations**:\n",
    "         - $N$: Number of samples\n",
    "         - $\\hat{y}_{i, y_i}$: Log-probability of the true class $y_i$ for the $i_{th}$ sample\n",
    "   \n",
    "   1. [Cross-Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (`torch.nn.CrossEntropyLoss`)\n",
    "      - Measures the cross-entropy loss between the target and the input logits.\n",
    "      - Combines a `torch.nn.LogSoftmax` and `torch.nn.NLLLoss` in one single class.\n",
    "      - It reduces the number of operations required compared to applying sigmoid and BCELoss separately\n",
    "      - **Formula**:\n",
    "         - $\\text{CrossEntropyLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left(\\frac{\\exp(\\hat{y}_{i, y_i})}{\\sum_{j=1}^{C} \\exp(\\hat{y}_{i, j})}\\right)$\n",
    "      - **Notations**:\n",
    "         - $N$: Number of samples\n",
    "         - $C$: Number of classes\n",
    "         - $\\hat{y}_{i, y_i}$: Log-probability of the true class $y_i$ for the $i_{th}$ sample\n",
    "         - $\\hat{y}_{i, j}$: Logit (raw model output) for class $j$ of the $i_{th}$ sample\n",
    "         - $y_i$: True class for the $i_{th}$ sample\n",
    "   \n",
    "   1. [Multi-Label Soft Margin](https://pytorch.org/docs/main/generated/torch.nn.MultiLabelSoftMarginLoss.html) (`torch.nn.MultiLabelSoftMarginLoss`)\n",
    "      - Measures the multi-label one-versus-all loss based on max-entropy between the target and the input.\n",
    "      - Useful for multi-label classification tasks where each sample can belong to multiple classes.\n",
    "      - **Formula**:\n",
    "         - $\\text{MultiLabelSoftMarginLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{C} \\sum_{j=1}^{C} \\left[ y_{i, j} \\log(\\sigma(\\hat{y}_{i, j})) + (1 - y_{i, j}) \\log(1 - \\sigma(\\hat{y}_{i, j})) \\right]$\n",
    "      - **Notations**:\n",
    "         - $N$: Number of samples\n",
    "         - $C$: Number of classes\n",
    "         - $y_{i, j}$: True label for class $j$ of the $i_{th}$ sample (0 or 1)\n",
    "         - $\\hat{y}_{i, j}$: Logit (raw model output) for class $j$ of the $i_{th}$ sample\n",
    "         - $\\sigma(\\hat{y}_{i, j})$: Sigmoid function applied to $\\hat{y}_{i, j}$, i.e., $\\sigma(\\hat{y}_{i, j}) = \\frac{1}{1 + e^{-\\hat{y}_{i, j}}}$\n",
    "\n",
    "1. **Specialized Classification**\n",
    "   1. [Connectionist Temporal Classification](https://pytorch.org/docs/main/generated/torch.nn.CTCLoss.html) (`torch.nn.CTCLoss`)\n",
    "   1. [Poisson Negative log likelihood](https://pytorch.org/docs/main/generated/torch.nn.PoissonNLLLoss.html) (`torch.nn.PoissonNLLLoss`)\n",
    "   1. [Gaussian negative log likelihood](https://pytorch.org/docs/main/generated/torch.nn.GaussianNLLLoss.html) (`torch.nn.GaussianNLLLoss`)\n",
    "\n",
    "1. **Ranking and Embedding**\n",
    "   1. [Margin Ranking](https://pytorch.org/docs/main/generated/torch.nn.MarginRankingLoss.html) (`torch.nn.MarginRankingLoss`)\n",
    "   1. [Hinge Embedding](https://pytorch.org/docs/main/generated/torch.nn.HingeEmbeddingLoss.html) (`torch.nn.HingeEmbeddingLoss`)\n",
    "   1. [Cosine Embedding](https://pytorch.org/docs/main/generated/torch.nn.CosineEmbeddingLoss.html) (`torch.nn.CosineEmbeddingLoss`)\n",
    "   1. [Multi Margin](https://pytorch.org/docs/main/generated/torch.nn.MultiMarginLoss.html) (`torch.nn.MultiMarginLoss`)\n",
    "   1. [Triplet Margin](https://pytorch.org/docs/main/generated/torch.nn.TripletMarginLoss.html) (`torch.nn.TripletMarginLoss`)\n",
    "   1. [Triplet Margin With Distance](https://pytorch.org/docs/main/generated/torch.nn.TripletMarginWithDistanceLoss.html) (`torch.nn.TripletMarginWithDistanceLoss`)\n",
    "\n",
    "1. **Other**\n",
    "   1. [Kullback-Leibler divergence](https://pytorch.org/docs/main/generated/torch.nn.KLDivLoss.html) (`torch.nn.KLDivLoss`)\n",
    "   1. [Multi-Label Margin](https://pytorch.org/docs/main/generated/torch.nn.MultiLabelMarginLoss.html) (`torch.nn.MultiLabelMarginLoss`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCELoss vs MSELoss\n",
    "   - BCELoss is more sensitive to the amount of error (grows faster if the distance between `y_true` & `y_pred` is high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: tensor([0., 0., 0.])\n",
      "y_pred: tensor([0.5000, 0.7500, 1.0000])\n",
      "--------------------------------------------------\n",
      "MSELoss [per sample]: tensor([0.2500, 0.5625, 0.9999])\n",
      "MSELoss             : 0.60414\n",
      "BCELoss [per sample]: tensor([0.6931, 1.3863, 9.9996])\n",
      "BCELoss             : 4.02635\n"
     ]
    }
   ],
   "source": [
    "# we have 3 samples for a binary classification\n",
    "y_true = torch.tensor([[0], [0], [0]], dtype=torch.float32)\n",
    "\n",
    "# output of model_1\n",
    "output = torch.tensor([[0], [1.09864], [10]], dtype=torch.float32)\n",
    "y_pred = torch.sigmoid(output)\n",
    "\n",
    "mse_1 = MSELoss(reduction='none')(y_pred, y_true).squeeze()\n",
    "mse_2 = MSELoss()(y_pred, y_true)\n",
    "bce_1 = BCELoss(reduction='none')(y_pred, y_true).squeeze()\n",
    "bce_2 = BCELoss()(y_pred, y_true)\n",
    "\n",
    "# log\n",
    "print(f\"y_true: {y_true.squeeze()}\")\n",
    "print(f\"y_pred: {y_pred.squeeze()}\")\n",
    "print('-' * 50)\n",
    "print(f\"MSELoss [per sample]: {mse_1}\")\n",
    "print(f\"MSELoss             : {mse_2:.5f}\")\n",
    "print(f\"BCELoss [per sample]: {bce_1}\")\n",
    "print(f\"BCELoss             : {bce_2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "y_true = torch.zeros(size=(100, 1))\n",
    "y_pred = torch.sigmoid(torch.linspace(-10, +10, 100).reshape(-1, 1))\n",
    "bce_loss = BCELoss(reduction='none')(y_pred, y_true)\n",
    "mse_loss = MSELoss(reduction='none')(y_pred, y_true)\n",
    "\n",
    "plt.plot(y_pred, bce_loss, label='BCELoss')\n",
    "plt.plot(y_pred, mse_loss, label='MSELoss')\n",
    "plt.title(f\"y_true = {y_true[0, 0]}   |   {y_pred.min().round()} <= y_pred <= {y_pred.max().round()}\")\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
