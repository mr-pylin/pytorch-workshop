{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** amirhosseinheydari78@gmail.com - üìç **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Dataset](#toc2_)    \n",
    "  - [Load Iris Dataset](#toc2_1_)    \n",
    "- [Torch Dataset](#toc3_)    \n",
    "- [Torch DataLoader](#toc4_)    \n",
    "  - [Strategies for updating weights](#toc4_1_)    \n",
    "    - [Batch Gradient Descent](#toc4_1_1_)    \n",
    "    - [Stochastic Gradient Descent](#toc4_1_2_)    \n",
    "    - [Mini-Batch Gradient Descent](#toc4_1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Dataset](#toc0_)\n",
    "\n",
    "$\n",
    "X = \\begin{bmatrix}\n",
    "        x_{1}^1 & x_{1}^2 & \\cdots & x_{1}^n \\\\\n",
    "        x_{2}^1 & x_{2}^2 & \\cdots & x_{2}^n \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        x_{m}^1 & x_{m}^2 & \\cdots & x_{m}^n \\\\\n",
    "    \\end{bmatrix}_{m \\times n} \\quad \\text{(m: number of samples, n: number of features)}\n",
    "$\n",
    "\n",
    "$\n",
    "Y = \\begin{bmatrix}\n",
    "        y_{1} \\\\\n",
    "        y_{2} \\\\\n",
    "        \\vdots \\\\\n",
    "        y_{m} \\\\\n",
    "    \\end{bmatrix}_{m \\times 1} \\quad \\text{(m: number of samples)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Load Iris Dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset_url = (\n",
    "    r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/iris/dataset.csv\"\n",
    ")\n",
    "\n",
    "# pandas data-frame\n",
    "df = pd.read_csv(iris_dataset_url, encoding=\"utf-8\")\n",
    "\n",
    "# log\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df[\"class\"].unique()\n",
    "class_to_idx = {l: i for i, l in enumerate(classes)}\n",
    "\n",
    "# split dataset into features and labels\n",
    "X, y = df.iloc[:, :4].values, df.iloc[:, 4].values\n",
    "\n",
    "# convert categorical labels into indices\n",
    "y = np.array([class_to_idx[l] for l in y])\n",
    "\n",
    "# properties of the dataset\n",
    "num_samples, num_features = X.shape\n",
    "classes, samples_per_class = np.unique(y, return_counts=True)\n",
    "\n",
    "# log\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "print(f\"X.dtype: {X.dtype}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"y.dtype: {y.dtype}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"classes          : {classes}\")\n",
    "print(f\"samples per class: {samples_per_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy.ndarray to torch.Tensor\n",
    "X = torch.from_numpy(X.astype(np.float32))\n",
    "y = torch.from_numpy(y.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "# log\n",
    "print(f\"x.shape: {X.shape}\")\n",
    "print(f\"x.dtype: {X.dtype}\")\n",
    "print(f\"x.ndim : {X.ndim}\\n\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"y.dtype: {y.dtype}\")\n",
    "print(f\"y.ndim : {y.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Torch Dataset](#toc0_)\n",
    "   - It's designed to store and manage tensor-based datasets.\n",
    "   - DataLoaders in PyTorch specifically require a tensor-based Dataset.\n",
    "   - `torch.utils.data.TensorDataset` is indeed a subclass of the `torch.utils.data.Dataset` class in PyTorch.\n",
    "   - To build highly customizable and versatile datasets, refer to [**custom-classes.ipynb**](./custom-classes.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a torch dataset\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "# log\n",
    "print(f\"dataset.tensors[0].shape : {dataset.tensors[0].shape}\")\n",
    "print(f\"dataset.tensors[1].shape : {dataset.tensors[1].shape}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"first sample:\")\n",
    "print(f\"    -> X: {dataset[0][0]}\")\n",
    "print(f\"    -> y: {dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Torch DataLoader](#toc0_)\n",
    "   - A DataLoader(`torch.utils.data.DataLoader`) is a utility for training and evaluation in deep learning tasks that enables:\n",
    "      - efficient loading datasets\n",
    "      - handling batching\n",
    "      - shuffling\n",
    "      - parallel data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Strategies for updating weights](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_1_'></a>[Batch Gradient Descent](#toc0_)\n",
    "   - Uses the **entire** dataset to compute the **gradient** of the loss function and **update** the **weights**.\n",
    "   - **Pros**: Provides a stable convergence.\n",
    "   - **Cons**: Can be very slow and computationally expensive for large datasets.\n",
    "\n",
    "üåü **Example**:\n",
    "| #Epoch | batch size | #batch per epoch                    | #iteration per epoch                |\n",
    "|:------:|:----------:|:-----------------------------------:|:-----------------------------------:|\n",
    "| $ 2 $  | $ 150 $    | $ \\lceil\\frac{150}{150}\\rceil = 1 $ | $ \\lceil\\frac{150}{150}\\rceil = 1 $ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = dataset.tensors[0].shape[0]\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# log\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch+1:0{len(str(epochs))}}/{epochs}\")\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        print(f\"    iteration {i}\")\n",
    "        print(f\"        x.shape: {x.shape}\")\n",
    "        print(f\"        y.shape: {y.shape}\")\n",
    "        print(\"    weights are updated.\")\n",
    "    print(f\"model saw the entire dataset.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_2_'></a>[Stochastic Gradient Descent](#toc0_)\n",
    "   - the model **updates** the **weights** using only **one data point** at a time.\n",
    "   - **Pros**: Faster updates and can escape local minima.\n",
    "   - **Cons**: Can be noisy and may not converge as smoothly as batch gradient descent.\n",
    "\n",
    "üåü **Example**:\n",
    "| #Epoch | batch size | #batch per epoch                    | #iteration per epoch                |\n",
    "|:------:|:----------:|:-----------------------------------:|:-----------------------------------:|\n",
    "| $ 2 $  | $ 1 $      | $ \\lceil\\frac{150}{1}\\rceil = 150 $ | $ \\lceil\\frac{150}{1}\\rceil = 150 $ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 1\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# log\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch+1:0{len(str(epochs))}}/{epochs}\")\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if i % 25 == 0 or i == len(X) - 1:\n",
    "            print(f\"    iteration {i}\")\n",
    "            print(f\"        x.shape: {x.shape}\")\n",
    "            print(f\"        y.shape: {y.shape}\")\n",
    "            print(\"    weights are updated.\")\n",
    "    print(f\"model saw the entire dataset.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_3_'></a>[Mini-Batch Gradient Descent](#toc0_)\n",
    "   - the model updates its weights after processing a small batch of 'm' samples from the training dataset.\n",
    "   - this method combines the advantages of both SGD and Batch Gradient Descent by providing a balance between efficiency and stability during training.\n",
    "\n",
    "üåü **Example**:\n",
    "| #Epoch | batch size | #batch                             | #iteration per epoch               |\n",
    "|:------:|:----------:|:----------------------------------:|:----------------------------------:|\n",
    "| $ 2 $  | $ 32 $     | $ \\lceil\\frac{150}{32}\\rceil = 5 $ | #batch                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 32\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# log\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch+1:0{len(str(epochs))}}/{epochs}\")\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        print(f\"    iteration {i}\")\n",
    "        print(f\"        x.shape: {x.shape}\")\n",
    "        print(f\"        y.shape: {y.shape}\")\n",
    "        print(\"    weights are updated.\")\n",
    "    print(f\"model saw the entire dataset.\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
