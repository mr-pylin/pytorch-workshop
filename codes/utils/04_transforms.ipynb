{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud83d\udcdd **Author:** Amirhossein Heydari - \ud83d\udce7 **Email:** amirhosseinheydari78@gmail.com - \ud83d\udccd **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set a seed for deterministic results\n",
        "random_state = 42\n",
        "torch.manual_seed(random_state)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = CIFAR10(root='./dataset', train=True, transform=None, download=True)\n",
        "\n",
        "x = trainset.data[:3]\n",
        "y = trainset.targets[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4), layout='compressed')\n",
        "for i, (img, label) in enumerate(zip(x, y)):\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].set(title=label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transforms\n",
        "   - [pytorch.org/vision/main/transforms.html](https://pytorch.org/vision/main/transforms.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Transformations\n",
        "   - v2.ToImage\n",
        "   - v2.ToDtype\n",
        "   - v2.Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x[0].shape : (32, 32, 3)\n",
            "x[0].dtype : uint8\n",
            "type(x[0]) : <class 'numpy.ndarray'>\n",
            "x[0].min() : 0\n",
            "x[0].max() : 255\n",
            "--------------------------------------------------\n",
            "x[1].shape : (32, 32, 3)\n",
            "x[1].dtype : uint8\n",
            "type(x[1]) : <class 'numpy.ndarray'>\n",
            "x[1].min() : 5\n",
            "x[1].max() : 254\n",
            "--------------------------------------------------\n",
            "x[2].shape : (32, 32, 3)\n",
            "x[2].dtype : uint8\n",
            "type(x[2]) : <class 'numpy.ndarray'>\n",
            "x[2].min() : 20\n",
            "x[2].max() : 255\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x[{i}].shape : {x[i].shape}\")\n",
        "    print(f\"x[{i}].dtype : {x[i].dtype}\")\n",
        "    print(f\"type(x[{i}]) : {type(x[i])}\")\n",
        "    print(f\"x[{i}].min() : {x[i].min()}\")\n",
        "    print(f\"x[{i}].max() : {x[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.ToImage\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.v2.ToImage.html](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.ToImage.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_2[0].shape : torch.Size([3, 32, 32])\n",
            "x_2[0].dtype : torch.uint8\n",
            "type(x_2[0]) : <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_2[0].min() : 0\n",
            "x_2[0].max() : 255\n",
            "--------------------------------------------------\n",
            "x_2[1].shape : torch.Size([3, 32, 32])\n",
            "x_2[1].dtype : torch.uint8\n",
            "type(x_2[1]) : <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_2[1].min() : 5\n",
            "x_2[1].max() : 254\n",
            "--------------------------------------------------\n",
            "x_2[2].shape : torch.Size([3, 32, 32])\n",
            "x_2[2].dtype : torch.uint8\n",
            "type(x_2[2]) : <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_2[2].min() : 20\n",
            "x_2[2].max() : 255\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "to_image_transform = v2.ToImage()\n",
        "x_2 = [to_image_transform(img) for img in x]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_2[{i}].shape : {x_2[i].shape}\")\n",
        "    print(f\"x_2[{i}].dtype : {x_2[i].dtype}\")\n",
        "    print(f\"type(x_2[{i}]) : {type(x_2[i])}\")\n",
        "    print(f\"x_2[{i}].min() : {x_2[i].min()}\")\n",
        "    print(f\"x_2[{i}].max() : {x_2[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.ToDtype\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.v2.ToDtype.html#torchvision.transforms.v2.ToDtype](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.ToDtype.html#torchvision.transforms.v2.ToDtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_3[0].shape: torch.Size([3, 32, 32])\n",
            "x_3[0].dtype: torch.float32\n",
            "type(x_3[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_3[0].min(): 0.0\n",
            "x_3[0].max(): 1.0\n",
            "--------------------------------------------------\n",
            "x_3[1].shape: torch.Size([3, 32, 32])\n",
            "x_3[1].dtype: torch.float32\n",
            "type(x_3[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_3[1].min(): 0.019607843831181526\n",
            "x_3[1].max(): 0.9960784912109375\n",
            "--------------------------------------------------\n",
            "x_3[2].shape: torch.Size([3, 32, 32])\n",
            "x_3[2].dtype: torch.float32\n",
            "type(x_3[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_3[2].min(): 0.0784313753247261\n",
            "x_3[2].max(): 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "to_dtype_transform = v2.ToDtype(dtype=torch.float32, scale=True)\n",
        "x_3 = [to_dtype_transform(img) for img in x_2]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_3[{i}].shape: {x_3[i].shape}\")\n",
        "    print(f\"x_3[{i}].dtype: {x_3[i].dtype}\")\n",
        "    print(f\"type(x_3[{i}]): {type(x_3[i])}\")\n",
        "    print(f\"x_3[{i}].min(): {x_3[i].min()}\")\n",
        "    print(f\"x_3[{i}].max(): {x_3[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.Normalize\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.v2.Normalize.html#torchvision.transforms.v2.Normalize](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Normalize.html#torchvision.transforms.v2.Normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_4[0].shape: torch.Size([3, 32, 32])\n",
            "x_4[0].dtype: torch.float32\n",
            "type(x_4[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_4[0].min(): -2.086963653564453\n",
            "x_4[0].max(): 1.9853252172470093\n",
            "--------------------------------------------------\n",
            "x_4[1].shape: torch.Size([3, 32, 32])\n",
            "x_4[1].dtype: torch.float32\n",
            "type(x_4[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_4[1].min(): -2.0096473693847656\n",
            "x_4[1].max(): 2.059528112411499\n",
            "--------------------------------------------------\n",
            "x_4[2].shape: torch.Size([3, 32, 32])\n",
            "x_4[2].dtype: torch.float32\n",
            "type(x_4[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_4[2].min(): -1.731309175491333\n",
            "x_4[2].max(): 2.073734760284424\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "mus = np.array(x_3).mean(axis=(0, 2, 3))\n",
        "stds = np.array(x_3).std(axis=(0, 2, 3))\n",
        "\n",
        "normalize_transform = v2.Normalize(mean=mus, std=stds)\n",
        "x_4 = [normalize_transform(img) for img in x_3]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_4[{i}].shape: {x_4[i].shape}\")\n",
        "    print(f\"x_4[{i}].dtype: {x_4[i].dtype}\")\n",
        "    print(f\"type(x_4[{i}]): {type(x_4[i])}\")\n",
        "    print(f\"x_4[{i}].min(): {x_4[i].min()}\")\n",
        "    print(f\"x_4[{i}].max(): {x_4[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x, x_4, y)):\n",
        "    axs[0, i].imshow(img1)\n",
        "    axs[0, i].set(title='Original')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='Normalize(ToDtype(ToImage()))')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Augmentation Techniques\n",
        "   - v2.RandomCrop\n",
        "   - v2.Resize\n",
        "   - v2.RandomVerticalFlip\n",
        "   - v2.RandomHorizontalFlip\n",
        "   - v2.RandomRotation\n",
        "   - v2.ColorJitter\n",
        "   - v2.RandomAffine\n",
        "   - ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.RandomCrop\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_5[0].shape: torch.Size([3, 24, 24])\n",
            "x_5[0].dtype: torch.float32\n",
            "type(x_5[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_5[0].min(): -1.5571343898773193\n",
            "x_5[0].max(): 1.9853252172470093\n",
            "--------------------------------------------------\n",
            "x_5[1].shape: torch.Size([3, 24, 24])\n",
            "x_5[1].dtype: torch.float32\n",
            "type(x_5[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_5[1].min(): -1.9632577896118164\n",
            "x_5[1].max(): 2.045320987701416\n",
            "--------------------------------------------------\n",
            "x_5[2].shape: torch.Size([3, 24, 24])\n",
            "x_5[2].dtype: torch.float32\n",
            "type(x_5[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_5[2].min(): -1.731309175491333\n",
            "x_5[2].max(): 2.073734760284424\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "random_crop_transform = v2.RandomCrop(size=(int(x_4[0].shape[1] / 4 * 3), int(x_4[0].shape[2] / 4 * 3)))\n",
        "x_5 = [random_crop_transform(img) for img in x_4]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_5[{i}].shape: {x_5[i].shape}\")\n",
        "    print(f\"x_5[{i}].dtype: {x_5[i].dtype}\")\n",
        "    print(f\"type(x_5[{i}]): {type(x_5[i])}\")\n",
        "    print(f\"x_5[{i}].min(): {x_5[i].min()}\")\n",
        "    print(f\"x_5[{i}].max(): {x_5[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x_4, x_5, y)):\n",
        "    axs[0, i].imshow(img1.permute(1, 2, 0))\n",
        "    axs[0, i].set(title='x_4')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='v2.RandomCrop')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.Resize\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.v2.Resize.html#torchvision.transforms.v2.Resize](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Resize.html#torchvision.transforms.v2.Resize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_6[0].shape: torch.Size([3, 32, 32])\n",
            "x_6[0].dtype: torch.float32\n",
            "type(x_6[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_6[0].min(): -1.5120868682861328\n",
            "x_6[0].max(): 1.9321309328079224\n",
            "--------------------------------------------------\n",
            "x_6[1].shape: torch.Size([3, 32, 32])\n",
            "x_6[1].dtype: torch.float32\n",
            "type(x_6[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_6[1].min(): -1.9098613262176514\n",
            "x_6[1].max(): 2.0131337642669678\n",
            "--------------------------------------------------\n",
            "x_6[2].shape: torch.Size([3, 32, 32])\n",
            "x_6[2].dtype: torch.float32\n",
            "type(x_6[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_6[2].min(): -1.7102888822555542\n",
            "x_6[2].max(): 2.073734760284424\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "resize_transform = v2.Resize(size=(x[0].shape[0], x[0].shape[1]))\n",
        "x_6 = [resize_transform(img) for img in x_5]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_6[{i}].shape: {x_6[i].shape}\")\n",
        "    print(f\"x_6[{i}].dtype: {x_6[i].dtype}\")\n",
        "    print(f\"type(x_6[{i}]): {type(x_6[i])}\")\n",
        "    print(f\"x_6[{i}].min(): {x_6[i].min()}\")\n",
        "    print(f\"x_6[{i}].max(): {x_6[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x_5, x_6, y)):\n",
        "    axs[0, i].imshow(img1.permute(1, 2, 0))\n",
        "    axs[0, i].set(title='x_5')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='v2.Resize')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.RandomVerticalFlip\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomVerticalFlip.html#torchvision.transforms.v2.RandomVerticalFlip](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomVerticalFlip.html#torchvision.transforms.v2.RandomVerticalFlip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_7[0].shape: torch.Size([3, 32, 32])\n",
            "x_7[0].dtype: torch.float32\n",
            "type(x_7[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_7[0].min(): -1.5120868682861328\n",
            "x_7[0].max(): 1.9321309328079224\n",
            "--------------------------------------------------\n",
            "x_7[1].shape: torch.Size([3, 32, 32])\n",
            "x_7[1].dtype: torch.float32\n",
            "type(x_7[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_7[1].min(): -1.9098613262176514\n",
            "x_7[1].max(): 2.0131337642669678\n",
            "--------------------------------------------------\n",
            "x_7[2].shape: torch.Size([3, 32, 32])\n",
            "x_7[2].dtype: torch.float32\n",
            "type(x_7[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_7[2].min(): -1.7102888822555542\n",
            "x_7[2].max(): 2.073734760284424\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "random_verical_flip_transform = v2.RandomVerticalFlip(p=.6)\n",
        "x_7 = [random_verical_flip_transform(img) for img in x_6]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_7[{i}].shape: {x_7[i].shape}\")\n",
        "    print(f\"x_7[{i}].dtype: {x_7[i].dtype}\")\n",
        "    print(f\"type(x_7[{i}]): {type(x_7[i])}\")\n",
        "    print(f\"x_7[{i}].min(): {x_7[i].min()}\")\n",
        "    print(f\"x_7[{i}].max(): {x_7[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x_6, x_7, y)):\n",
        "    axs[0, i].imshow(img1.permute(1, 2, 0))\n",
        "    axs[0, i].set(title='x_6')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='v2.RandomVerticalFlip')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.RandomHorizontalFlip\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_8[0].shape: torch.Size([3, 32, 32])\n",
            "x_8[0].dtype: torch.float32\n",
            "type(x_8[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_8[0].min(): -1.5120868682861328\n",
            "x_8[0].max(): 1.9321309328079224\n",
            "--------------------------------------------------\n",
            "x_8[1].shape: torch.Size([3, 32, 32])\n",
            "x_8[1].dtype: torch.float32\n",
            "type(x_8[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_8[1].min(): -1.9098613262176514\n",
            "x_8[1].max(): 2.0131337642669678\n",
            "--------------------------------------------------\n",
            "x_8[2].shape: torch.Size([3, 32, 32])\n",
            "x_8[2].dtype: torch.float32\n",
            "type(x_8[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_8[2].min(): -1.7102888822555542\n",
            "x_8[2].max(): 2.073734760284424\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "random_horizontal_flip_transform = v2.RandomHorizontalFlip(p=.7)\n",
        "x_8 = [random_horizontal_flip_transform(img) for img in x_7]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_8[{i}].shape: {x_8[i].shape}\")\n",
        "    print(f\"x_8[{i}].dtype: {x_8[i].dtype}\")\n",
        "    print(f\"type(x_8[{i}]): {type(x_8[i])}\")\n",
        "    print(f\"x_8[{i}].min(): {x_8[i].min()}\")\n",
        "    print(f\"x_8[{i}].max(): {x_8[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x_7, x_8, y)):\n",
        "    axs[0, i].imshow(img1.permute(1, 2, 0))\n",
        "    axs[0, i].set(title='x_7')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='v2.RandomHorizontalFlip')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.RandomRotation\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomRotation.html#torchvision.transforms.v2.RandomRotation](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomRotation.html#torchvision.transforms.v2.RandomRotation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_9[0].shape: torch.Size([3, 32, 32])\n",
            "x_9[0].dtype: torch.float32\n",
            "type(x_9[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_9[0].min(): -1.5120868682861328\n",
            "x_9[0].max(): 1.9321309328079224\n",
            "--------------------------------------------------\n",
            "x_9[1].shape: torch.Size([3, 32, 32])\n",
            "x_9[1].dtype: torch.float32\n",
            "type(x_9[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_9[1].min(): -1.9098613262176514\n",
            "x_9[1].max(): 2.0131337642669678\n",
            "--------------------------------------------------\n",
            "x_9[2].shape: torch.Size([3, 32, 32])\n",
            "x_9[2].dtype: torch.float32\n",
            "type(x_9[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_9[2].min(): -1.7102888822555542\n",
            "x_9[2].max(): 2.073734760284424\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "random_rotation_transform = v2.RandomRotation(degrees=[0, 45])\n",
        "x_9 = [random_rotation_transform(img) for img in x_8]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_9[{i}].shape: {x_9[i].shape}\")\n",
        "    print(f\"x_9[{i}].dtype: {x_9[i].dtype}\")\n",
        "    print(f\"type(x_9[{i}]): {type(x_9[i])}\")\n",
        "    print(f\"x_9[{i}].min(): {x_9[i].min()}\")\n",
        "    print(f\"x_9[{i}].max(): {x_9[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x_8, x_9, y)):\n",
        "    axs[0, i].imshow(img1.permute(1, 2, 0))\n",
        "    axs[0, i].set(title='x_8')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='v2.RandomRotation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.ColorJitter\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html#torchvision.transforms.ColorJitter](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html#torchvision.transforms.ColorJitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_10[0].shape: torch.Size([3, 32, 32])\n",
            "x_10[0].dtype: torch.float32\n",
            "type(x_10[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_10[0].min(): 0.0\n",
            "x_10[0].max(): 0.9999585747718811\n",
            "--------------------------------------------------\n",
            "x_10[1].shape: torch.Size([3, 32, 32])\n",
            "x_10[1].dtype: torch.float32\n",
            "type(x_10[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_10[1].min(): 0.0\n",
            "x_10[1].max(): 0.677256166934967\n",
            "--------------------------------------------------\n",
            "x_10[2].shape: torch.Size([3, 32, 32])\n",
            "x_10[2].dtype: torch.float32\n",
            "type(x_10[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_10[2].min(): 0.0\n",
            "x_10[2].max(): 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "color_jitter_transform = v2.ColorJitter(brightness=.7, contrast=.5, saturation=.9, hue=.3)\n",
        "x_10 = [color_jitter_transform(img) for img in x_9]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_10[{i}].shape: {x_10[i].shape}\")\n",
        "    print(f\"x_10[{i}].dtype: {x_10[i].dtype}\")\n",
        "    print(f\"type(x_10[{i}]): {type(x_10[i])}\")\n",
        "    print(f\"x_10[{i}].min(): {x_10[i].min()}\")\n",
        "    print(f\"x_10[{i}].max(): {x_10[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x_9, x_10, y)):\n",
        "    axs[0, i].imshow(img1.permute(1, 2, 0))\n",
        "    axs[0, i].set(title='x_9')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='v2.ColorJitter')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### v2.RandomAffine\n",
        "   - [pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomAffine.html#torchvision.transforms.v2.RandomAffine](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomAffine.html#torchvision.transforms.v2.RandomAffine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_11[0].shape: torch.Size([3, 32, 32])\n",
            "x_11[0].dtype: torch.float32\n",
            "type(x_11[0]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_11[0].min(): 0.0\n",
            "x_11[0].max(): 0.9999585747718811\n",
            "--------------------------------------------------\n",
            "x_11[1].shape: torch.Size([3, 32, 32])\n",
            "x_11[1].dtype: torch.float32\n",
            "type(x_11[1]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_11[1].min(): 0.0\n",
            "x_11[1].max(): 0.677256166934967\n",
            "--------------------------------------------------\n",
            "x_11[2].shape: torch.Size([3, 32, 32])\n",
            "x_11[2].dtype: torch.float32\n",
            "type(x_11[2]): <class 'torchvision.tv_tensors._image.Image'>\n",
            "x_11[2].min(): 0.0\n",
            "x_11[2].max(): 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "random_affine_transform = v2.RandomAffine(degrees=0, shear=.5, scale=[0.5, 1.5])\n",
        "x_11 = [random_affine_transform(img) for img in x_10]\n",
        "\n",
        "# log\n",
        "for i in range(len(x)):\n",
        "    print(f\"x_11[{i}].shape: {x_11[i].shape}\")\n",
        "    print(f\"x_11[{i}].dtype: {x_11[i].dtype}\")\n",
        "    print(f\"type(x_11[{i}]): {type(x_11[i])}\")\n",
        "    print(f\"x_11[{i}].min(): {x_11[i].min()}\")\n",
        "    print(f\"x_11[{i}].max(): {x_11[i].max()}\")\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x_10, x_11, y)):\n",
        "    axs[0, i].imshow(img1.permute(1, 2, 0))\n",
        "    axs[0, i].set(title='x_10')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='v2.RandomAffine')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mix transforms\n",
        "   - v2.Compose\n",
        "      - [pytorch.org/vision/main/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "transforms = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(dtype=torch.float32, scale=True),\n",
        "        v2.Normalize(mean=mus, std=stds),\n",
        "        v2.RandomCrop(size=(int(x_4[0].shape[1] / 4 * 3), int(x_4[0].shape[2] / 4 * 3))),\n",
        "        v2.Resize(size=(x[0].shape[0], x[0].shape[1])),\n",
        "        v2.RandomVerticalFlip(p=.6),\n",
        "        v2.RandomHorizontalFlip(p=.7),\n",
        "        v2.RandomRotation(degrees=[0, 45]),\n",
        "        v2.ColorJitter(brightness=.7, contrast=.5, saturation=.9, hue=.3),\n",
        "        v2.RandomAffine(degrees=0, shear=.5, scale=[0.5, 1.5]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_12 = [transforms(img) for img in x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout='compressed')\n",
        "for i, (img1, img2, label) in enumerate(zip(x, x_12, y)):\n",
        "    axs[0, i].imshow(img1)\n",
        "    axs[0, i].set(title='x')\n",
        "    axs[1, i].imshow(img2.permute(1, 2, 0))\n",
        "    axs[1, i].set(title='x_12')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Effect of transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "trainset:\n",
            "\tlen(trainset)          : 50000\n",
            "\ttrainset.transform     : None\n",
            "\ttype(trainset[0][0])   : <class 'PIL.Image.Image'>\n",
            "\ttype(trainset[0][1])   : <class 'int'>\n",
            "\ttype(trainset.data[0]) : <class 'numpy.ndarray'>\n",
            "--------------------------------------------------\n",
            "trainsubset:\n",
            "\tlen(trainsubset)             : 10\n",
            "\trainsubset.dataset.transform : None\n",
            "\ttype(trainsubset[0][0])      : <class 'PIL.Image.Image'>\n",
            "\ttype(trainsubset[0][1])      : <class 'int'>\n"
          ]
        }
      ],
      "source": [
        "# pytorch dataset\n",
        "trainset = CIFAR10(root='./dataset', train=True, transform=None, download=True)\n",
        "\n",
        "# pytorch subset\n",
        "num_samples = 10\n",
        "trainsubset = Subset(trainset, indices=range(num_samples))\n",
        "\n",
        "# log\n",
        "print('trainset:')\n",
        "print(f\"\\tlen(trainset)          : {len(trainset)}\")\n",
        "print(f\"\\ttrainset.transform     : {trainset.transform}\")\n",
        "print(f\"\\ttype(trainset[0][0])   : {type(trainset[0][0])}\")\n",
        "print(f\"\\ttype(trainset[0][1])   : {type(trainset[0][1])}\")\n",
        "print(f\"\\ttype(trainset.data[0]) : {type(trainset.data[0])}\")\n",
        "print('-' * 50)\n",
        "print('trainsubset:')\n",
        "print(f\"\\tlen(trainsubset)             : {len(trainsubset)}\")\n",
        "print(f\"\\trainsubset.dataset.transform : {trainsubset.dataset.transform}\")\n",
        "print(f\"\\ttype(trainsubset[0][0])      : {type(trainsubset[0][0])}\")\n",
        "print(f\"\\ttype(trainsubset[0][1])      : {type(trainsubset[0][1])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainset:\n",
            "\tlen(trainset): 50000\n",
            "\ttrainset.transform:\n",
            "Compose(\n",
            "      ToImage()\n",
            "      ToDtype(scale=False)\n",
            ")\n",
            "\ttype(trainset[0][0])   : <class 'torchvision.tv_tensors._image.Image'>\n",
            "\ttrainset[0][0].dtype   : torch.float32\n",
            "\ttype(trainset[0][1])   : <class 'int'>\n",
            "\ttype(trainset.data[0]) : <class 'numpy.ndarray'>\n",
            "--------------------------------------------------\n",
            "trainsubset:\n",
            "\tlen(trainsubset): 10\n",
            "\ttrainsubset.dataset.transform:\n",
            "Compose(\n",
            "      ToImage()\n",
            "      ToDtype(scale=False)\n",
            ")\n",
            "\ttype(trainsubset[0][0]) : <class 'torchvision.tv_tensors._image.Image'>\n",
            "\ttrainsubset[0][0].dtype : torch.float32\n",
            "\ttype(trainsubset[0][1]) : <class 'int'>\n"
          ]
        }
      ],
      "source": [
        "transforms = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# add transforms to the dataset\n",
        "trainset.transform = transforms\n",
        "\n",
        "# log\n",
        "print('trainset:')\n",
        "print(f\"\\tlen(trainset): {len(trainset)}\")\n",
        "print(f\"\\ttrainset.transform:\\n{trainset.transform}\")\n",
        "print(f\"\\ttype(trainset[0][0])   : {type(trainset[0][0])}\")\n",
        "print(f\"\\ttrainset[0][0].dtype   : {trainset[0][0].dtype}\")\n",
        "print(f\"\\ttype(trainset[0][1])   : {type(trainset[0][1])}\")\n",
        "print(f\"\\ttype(trainset.data[0]) : {type(trainset.data[0])}\")\n",
        "print('-' * 50)\n",
        "print('trainsubset:')\n",
        "print(f\"\\tlen(trainsubset): {len(trainsubset)}\")\n",
        "print(f\"\\ttrainsubset.dataset.transform:\\n{trainsubset.dataset.transform}\")\n",
        "print(f\"\\ttype(trainsubset[0][0]) : {type(trainsubset[0][0])}\")\n",
        "print(f\"\\ttrainsubset[0][0].dtype : {trainsubset[0][0].dtype}\")\n",
        "print(f\"\\ttype(trainsubset[0][1]) : {type(trainsubset[0][1])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainloader:\n",
            "\ttype(next_iter_trainloader[0]) : <class 'torch.Tensor'>\n",
            "\tnext_iter_trainloader[0].dtype : torch.float32\n",
            "\ttype(next_iter_trainloader[1]) : <class 'torch.Tensor'>\n",
            "\tnext_iter_trainloader[1].dtype : torch.int64\n"
          ]
        }
      ],
      "source": [
        "# pytorch dataloader\n",
        "trainloader = DataLoader(trainsubset, batch_size=num_samples, shuffle=False)\n",
        "next_iter_trainloader = next(iter(trainloader))\n",
        "\n",
        "print('trainloader:')\n",
        "print(f\"\\ttype(next_iter_trainloader[0]) : {type(next_iter_trainloader[0])}\")\n",
        "print(f\"\\tnext_iter_trainloader[0].dtype : {next_iter_trainloader[0].dtype}\")\n",
        "print(f\"\\ttype(next_iter_trainloader[1]) : {type(next_iter_trainloader[1])}\")\n",
        "print(f\"\\tnext_iter_trainloader[1].dtype : {next_iter_trainloader[1].dtype}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "author_name": "Amirhossein Heydari",
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}