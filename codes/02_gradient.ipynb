{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** amirhosseinheydari78@gmail.com - üìç **Linktree:** [linktr.ee/mr_pylin](https://linktr.ee/mr_pylin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Neuron Structure (Perceptron)\n",
    "   - In many contexts, the terms **Neuron** and **Perceptron** are used interchangeably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; margin-top:50px;\">\n",
    "   <div style=\"width:20%; margin-right:auto; margin-left:auto;\">\n",
    "      <table style=\"margin:0 auto; width:80%; text-align:center\">\n",
    "         <caption style=\"font-weight:bold;\">Dataset</caption>\n",
    "         <thead>\n",
    "            <tr>\n",
    "               <th style=\"width:25%; text-align:center\"><span style=\"color:magenta;\">#</span></th>\n",
    "               <th style=\"width:25%; text-align:center\"><span style=\"color:#9090ff;\">x<sub>1</sub></span></th>\n",
    "               <th style=\"width:25%; text-align:center\"><span style=\"color:#9090ff;\">x<sub>2</sub></span></th>\n",
    "               <th style=\"width:25%; text-align:center\"><span style=\"color:red;\">y</span></th>\n",
    "            </tr>\n",
    "         </thead>\n",
    "         <tbody>\n",
    "            <tr><th>1</th><td>1</td><td>1</td><td>2</td></tr>\n",
    "            <tr><th>2</th><td>2</td><td>3</td><td>5</td></tr>\n",
    "            <tr><th>3</th><td>1</td><td>2</td><td>3</td></tr>\n",
    "            <tr><th>4</th><td>3</td><td>1</td><td>4</td></tr>\n",
    "            <tr><th>5</th><td>2</td><td>4</td><td>6</td></tr>\n",
    "            <tr><th>6</th><td>3</td><td>2</td><td>5</td></tr>\n",
    "            <tr><th>7</th><td>4</td><td>1</td><td>5</td></tr>\n",
    "         </tbody>\n",
    "      </table>\n",
    "   </div>\n",
    "   <div style=\"width:80%; padding:10px;\">\n",
    "      <figure style=\"text-align:center; margin:0;\">\n",
    "         <img src=\"../assets/images/original/perceptron/perceptron-1.svg\" alt=\"perceptron-1.svg\" style=\"max-width:80%; height:auto;\">\n",
    "         <figcaption style=\"font-size:smaller; text-align:center;\">A simple Neuron (Perceptron)</figcaption>\n",
    "      </figure>\n",
    "   </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to estimate **y**?\n",
    "   1. **System of Equations**\n",
    "      $$\n",
    "      \\left\\{\n",
    "      \\begin{aligned}\n",
    "      1w_1 + 1w_2 &= 2 \\\\\n",
    "      2w_1 + 3w_2 &= 5 \\\\\n",
    "      1w_1 + 2w_2 &= 3 \\\\\n",
    "      3w_1 + 1w_2 &= 4 \\\\\n",
    "      2w_1 + 4w_2 &= 6 \\\\\n",
    "      3w_1 + 2w_2 &= 5 \\\\\n",
    "      4w_1 + 1w_2 &= 5 \\\\\n",
    "      \\end{aligned}\n",
    "      \\right.\n",
    "      $$\n",
    "\n",
    "      - **Disadvantages**\n",
    "        - `Complexity`: Neural networks are highly complex systems with millions of parameters ([GPT-4 has 1.76 trillion parameters](https://en.wikipedia.org/wiki/GPT-4#:~:text=Rumors%20claim%20that%20GPT%2D4,running%20and%20by%20George%20Hotz.)).\n",
    "        - `Non-linearity`: Neural networks use activation functions like Sigmoid, which introduce non-linearity into the network.\n",
    "      - **Critical issue: Overdetermined system**\n",
    "        - The number of equations are more than the number of unknowns.\n",
    "        - The system becomes inconsistent and cannot be solved exactly.\n",
    "        - It may lead to either \"No solution\" or \"An infinite number of solutions\".\n",
    "\n",
    "   1. **Delta Rule**\n",
    "      - The delta rule, also known as the Widrow-Hoff rule or the LMS (least mean squares) rule.\n",
    "      - The delta rule is commonly associated with the AdaLiNe (Adaptive Linear Neuron) model.\n",
    "      - It is a simple supervised learning rule used for training single-layer neural networks (perceptrons).\n",
    "\n",
    "   1. **Backpropagation**\n",
    "      - Backpropagation is an extended version of Delta Rule for multi-layer neural networks.\n",
    "      - It allows the network to learn from its mistakes by updating the weights iteratively using **Gradient Descent** (aka Steepest Descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient\n",
    "   - **Definition**:\n",
    "      - The gradient represents the rate of change of the output of a function with respect to its inputs. \n",
    "      - For functions with multiple variables, it generalizes the concept of a derivative, forming a vector of partial derivatives.\n",
    "   - **Intuition**:\n",
    "      - In one-dimensional functions, the gradient (or derivative) corresponds to the slope of the function.\n",
    "      - In multi-dimensional functions, the gradient points in the direction of the steepest ascent of the function, with its magnitude indicating the rate of change.\n",
    "   - **Applications**:\n",
    "      - Crucial for optimization techniques like **Gradient Descent**, where gradients guide the updates to minimize loss functions in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd\n",
    "   - **Overview**:\n",
    "      - PyTorch's **automatic differentiation engine**, which computes gradients efficiently for tensor operations.\n",
    "      - It enables dynamic computation graphs, making it flexible for building and training complex neural networks.\n",
    "   - **How it Works**:\n",
    "      1. **Backward Pass**:\n",
    "         - Calling `torch.Tensor.backward()` computes the gradients for all tensors in the computation graph with `requires_grad=True`. These gradients are accumulated in the `grad` attribute of the respective tensors.\n",
    "      2. **Accessing Gradients**:\n",
    "         - Gradients are stored in `torch.Tensor.grad` after the backward pass.\n",
    "         - Optimizers (e.g., `torch.optim.SGD`, `torch.optim.Adam`) use these gradients to update model parameters during training.\n",
    "\n",
    "üìö **Tutorials**:\n",
    "   - A Gentle Introduction to `torch.autograd`: [pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGrad in Details : Example 1\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../assets/images/original/gradient/autograd.svg\" alt=\"autograd.svg\" style=\"width: 80%;\">\n",
    "    <figcaption style=\"text-align: center;\">Lower-Level AutoGrad Mechanism</figcaption>\n",
    "</figure>\n",
    "\n",
    "¬©Ô∏è **Credits**:\n",
    "   - more info about autograd: [https://www.youtube.com/@elliotwaite](https://www.youtube.com/watch?v=MswxJw-8PvE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMul(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input1, input2):\n",
    "        ctx.save_for_backward(input1, input2)\n",
    "        return input1 * input2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input1, input2 = ctx.saved_tensors\n",
    "        grad_input1 = grad_output * input2\n",
    "        grad_input2 = grad_output * input1\n",
    "        return grad_input1, grad_input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_1.grad: None\n",
      "t_2.grad: 2.0\n",
      "t_3.grad_fn.next_functions : ((None, 0), (<AccumulateGrad object at 0x0000025845EDF970>, 0))\n"
     ]
    }
   ],
   "source": [
    "# leaf nodes\n",
    "t_1 = torch.tensor(2.0)\n",
    "t_2 = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# perform a multiplication operation\n",
    "t_3 = CustomMul.apply(t_1, t_2)\n",
    "\n",
    "# backward\n",
    "t_3.backward()\n",
    "\n",
    "# log\n",
    "print(f\"t_1.grad: {t_1.grad}\")\n",
    "print(f\"t_2.grad: {t_2.grad}\")\n",
    "print(f\"t_3.grad_fn.next_functions : {t_3.grad_fn.next_functions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGrad in Details : Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computation graph:\n",
      " <MeanBackward0 object at 0x0000025845F1EC80>\n",
      "     <MulBackward0 object at 0x0000025846000D60>\n",
      "         <PowBackward0 object at 0x0000025845F7E860>\n",
      "             <AddBackward0 object at 0x0000025826156860>\n",
      "                 <AccumulateGrad object at 0x0000025826156B90>\n"
     ]
    }
   ],
   "source": [
    "# grad_fn\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# perform operations\n",
    "y = x + 1\n",
    "z = y ** 2 * 3\n",
    "out = z.mean()\n",
    "\n",
    "# function to traverse the graph\n",
    "def print_computation_graph(grad_fn, level=0):\n",
    "    if grad_fn is not None:\n",
    "        print(\" \" * level, grad_fn)\n",
    "        if hasattr(grad_fn, 'next_functions'):\n",
    "            for fn in grad_fn.next_functions:\n",
    "                print_computation_graph(fn[0], level + 4)\n",
    "\n",
    "# start from the output node (out) and traverse backward\n",
    "print(\"computation graph:\")\n",
    "print_computation_graph(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Automatic Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: $f(x) = 2x + 3 \\rightarrow \\nabla f(x) = \\frac{\\partial f}{\\partial x} = 2$\n",
    "   - $\\nabla f(4) = 2$\n",
    "   - $\\nabla f(0) = 2$\n",
    "   - $\\nabla f(1) = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x     : tensor(1., requires_grad=True)\n",
      "y     : tensor(5., grad_fn=<AddBackward0>)\n",
      "x.grad: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "def f(x: torch.Tensor):\n",
    "    return 2 * x + 3  # torch.add(torch.multiply(2, x), 3)\n",
    "\n",
    "\n",
    "# x: independent variable\n",
    "x = torch.tensor(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# f(x) or y : dependent variable\n",
    "y = f(x)\n",
    "\n",
    "# compute the gradients with respect to all Tensors that have `requires_grad=True`\n",
    "y.backward()\n",
    "\n",
    "# access computed gradients\n",
    "# if x at 1 moves by Œµ, then y moves by 2Œµ\n",
    "gradients = x.grad\n",
    "\n",
    "# log\n",
    "print('x     :', x)\n",
    "print('y     :', y)\n",
    "print(\"x.grad:\", gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "_ = np.linspace(-4, 6, 100)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.title(f\"x.grad: {x.grad}\")\n",
    "plt.plot(_, f(_), label=\"f(x) = 2x + 3\", color='blue')\n",
    "plt.axvline(x=x.item(), color='red', linestyle='--', label=f\"x = {x}\")\n",
    "plt.axhline(y=f(x).item(), color='green', linestyle='--', label=f\"y = {f(x)}\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xticks(range(-10, 16, 2))\n",
    "plt.yticks(range(-10, 16, 2))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: $f(x) = 3x^2 - 2x + 5 \\quad\\rightarrow\\quad \\nabla f(x) = \\frac{\\partial f}{\\partial x} = 6x - 2$\n",
    "   - $\\nabla f(3) = 16$\n",
    "   - $\\nabla f(0) = -2$\n",
    "   - $\\nabla f(1) = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x     : tensor(3., requires_grad=True)\n",
      "y     : tensor(26., grad_fn=<AddBackward0>)\n",
      "x.grad: 16.0 [at x=3.0]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    # torch.add(torch.sub(torch.mul(3, torch.pow(x, 2)), torch.mul(2, x)), 5)\n",
    "    return 3 * x ** 2 - 2 * x + 5\n",
    "\n",
    "\n",
    "x = torch.tensor(3, dtype=torch.float32, requires_grad=True)\n",
    "y = f(x)\n",
    "\n",
    "# compute the gradients with respect to all Tensors that have `requires_grad=True`\n",
    "y.backward()\n",
    "\n",
    "# access computed gradients\n",
    "# if x at 3 moves by Œµ, then y moves by (6 * 3 - 2)Œµ\n",
    "gradients = x.grad\n",
    "\n",
    "# log\n",
    "print('x     :', x)\n",
    "print('y     :', y)\n",
    "print(f\"x.grad: {gradients} [at x={x}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "_ = np.linspace(-5, 5, 100)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.title(f\"x.grad: {x.grad}\")\n",
    "plt.plot(_, f(_), label=\"f(x) = 3x^2 - 2x + 5\", color='blue')\n",
    "plt.axvline(x=x.item(), color='red', linestyle='--', label=f\"x = {x}\")\n",
    "plt.axhline(y=f(x).item(), color='green', linestyle='--', label=f\"y = {f(x).item()}\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xticks(range(-5, 6))\n",
    "plt.yticks(range(0, 101, 10))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: $f(w_1, w_2) = w_1x_1 + w_2x_2 \\quad\\rightarrow\\quad \\nabla f(W) = \\left( \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2} \\right) = (x_1, x_2)$\n",
    "   - `magnitude:` $|\\nabla f(w_1, w_2)| = \\sqrt{x_1^2 + x_2^2}$\n",
    "\n",
    "   - `direction:` $\\nabla f(w_1, w_2) = \\frac{x_1}{\\sqrt{x_1^2 + x_2^2}} \\hat{i} + \\frac{x_2}{\\sqrt{x_1^2 + x_2^2}} \\hat{j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: tensor([1., 2.], requires_grad=True)\n",
      "X: tensor([2., 3.])\n",
      "y: tensor(8., grad_fn=<DotBackward0>)\n",
      "--------------------------------------------------\n",
      "magnitude of gradients: 3.605551242828369\n",
      "direction of gradients: tensor([0.5547, 0.8321])\n"
     ]
    }
   ],
   "source": [
    "def f(X, W):\n",
    "    return torch.dot(X, W)\n",
    "\n",
    "\n",
    "W = torch.tensor([1, 2], dtype=torch.float32, requires_grad=True)\n",
    "X = torch.tensor([2, 3], dtype=torch.float32)\n",
    "y = f(X, W)\n",
    "\n",
    "# compute the gradients\n",
    "y.backward()\n",
    "\n",
    "# access the gradients\n",
    "gradients = W.grad\n",
    "\n",
    "magnitude_grad = torch.norm(gradients)      # same as (grad ** 2).sum().sqrt()\n",
    "direction_grad = gradients / magnitude_grad  # normalized (unit vector)\n",
    "\n",
    "# log\n",
    "print('W:', W)\n",
    "print('X:', X)\n",
    "print('y:', y)\n",
    "print('-' * 50)\n",
    "print('magnitude of gradients:', magnitude_grad.item())\n",
    "print('direction of gradients:', direction_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "w1 = np.linspace(-10, 10, 100)\n",
    "w2 = np.linspace(-10, 10, 100)\n",
    "X1, X2 = np.meshgrid(w1, w2)\n",
    "_ = X1 * W[0].detach().numpy() + X2 * W[1].detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), layout='compressed')\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X1, X2, _, cmap='viridis')\n",
    "ax1.set_xlabel('w1')\n",
    "ax1.set_ylabel('w2')\n",
    "ax1.set_zlabel('f(w1, w2)')\n",
    "ax1.set_title(\"f(w1, w2) = 2w1 + 3w2\")\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.quiver(0, 0, direction_grad[0], direction_grad[1], angles='xy', scale_units='xy', scale=1, color='red')\n",
    "ax2.set_xlim(-2, 2)\n",
    "ax2.set_ylim(-2, 2)\n",
    "ax2.set_xlabel('w1')\n",
    "ax2.set_ylabel('w2')\n",
    "ax2.set_title(\"Direction of gradients\")\n",
    "ax2.grid('on')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place Operations with `requires_grad=True` on Leaf Nodes\n",
    "   - **In-place operations** modify the content of a tensor **directly** without creating a new tensor.\n",
    "   - Examples include operations like `+=`, `-=` or using functions with an underscore like `.add_()`, `.mul_()`, etc.\n",
    "\n",
    "**Why In-place Operations are Problematic for Gradients?**\n",
    "   - **Loss of Original Data:**  \n",
    "      - When you perform an in-place operation on a tensor that requires gradients, PyTorch **loses track** of the original tensor values, which is essential for correctly calculating the gradient during the backward pass.\n",
    "      - This happens because, during the backward pass, PyTorch needs the original values to compute the gradients. If the tensor is modified in place, the **original value is overwritten** and cannot be accessed later for the backward calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor(1., dtype=torch.float64)\n",
      "x2: tensor(1., dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(0, dtype=torch.float64)\n",
    "x2 = torch.tensor(0, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# out-of-place assignment\n",
    "x1 = x1 + 1  # x1 = x1.add(1)\n",
    "x2 = x2 + 1  # x2 = x2.add(1)\n",
    "\n",
    "# log\n",
    "print('x1:', x1)\n",
    "print('x2:', x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a leaf Variable that requires grad is being used in an in-place operation.\n",
      "x1: tensor(1., dtype=torch.float64)\n",
      "x2: tensor(0., dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(0, dtype=torch.float64)\n",
    "x2 = torch.tensor(0, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# in-place assignment\n",
    "x1 += 1  # x1.add_(1)\n",
    "\n",
    "try:\n",
    "    x2 += 1  # x2.add_(1)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "# log\n",
    "print('x1:', x1)\n",
    "print('x2:', x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "   - The gradient direction is indeed the direction in which a function increases most rapidly\n",
    "   - To minimize the loss function, we shall move in the opposite of the gradient direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: $f(w_1, w_2, b) = w_1x_1 + w_2x_2 + b \\quad\\rightarrow\\quad \\nabla f(W) = \\left( \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}, \\frac{\\partial f}{\\partial b} \\right) = (x_1, x_2, 1)$\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"../assets/images/original/perceptron/adaline.svg\" alt=\"adaline.svg\" style=\"width: 80%;\">\n",
    "    <figcaption style=\"text-align: center;\">ADAptive LInear NEuron (ADALINE)</figcaption>\n",
    "</figure>\n",
    "\n",
    "$\n",
    "    W = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\end{bmatrix}\\quad,\\quad\n",
    "    X = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}\\quad,\\quad\n",
    "    output = W^T X = \\begin{bmatrix} w_0 \\ w_1 \\ w_2 \\end{bmatrix}.\\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}= w_0 + w_1x_1 + w_2x_2\n",
    "$\n",
    "\n",
    "#### Chain Rule\n",
    "   - Activation function must be differentiable\n",
    "   - Loss (error) function must be differentiable\n",
    "$$\n",
    "\\nabla L(W) = (\\frac{\\partial \\text{loss}}{\\partial y_{\\text{pred}}} \\cdot \\frac{\\partial y_{\\text{pred}}}{\\partial \\text{output}} \\cdot \\frac{\\partial \\text{output}}{\\partial W})\n",
    "$$\n",
    "\n",
    "#### Updating Weights\n",
    "$$\n",
    "W_{new} = W_{old} - \\alpha \\nabla L(W_{old})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Optimization Example\n",
    "   - $x = [2, 3] \\quad,\\quad y = 0$\n",
    "   - Note: $x$ is a single sample with two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      : 0\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.960834264755249\n",
      "prediction : 1 (label)\n",
      "loss       : 0.9232024550437927\n",
      "grad       : tensor([0.0723, 0.1446, 0.2169])\n",
      "step       : tensor([0.0362, 0.0723, 0.1085])\n",
      "W_new      : tensor([0.2638, 0.6277, 0.3915], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 1\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.9366591572761536\n",
      "prediction : 1 (label)\n",
      "loss       : 0.8773303627967834\n",
      "grad       : tensor([0.1111, 0.2223, 0.3334])\n",
      "step       : tensor([0.0556, 0.1111, 0.1667])\n",
      "W_new      : tensor([0.2083, 0.5165, 0.2248], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 2\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.8716690540313721\n",
      "prediction : 1 (label)\n",
      "loss       : 0.7598069310188293\n",
      "grad       : tensor([0.1950, 0.3900, 0.5850])\n",
      "step       : tensor([0.0975, 0.1950, 0.2925])\n",
      "W_new      : tensor([ 0.1108,  0.3215, -0.0677], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 3\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.6342986822128296\n",
      "prediction : 1 (label)\n",
      "loss       : 0.4023348093032837\n",
      "grad       : tensor([0.2943, 0.5885, 0.8828])\n",
      "step       : tensor([0.1471, 0.2943, 0.4414])\n",
      "W_new      : tensor([-0.0364,  0.0273, -0.5091], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 4\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.18106062710285187\n",
      "prediction : 0 (label)\n",
      "loss       : 0.03278294950723648\n",
      "grad       : tensor([0.0537, 0.1074, 0.1611])\n",
      "step       : tensor([0.0268, 0.0537, 0.0805])\n",
      "W_new      : tensor([-0.0632, -0.0264, -0.5897], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 5\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.13181111216545105\n",
      "prediction : 0 (label)\n",
      "loss       : 0.01737416908144951\n",
      "grad       : tensor([0.0302, 0.0603, 0.0905])\n",
      "step       : tensor([0.0151, 0.0302, 0.0453])\n",
      "W_new      : tensor([-0.0783, -0.0566, -0.6349], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 6\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.1094651073217392\n",
      "prediction : 0 (label)\n",
      "loss       : 0.011982609517872334\n",
      "grad       : tensor([0.0213, 0.0427, 0.0640])\n",
      "step       : tensor([0.0107, 0.0213, 0.0320])\n",
      "W_new      : tensor([-0.0890, -0.0779, -0.6669], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 7\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.09572887420654297\n",
      "prediction : 0 (label)\n",
      "loss       : 0.009164017625153065\n",
      "grad       : tensor([0.0166, 0.0331, 0.0497])\n",
      "step       : tensor([0.0083, 0.0166, 0.0249])\n",
      "W_new      : tensor([-0.0973, -0.0945, -0.6918], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 8\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.0861462652683258\n",
      "prediction : 0 (label)\n",
      "loss       : 0.007421179208904505\n",
      "grad       : tensor([0.0136, 0.0271, 0.0407])\n",
      "step       : tensor([0.0068, 0.0136, 0.0203])\n",
      "W_new      : tensor([-0.1040, -0.1081, -0.7121], requires_grad=True)\n",
      "--------------------------------------------------\n",
      "epoch      : 9\n",
      "y_true     : 0 (label)\n",
      "y_pred     : 0.07895941287279129\n",
      "prediction : 0 (label)\n",
      "loss       : 0.006234589032828808\n",
      "grad       : tensor([0.0115, 0.0230, 0.0345])\n",
      "step       : tensor([0.0057, 0.0115, 0.0172])\n",
      "W_new      : tensor([-0.1098, -0.1196, -0.7293], requires_grad=True)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# y = 0\n",
    "y_true = torch.tensor(0, dtype=torch.int64)\n",
    "\n",
    "# 1 is the multiplication for bias\n",
    "X = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "\n",
    "# initial weights [bias = .3]\n",
    "W = torch.tensor([.3, .7, .5], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# hyper parameters\n",
    "epochs = 10\n",
    "learning_rate = .5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch      : {epoch}\")\n",
    "\n",
    "    # feed-forward\n",
    "    output = torch.dot(X, W)\n",
    "    y_pred = torch.sigmoid(output)\n",
    "    print(f\"y_true     : {y_true.item()} (label)\")\n",
    "    print(f\"y_pred     : {y_pred.item()}\")\n",
    "    print(f\"prediction : {torch.where(y_pred < .5, 0, 1)} (label)\")\n",
    "\n",
    "    # loss\n",
    "    loss = (y_pred - y_true) ** 2\n",
    "    print(f\"loss       : {loss.item()}\")\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    dW = W.grad\n",
    "    step = learning_rate * dW\n",
    "    print(f\"grad       : {dW}\")\n",
    "    print(f\"step       : {step}\")\n",
    "\n",
    "    # update weights [method 1]\n",
    "    # W.requires_grad_(False)\n",
    "    # W -= step\n",
    "    # W.grad.zero_()\n",
    "    # W.requires_grad_(True)\n",
    "\n",
    "    # update weights [method 2]\n",
    "    # W = W.detach() - step\n",
    "    # W.requires_grad_(True)\n",
    "\n",
    "    # update weights [method 3] : preferred\n",
    "    with torch.no_grad():\n",
    "        W -= step\n",
    "        W.grad.zero_()\n",
    "\n",
    "    print(f\"W_new      : {W}\")\n",
    "    print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
