{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/pytorch-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"text-align: right; flex: 1;\">\n",
    "        <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../assets/images/pytorch/logo/pytorch-logo-dark.svg\" \n",
    "                 alt=\"PyTorch Logo\"\n",
    "                 style=\"max-height: 48px; width: auto; background-color: #ffffff; border-radius: 8px;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [array_like Sequences](#toc2_)    \n",
    "- [PyTorch Tensors](#toc3_)    \n",
    "  - [Ones, Zeros, Full, Empty](#toc3_1_)    \n",
    "  - [Index & Slice](#toc3_2_)    \n",
    "  - [Math operations](#toc3_3_)    \n",
    "    - [Pointwise Ops](#toc3_3_1_)    \n",
    "      - [Broadcasting](#toc3_3_1_1_)    \n",
    "  - [Reshape & View](#toc3_4_)    \n",
    "  - [Mutable Objects](#toc3_5_)    \n",
    "    - [Copy Tensors](#toc3_5_1_)    \n",
    "    - [torch.Tensor to numpy.ndarray](#toc3_5_2_)    \n",
    "    - [numpy.ndarray to torch.Tensor](#toc3_5_3_)    \n",
    "    - [In-Place Operations](#toc3_5_4_)    \n",
    "  - [GPU Acceleration](#toc3_6_)    \n",
    "  - [Reproducibility](#toc3_7_)    \n",
    "    - [`torch.backends.cudnn.deterministic`](#toc3_7_1_)    \n",
    "    - [`torch.backends.cudnn.benchmark`](#toc3_7_2_)    \n",
    "    - [`torch.use_deterministic_algorithms`](#toc3_7_3_)    \n",
    "  - [Random Sampling from a Distribution](#toc3_8_)    \n",
    "  - [`torch.Tensor.item()`](#toc3_9_)    \n",
    "  - [Miscellaneous](#toc3_10_)    \n",
    "    - [`torch.float32` is preferred over `torch.float64` in most deep learning tasks](#toc3_10_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set print options to increase line width\n",
    "torch.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[array_like Sequences](#toc0_)\n",
    "\n",
    "- `list`\n",
    "  - Used for storing elements of different data types\n",
    "  - Flexible: there is no length & shape limit\n",
    "  - Not optimized for mathematical operations\n",
    "- `numpy.ndarray`\n",
    "  - Implemented in C\n",
    "  - Used for mathematical operations\n",
    "  - Arrays are homogeneous: they can store elements of the same data type\n",
    "- `troch.Tensor`\n",
    "  - PyTorch's core functionality is implemented in C++\n",
    "  - Optimized for deep learning operations e.g. auto gradient\n",
    "  - Support GPU acceleration [NVIDIA/AMD GPUs]\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- More on Lists: [docs.python.org/3/tutorial/datastructures.html#more-on-lists](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists)\n",
    "- `numpy.ndarray`: [numpy.org/doc/stable/reference/generated/numpy.ndarray.html](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)\n",
    "- `torch.Tensor`: [docs.pytorch.org/docs/stable/tensors.html](https://docs.pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "üìö **Tutorials**:\n",
    "\n",
    "- Tensors: [pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar : 0-dimensional array/tensor\n",
    "scalar_1 = 2\n",
    "scalar_2 = np.array(2)\n",
    "scalar_3 = torch.tensor(2)\n",
    "\n",
    "# log\n",
    "print(f\"scalar_1: {scalar_1} | ndim: 0 | dtype: {type(scalar_1)}\")\n",
    "print(f\"scalar_2: {scalar_2} | ndim: {scalar_2.ndim} | dtype: numpy.{scalar_2.dtype}\")\n",
    "print(f\"scalar_3: {scalar_3} | ndim: {scalar_3.ndim} | dtype: {scalar_3.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector : 1-dimensional list/array/tensor\n",
    "vector_1 = [1, 2, 3]\n",
    "vector_2 = np.array(vector_1)\n",
    "vector_3 = torch.tensor(vector_1)\n",
    "\n",
    "# log\n",
    "print(f\"vector_1: {str(vector_1):<17} | ndim: 1 | dtype: {type(vector_1[0])}\")\n",
    "print(f\"vector_2: {str(vector_2):<17} | ndim: {vector_2.ndim} | dtype: numpy.{vector_2.dtype}\")\n",
    "print(f\"vector_3: {vector_3} | ndim: {vector_3.ndim} | dtype: {vector_3.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix : 2-dimensional list/array/tensor\n",
    "matrix_1 = [[0, 1], [2, 3]]\n",
    "matrix_2 = np.array(matrix_1)\n",
    "matrix_3 = torch.tensor(matrix_1)\n",
    "\n",
    "# log\n",
    "print(f\"matrix_1:\\n{matrix_1}\\nndim : 2\\ndtype: {type(matrix_1[0][0])}\")\n",
    "print(\"-\" * 50)\n",
    "print(\n",
    "    f\"matrix_2:\\n{matrix_2}\\nmatrix_2.ndim : {matrix_2.ndim}\\nmatrix_2.shape: {matrix_2.shape}\\nmatrix_2.dtype: numpy.{matrix_2.dtype}\"\n",
    ")\n",
    "print(\"-\" * 50)\n",
    "print(\n",
    "    f\"matrix_3:\\n{matrix_3}\\nmatrix_3.ndim : {matrix_3.ndim}\\nmatrix_3.shape: {matrix_3.shape}\\nmatrix_3.dtype: {matrix_3.dtype}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-dimensional list/array/tensor\n",
    "list_3d_1 = [[[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n",
    "array_3d_1 = np.array(list_3d_1)\n",
    "tensor_3d_1 = torch.tensor(list_3d_1)\n",
    "\n",
    "# log\n",
    "print(f\"lst:\\n{list_3d_1}\\nndim : 3\\ndtype: {type(list_3d_1[0][0][0])}\")\n",
    "print(\"-\" * 50)\n",
    "print(\n",
    "    f\"arr:\\n{array_3d_1}\\narr.ndim : {array_3d_1.ndim}\\narr.shape: {array_3d_1.shape}\\narr.dtype: numpy.{array_3d_1.dtype}\"\n",
    ")\n",
    "print(\"-\" * 50)\n",
    "print(\n",
    "    f\"tsr:\\n{tensor_3d_1}\\ntsr.ndim : {tensor_3d_1.ndim}\\ntsr.shape: {tensor_3d_1.shape}\\ntsr.dtype: {tensor_3d_1.dtype}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[PyTorch Tensors](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Ones, Zeros, Full, Empty](#toc0_)\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- Creation Ops: [docs.pytorch.org/docs/stable/torch.html#creation-ops](https://docs.pytorch.org/docs/stable/torch.html#creation-ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ones\n",
    "ones_1 = torch.ones(size=())\n",
    "ones_2 = torch.ones(size=(2, 2))\n",
    "\n",
    "# zeros\n",
    "zeros_1 = torch.zeros(size=(2,))\n",
    "zeros_2 = torch.zeros(size=(3,), dtype=torch.int16)\n",
    "\n",
    "# full\n",
    "full_1 = torch.full(size=(3,), fill_value=3, dtype=torch.int16)\n",
    "\n",
    "# empty\n",
    "empty_1 = torch.empty(size=(2, 3))\n",
    "\n",
    "# log\n",
    "for variable in [\"ones_1\", \"ones_2\", \"zeros_1\", \"zeros_2\", \"full_1\", \"empty_1\"]:\n",
    "    print(f\"{variable}:\\n{eval(variable)}\")\n",
    "    print(f\"{variable}.size() : {eval(variable).size()}\")\n",
    "    print(f\"{variable}.ndim   : {eval(variable).ndim}\")\n",
    "    print(f\"{variable}.dtype  : {eval(variable).dtype}\")\n",
    "    print(f\"type({variable})  : {type(eval(variable))}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Index & Slice](#toc0_)\n",
    "\n",
    "- Indexing a tensor in the PyTorch C++ API works very similar to the Python API.\n",
    "- All index types such as `None` / `...` / `integer` / `boolean` / `slice` / `tensor` are available in the C++ API, making translation from Python indexing code to C++ very simple.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- Tensor Indexing API: [pytorch.org/cppdocs/notes/tensor_indexing.html](https://pytorch.org/cppdocs/notes/tensor_indexing.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2d_1 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "\n",
    "# index\n",
    "index_1 = tensor_2d_1[0]\n",
    "index_2 = tensor_2d_1[1]\n",
    "index_3 = tensor_2d_1[-1]\n",
    "index_4 = tensor_2d_1[0, 0]\n",
    "index_5 = tensor_2d_1[2, -2]\n",
    "\n",
    "# log\n",
    "print(f\"index_1: {index_1}\")\n",
    "print(f\"index_2: {index_2}\")\n",
    "print(f\"index_3: {index_3}\")\n",
    "print(f\"index_4: {index_4}\")\n",
    "print(f\"index_5: {index_5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2d_2 = torch.arange(12).reshape((3, 4))\n",
    "\n",
    "# slice\n",
    "slice_1 = tensor_2d_2[0, :]  # same as tensor_2d_2[0]\n",
    "slice_2 = tensor_2d_2[:, 1]\n",
    "slice_3 = tensor_2d_2[:2, 2:]\n",
    "slice_4 = tensor_2d_2[-1:, 0]\n",
    "\n",
    "# log\n",
    "print(f\"slice_1:\\n{slice_1}\\n\")\n",
    "print(f\"slice_2:\\n{slice_2}\\n\")\n",
    "print(f\"slice_3:\\n{slice_3}\\n\")\n",
    "print(f\"slice_4:\\n{slice_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Math operations](#toc0_)\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- Math operations: [docs.pytorch.org/docs/stable/torch.html#math-operations](https://docs.pytorch.org/docs/stable/torch.html#math-operations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_1_'></a>[Pointwise Ops](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2d_3 = torch.arange(4).reshape(2, 2)\n",
    "tensor_2d_4 = torch.full(size=(2, 2), fill_value=2, dtype=torch.int64)\n",
    "\n",
    "# arithmetic operations\n",
    "arithmetic_1 = tensor_2d_3 + tensor_2d_4   # torch.add\n",
    "arithmetic_2 = tensor_2d_3 - tensor_2d_4   # torch.sub\n",
    "arithmetic_3 = tensor_2d_3 * tensor_2d_4   # torch.multiply\n",
    "arithmetic_4 = tensor_2d_3 / tensor_2d_4   # torch.divide\n",
    "arithmetic_5 = tensor_2d_3 // tensor_2d_4  # torch.floor_divide\n",
    "arithmetic_6 = tensor_2d_3 % tensor_2d_4   # torch.remainder\n",
    "arithmetic_7 = tensor_2d_3**tensor_2d_4    # torch.power\n",
    "\n",
    "# log\n",
    "print(f\"tensor_2d_3:\\n{tensor_2d_3}\\n\")\n",
    "print(f\"tensor_2d_4:\\n{tensor_2d_4}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(7):\n",
    "    print(f\"arithmetic_{i+1}:\\n{eval(f'arithmetic_{i+1}')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_1_1_'></a>[Broadcasting](#toc0_)\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- Broadcasting semantics: [docs.pytorch.org/docs/stable/notes/broadcasting.html](https://docs.pytorch.org/docs/stable/notes/broadcasting.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2d_5 = torch.arange(4).reshape(2, 2) + 1\n",
    "tensor_2d_6 = torch.tensor([[1], [2]])\n",
    "\n",
    "# broadcasting\n",
    "broadcasting_1 = tensor_2d_5 + 1\n",
    "broadcasting_2 = tensor_2d_5 + tensor_2d_6\n",
    "\n",
    "# log\n",
    "print(f\"tensor_2d_5:\\n{tensor_2d_5}\\n\")\n",
    "print(f\"tensor_2d_5:\\n{tensor_2d_6}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"broadcasting_1:\\n{broadcasting_1}\\n\")\n",
    "print(f\"broadcasting_2:\\n{broadcasting_2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[Reshape & View](#toc0_)\n",
    "\n",
    "- torch.Tensor.**view**:\n",
    "  - requires the tensor to be contiguous\n",
    "  - less flexible due to the contiguity requirement\n",
    "  - generally faster since it doesn't involve copying data, just changes the metadata\n",
    "  - [docs.pytorch.org/docs/stable/generated/torch.Tensor.reshape.html](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.reshape.html)\n",
    "- torch.Tensor.**reshape**:\n",
    "  - it can handle non-contiguous tensors by copying data if necessary\n",
    "  - more flexible as it can work with both contiguous and non-contiguous tensors\n",
    "  - might be slower if it needs to copy the data to create a contiguous block\n",
    "  - [docs.pytorch.org/docs/stable/generated/torch.Tensor.view.html](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.view.html)\n",
    "\n",
    "‚úçÔ∏è **Note**:\n",
    "\n",
    "- it is advisable to use `reshape`, which returns a `view` if the shapes are compatible, and copies otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2d_7 = torch.arange(16).reshape(4, 4)\n",
    "\n",
    "# reshape\n",
    "reshape_1 = tensor_2d_7.reshape(2, 8)\n",
    "reshape_2 = tensor_2d_7.reshape(2, -1, 2)\n",
    "\n",
    "# log\n",
    "print(f\"tensor_2d_7:\\n{tensor_2d_7}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"reshape_1:\\n{reshape_1}\\n\")\n",
    "print(f\"reshape_1.shape: {reshape_1.shape}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"reshape_2:\\n{reshape_2}\\n\")\n",
    "print(f\"reshape_2.shape: {reshape_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment by index\n",
    "tensor_2d_7[0, 0] = 100\n",
    "\n",
    "# log\n",
    "print(f\"tensor_2d_7:\\n{tensor_2d_7}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"reshape_1:\\n{reshape_1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[Mutable Objects](#toc0_)\n",
    "\n",
    "- mutable objects refer to objects that can be modified after they are created.\n",
    "- For example, `list`, `numpy.ndarray`, `torch.Tensor` are mutable objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_1_'></a>[Copy Tensors](#toc0_)\n",
    "\n",
    "- `torch.clone`:\n",
    "  - creates a hard/deep copy\n",
    "  - This function is differentiable, so gradients will flow back from the result of this operation to `input`\n",
    "- [docs.pytorch.org/docs/stable/generated/torch.clone.html](https://docs.pytorch.org/docs/stable/generated/torch.clone.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1d_1 = torch.zeros(size=(5,))\n",
    "\n",
    "# clone\n",
    "clone_1 = tensor_1d_1.clone()\n",
    "\n",
    "# assignment by index\n",
    "tensor_1d_1[0] = 1\n",
    "\n",
    "# log\n",
    "print(f\"clone_1: {clone_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_2_'></a>[torch.Tensor to numpy.ndarray](#toc0_)\n",
    "\n",
    "- [docs.pytorch.org/docs/stable/generated/torch.Tensor.numpy.html](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1d_2 = torch.tensor([1, 2, 6, 3])\n",
    "\n",
    "# shared memory\n",
    "tensor_to_numpy_1 = tensor_1d_2.numpy()\n",
    "\n",
    "# hard/deep copy [using <np.array> is deprecated!]\n",
    "tensor_to_numpy_2 = np.copy(tensor_1d_2)  # alternative: tensor_1d_2.clone().numpy() \n",
    "\n",
    "# assignment by index\n",
    "tensor_1d_2[0] = 0\n",
    "\n",
    "# deep dive into data buffer addresses (virtual memory address)\n",
    "dba_1 = hex(tensor_1d_2.data_ptr())\n",
    "dba_2 = hex(tensor_to_numpy_1.__array_interface__['data'][0])\n",
    "dba_3 = hex(tensor_to_numpy_2.__array_interface__['data'][0])\n",
    "\n",
    "# log\n",
    "print(f\"tensor_1d_2             : {tensor_1d_2}\")\n",
    "print(f\"type(tensor_1d_2)       : {type(tensor_1d_2)}\")\n",
    "print(f\"Memory Address (hex)    : {dba_1}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"tensor_to_numpy_1       : {tensor_to_numpy_1}\")\n",
    "print(f\"type(tensor_to_numpy_1) : {type(tensor_to_numpy_1)}\")\n",
    "print(f\"Memory Address (hex)    : {dba_2}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"tensor_to_numpy_2       : {tensor_to_numpy_2}\")\n",
    "print(f\"type(tensor_to_numpy_2) : {type(tensor_to_numpy_2)}\")\n",
    "print(f\"Memory Address (hex)    : {dba_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_3_'></a>[numpy.ndarray to torch.Tensor](#toc0_)\n",
    "\n",
    "- [docs.pytorch.org/docs/stable/generated/torch.from_numpy.html](https://docs.pytorch.org/docs/stable/generated/torch.from_numpy.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1d_1 = np.array([1, 4, 2, 3])\n",
    "\n",
    "# convert + shared memory\n",
    "numpy_to_tensor_1 = torch.from_numpy(array_1d_1)\n",
    "\n",
    "# convert + copy\n",
    "numpy_to_tensor_2 = torch.tensor(array_1d_1)\n",
    "\n",
    "# assignment by index\n",
    "array_1d_1[0] = 0\n",
    "\n",
    "# deep dive into data buffer addresses (virtual memory address)\n",
    "dba_1 = hex(array_1d_1.__array_interface__['data'][0])\n",
    "dba_2 = hex(numpy_to_tensor_1.data_ptr())\n",
    "dba_3 = hex(numpy_to_tensor_2.data_ptr())\n",
    "\n",
    "# log\n",
    "print(f\"array_1d_1              : {array_1d_1}\")\n",
    "print(f\"type(array_1d_1)        : {type(array_1d_1)}\")\n",
    "print(f\"Memory Address (hex)    : {dba_1}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"numpy_to_tensor_1       : {numpy_to_tensor_1}\")\n",
    "print(f\"type(numpy_to_tensor_1) : {type(numpy_to_tensor_1)}\")\n",
    "print(f\"Memory Address (hex)    : {dba_2}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"numpy_to_tensor_2       : {numpy_to_tensor_2}\")\n",
    "print(f\"type(numpy_to_tensor_2) : {type(numpy_to_tensor_2)}\")\n",
    "print(f\"Memory Address (hex)    : {dba_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_4_'></a>[In-Place Operations](#toc0_)\n",
    "\n",
    "- Operations that have a `_` suffix are in-place.\n",
    "- In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history.\n",
    "- Hence, their use is discouraged.\n",
    "\n",
    "üìö **Tutorials**:\n",
    "\n",
    "- in-place operations: [pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#:~:text=3.%2C%203.%2C%203.%5D%5D\\)-,In%2Dplace%20operations,-Operations%20that%20have)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1d_3 = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# in-place addition\n",
    "tensor_1d_3.add_(2)  # tensor_1d_3 += 2\n",
    "\n",
    "# out-of-place addition\n",
    "another_tensor = torch.add(tensor_1d_3, 2)  # another_tensor = tensor_1d_3 + 2\n",
    "\n",
    "# log\n",
    "print(f\"tensor_1d_3    : {tensor_1d_3}\")\n",
    "print(f\"another_tensor : {another_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_6_'></a>[GPU Acceleration](#toc0_)\n",
    "\n",
    "- PyTorch supports GPU acceleration, leveraging industry-standard libraries to enable high-performance computing.\n",
    "\n",
    "‚ùì **Supported GPU Platforms**:\n",
    "\n",
    "<table style=\"margin:0 auto; text-align:center\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>GPU Vendor</th>\n",
    "      <th>Operating System</th>\n",
    "      <th>Supported</th>\n",
    "      <th>Library/Backend</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>NVIDIA</td>\n",
    "      <td>Linux</td>\n",
    "      <td>‚úÖ</td>\n",
    "      <td>CUDA, cuDNN, cuBLAS</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>NVIDIA</td>\n",
    "      <td>Windows</td>\n",
    "      <td>‚úÖ</td>\n",
    "      <td>CUDA, cuDNN, cuBLAS</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>NVIDIA</td>\n",
    "      <td>Mac</td>\n",
    "      <td>‚ùå</td>\n",
    "      <td>N/A</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>AMD</td>\n",
    "      <td>Linux</td>\n",
    "      <td>‚úÖ</td>\n",
    "      <td>ROCm (HIP)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>AMD</td>\n",
    "      <td>Windows</td>\n",
    "      <td>‚ö†Ô∏è</td>\n",
    "      <td>DirectML</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>AMD</td>\n",
    "      <td>Mac</td>\n",
    "      <td>‚úÖ</td>\n",
    "      <td>MPS (Metal)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Intel</td>\n",
    "      <td>Linux</td>\n",
    "      <td>‚úÖ</td>\n",
    "      <td>SYCL</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Intel</td>\n",
    "      <td>Windows</td>\n",
    "      <td>‚úÖ</td>\n",
    "      <td>SYCL</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Intel</td>\n",
    "      <td>Mac</td>\n",
    "      <td>‚ùå</td>\n",
    "      <td>N/A</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "‚úçÔ∏è **Notes**:\n",
    "\n",
    "- PyTorch comes **prebuilt** with `CUDA` and `cuDNN` binaries for NVIDIA GPUs, so you do not need to install `CUDA` or `cuDNN` separately.\n",
    "- Tensors on the `GPU` cannot be directly converted to `np.ndarray` or other structures that do not support GPU operations.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- CUDA semantics: [docs.pytorch.org/docs/stable/notes/cuda.html](https://docs.pytorch.org/docs/stable/notes/cuda.html)\n",
    "- HIP (ROCm) semantics: [docs.pytorch.org/docs/stable/notes/hip.html](https://docs.pytorch.org/docs/stable/notes/hip.html)\n",
    "- Getting Started on Intel GPU: [docs.pytorch.org/docs/stable/notes/get_start_xpu.html](https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html)\n",
    "- MPS backend: [docs.pytorch.org/docs/stable/notes/mps.html](https://docs.pytorch.org/docs/stable/notes/mps.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# alternative\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# log\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "\n",
    "    # number of cuda devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(f\"num_cuda_devices  : {num_cuda_devices}\")\n",
    "\n",
    "    # cuda models\n",
    "    for i in range(num_cuda_devices):\n",
    "        print(f\"cuda {i}:\")\n",
    "        print(f\"\\tname                  : {torch.cuda.get_device_properties(i).name}\")\n",
    "        print(f\"\\ttotal_memory          : {torch.cuda.get_device_properties(i).total_memory} bytes\")\n",
    "        print(f\"\\tmulti_processor_count : {torch.cuda.get_device_properties(i).multi_processor_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1d_4 = torch.ones(5)                 # CPU                                     [default]\n",
    "tensor_1d_5 = torch.ones(5, device=device)  # CPU/GPU (depends on device)             [dynamic]\n",
    "tensor_1d_6 = tensor_1d_3.to(device)        # convert to CPU/GPU (depends on device)  [dynamic]\n",
    "tensor_1d_7 = tensor_1d_3.cuda()            # convert to GPU                          [static]\n",
    "tensor_1d_8 = tensor_1d_3.cpu()             # convert to CPU                          [static]\n",
    "\n",
    "# log\n",
    "print(f\"tensor_1d_4        : {tensor_1d_4}\")\n",
    "print(f\"tensor_1d_4.device : {tensor_1d_4.device}\\n\")\n",
    "print(f\"tensor_1d_5        : {tensor_1d_5}\")\n",
    "print(f\"tensor_1d_5.device : {tensor_1d_5.device}\\n\")\n",
    "print(f\"tensor_1d_6        : {tensor_1d_6}\")\n",
    "print(f\"tensor_1d_6.device : {tensor_1d_6.device}\\n\")\n",
    "print(f\"tensor_1d_7        : {tensor_1d_7}\")\n",
    "print(f\"tensor_1d_7.device : {tensor_1d_7.device}\\n\")\n",
    "print(f\"tensor_1d_8        : {tensor_1d_8}\")\n",
    "print(f\"tensor_1d_8.device : {tensor_1d_8.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1d_9 = torch.ones(size=(5,), device=device)\n",
    "\n",
    "# torch.Tensor to numpy.ndarray\n",
    "try:\n",
    "    tensor_1d_9.numpy()\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1d_10 = torch.ones(size=(5,), device=device)\n",
    "\n",
    "# torch.Tensor to numpy.ndarray\n",
    "tensor_to_numpy_3 = tensor_1d_10.cpu().numpy()\n",
    "\n",
    "# log\n",
    "print(f\"tensor_to_numpy_3       : {tensor_to_numpy_3}\")\n",
    "print(f\"type(tensor_to_numpy_3) : {type(tensor_to_numpy_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_7_'></a>[Reproducibility](#toc0_)\n",
    "\n",
    "- **Seed**: An initial value used to initialize a pseudo-random number generator, ensuring reproducibility of random sequences.\n",
    "\n",
    "- **Platform and Release Variations**:\n",
    "  - Completely reproducible results are not guaranteed across:\n",
    "    - Different PyTorch releases\n",
    "    - Individual commits\n",
    "    - Different platforms (e.g., CPU vs. GPU, different OS)\n",
    "\n",
    "- **Performance Trade-offs**:\n",
    "  - Deterministic operations are often slower than nondeterministic operations.\n",
    "  \n",
    "- **Benefits of Determinism**:\n",
    "  - Determinism can save time in development by facilitating:\n",
    "    - Experimentation\n",
    "    - Debugging\n",
    "    - Testing\n",
    "\n",
    "- **Random Seed Functions**:\n",
    "\n",
    "<table style=\"margin:0 auto; text-align:left\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Function</th>\n",
    "      <th>Scope</th>\n",
    "      <th>Devices Affected</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><code>torch.manual_seed(seed)</code></td>\n",
    "      <td>CPU and CUDA/ROCm GPUs</td>\n",
    "      <td>All CPU and GPU devices</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>torch.cuda.manual_seed(seed)</code></td>\n",
    "      <td>Current CUDA/ROCm GPU</td>\n",
    "      <td>Single GPU (current device)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>torch.cuda.manual_seed_all(seed)</code></td>\n",
    "      <td>All CUDA/ROCm GPUs</td>\n",
    "      <td>All GPU devices</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- Reproducibility: [docs.pytorch.org/docs/stable/notes/randomness.html](https://docs.pytorch.org/docs/stable/notes/randomness.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed only for GPU\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# deep dive into random number generator state\n",
    "state_1 = torch.get_rng_state()\n",
    "state_1_int = int.from_bytes(state_1[:8].numpy().tobytes(), byteorder='little')\n",
    "state_2 = torch.cuda.get_rng_state()\n",
    "state_2_int = int.from_bytes(state_2[:8].numpy().tobytes(), byteorder='little')\n",
    "\n",
    "# log\n",
    "print(f\"torch.get_rng_state()             : {state_1}\")\n",
    "print(f\"state_1 as integer [only 64 bits] : {state_1_int}\")\n",
    "print(f\"torch.cuda.get_rng_state()        : {state_2}\")\n",
    "print(f\"state_2 as integer [only 64 bits] : {state_2_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for both CPU & GPU\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# deep dive into random number generator state\n",
    "state_1 = torch.get_rng_state()\n",
    "state_1_int = int.from_bytes(state_1[:8].numpy().tobytes(), byteorder='little')\n",
    "state_2 = torch.cuda.get_rng_state()\n",
    "state_2_int = int.from_bytes(state_2[:8].numpy().tobytes(), byteorder='little')\n",
    "\n",
    "# log\n",
    "print(f\"torch.get_rng_state()             : {state_1}\")\n",
    "print(f\"state_1 as integer [only 64 bits] : {state_1_int}\")\n",
    "print(f\"torch.cuda.get_rng_state()        : {state_2}\")\n",
    "print(f\"state_2 as integer [only 64 bits] : {state_2_int}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_7_1_'></a>[`torch.backends.cudnn.deterministic`](#toc0_)\n",
    "\n",
    "- This flag ensures that the CUDA Deep Neural Network library (cuDNN) uses deterministic algorithms.\n",
    "- the results will be the same for every run when the same input and seed are provided.\n",
    "- Default value is `False`.\n",
    "\n",
    "üí• **Impact**\n",
    "\n",
    "- When set to `True`, It can slow down your computations because deterministic algorithms are typically slower due to fewer optimizations\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [docs.pytorch.org/docs/stable/backends.html#torch.backends.cudnn.deterministic](https://docs.pytorch.org/docs/stable/backends.html#torch.backends.cudnn.deterministic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_7_2_'></a>[`torch.backends.cudnn.benchmark`](#toc0_)\n",
    "\n",
    "- This flag enables the cuDNN auto-tuner to find the best algorithm for your hardware.\n",
    "- It is useful when the input sizes to your model are changing or not fixed.\n",
    "- Default value is `False`\n",
    "\n",
    "üí• **Impact**\n",
    "\n",
    "- When set to `True`, cuDNN will select the best algorithm for your hardware, potentially improving performance.\n",
    "- If you need exact reproducibility, you should not set benchmark to `True`!\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [docs.pytorch.org/docs/stable/backends.html#torch.backends.cudnn.benchmark](https://docs.pytorch.org/docs/stable/backends.html#torch.backends.cudnn.benchmark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_7_3_'></a>[`torch.use_deterministic_algorithms`](#toc0_)\n",
    "\n",
    "- This function ensures that all the operations that could be non-deterministic are forced to use deterministic algorithms.\n",
    "- Default value is `False`.\n",
    "\n",
    "üí• **Impact**\n",
    "\n",
    "- When set to `True`, This could lead to slower performance as deterministic algorithms are often slower due to the lack of certain optimizations.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [docs.pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms](https://docs.pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# check\n",
    "print(f\"torch.are_deterministic_algorithms_enabled(): {torch.are_deterministic_algorithms_enabled()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_8_'></a>[Random Sampling from a Distribution](#toc0_)\n",
    "\n",
    "- Random sampling from a distribution refers to the process of generating random samples from a specific probability distribution.\n",
    "- In most cases, the goal is to sample from these distributions to simulate or model real-world phenomena.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [docs.pytorch.org/docs/stable/torch.html#random-sampling](https://docs.pytorch.org/docs/stable/torch.html#random-sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tensor filled with random numbers from a uniform distribution on the interval [0,1)\n",
    "rand_1 = torch.rand(size=(5,))\n",
    "\n",
    "# log\n",
    "print(f\"rand_1       : {rand_1}\")\n",
    "print(f\"rand_1.dtype : {rand_1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tensor of random numbers drawn from separate normal distributions\n",
    "normal_1 = torch.normal(mean=0, std=0.1, size=(5,))\n",
    "\n",
    "# log\n",
    "print(f\"normal_1        : {normal_1}\")\n",
    "print(f\"normal_1.mean() : {normal_1.mean()}\")\n",
    "print(f\"normal_1.std()  : {normal_1.std()}\")\n",
    "print(f\"normal_1.dtype  : {normal_1.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_9_'></a>[`torch.Tensor.item()`](#toc0_)\n",
    "\n",
    "- What you see is not necessarily the actual value!\n",
    "- [docs.pytorch.org/docs/stable/generated/torch.Tensor.item.html](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.item.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1d_11 = torch.rand(size=(6,))\n",
    "\n",
    "# item()\n",
    "value_1 = tensor_1d_11[0]\n",
    "value_2 = tensor_1d_11[0].item()\n",
    "\n",
    "# log\n",
    "print(f\"tensor_1d_11: {tensor_1d_11}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"value_1       : {value_1}\")\n",
    "print(f\"value_1.dtype : {value_1.dtype}\\n\")\n",
    "print(f\"value_2       : {value_2}\")\n",
    "print(f\"type(value_2) : {type(value_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_10_'></a>[Miscellaneous](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_10_1_'></a>[`torch.float32` is preferred over `torch.float64` in most deep learning tasks](#toc0_)\n",
    "\n",
    "<table style=\"margin:0 auto;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center\">Aspect</th>\n",
    "      <th style=\"text-align:center\">Description</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><b>Performance and Speed</b></td>\n",
    "      <td>Single-precision (<code>torch.float32</code>) operations are faster and require less computational effort compared to double-precision (<code>torch.float64</code>).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Memory Usage</b></td>\n",
    "      <td><code>torch.float32</code> uses 32 bits (4 bytes) per value, while <code>torch.float64</code> uses 64 bits (8 bytes), leading to lower memory requirements for <code>float32</code>.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Adequate Precision</b></td>\n",
    "      <td>For most deep learning tasks, <code>torch.float32</code> offers sufficient precision. Double-precision (<code>torch.float64</code>) is often unnecessary.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Energy Efficiency</b></td>\n",
    "      <td>Single-precision arithmetic is more energy-efficient than double-precision, making it ideal for tasks that demand lower power consumption.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Industry Standards</b></td>\n",
    "      <td><code>torch.float32</code> is the standard in deep learning frameworks and is widely used across research and production environments.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Hardware Constraints</b></td>\n",
    "      <td>Many hardware platforms, including embedded systems and mobile devices, have limited computational resources and memory, making <code>torch.float32</code> more suitable.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "‚úçÔ∏è **Notes**:\n",
    "\n",
    "- `torch.float32` is also called `float` or `single-precision`.\n",
    "- `torch.float64` is also called `double` or `double-precision`.\n"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
