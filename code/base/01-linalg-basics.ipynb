{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/media-processing-workshop](https://github.com/mr-pylin/media-processing-workshop)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Dependencies](#toc1_)    \n",
        "- [Linear Algebra](#toc2_)    \n",
        "  - [Vector & Matrix Multiplications](#toc2_1_)    \n",
        "    - [Dot Product](#toc2_1_1_)    \n",
        "    - [Outer Product](#toc2_1_2_)    \n",
        "    - [Matrix-Vector Multiplication](#toc2_1_3_)    \n",
        "    - [Matrix-Matrix Multiplication](#toc2_1_4_)    \n",
        "    - [Element-wise Multiplication (Hadamard Product)](#toc2_1_5_)    \n",
        "  - [Fundamental Matrix Properties](#toc2_2_)    \n",
        "    - [Linear Independence](#toc2_2_1_)    \n",
        "    - [Determinant](#toc2_2_2_)    \n",
        "    - [Invertibility](#toc2_2_3_)    \n",
        "    - [Moore-Penrose Pseudoinverse (Generalized Inverse)](#toc2_2_4_)    \n",
        "    - [Trace](#toc2_2_5_)    \n",
        "    - [Matrix Rank](#toc2_2_6_)    \n",
        "  - [Orthogonality & Unitarity](#toc2_3_)    \n",
        "    - [Orthogonality](#toc2_3_1_)    \n",
        "    - [Unitary](#toc2_3_2_)    \n",
        "    - [Hermitian](#toc2_3_3_)    \n",
        "  - [Matrix Structures](#toc2_4_)    \n",
        "    - [Symmetric Matrices](#toc2_4_1_)    \n",
        "    - [Positive Semi-Definite (PSD)](#toc2_4_2_)    \n",
        "    - [Vandermonde Structure](#toc2_4_3_)    \n",
        "  - [Matrix Decompositions (Factorization)](#toc2_5_)    \n",
        "    - [Eigen-decomposition](#toc2_5_1_)    \n",
        "    - [Singular Value Decomposition (SVD)](#toc2_5_2_)    \n",
        "      - [Definition](#toc2_5_2_1_)    \n",
        "      - [General Rank-$r$ SVD](#toc2_5_2_2_)    \n",
        "      - [Connection to Eigen-Decomposition](#toc2_5_2_3_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reduce default marker size for stem plots\n",
        "plt.rcParams[\"lines.markersize\"] = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Linear Algebra](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_1_'></a>[Vector & Matrix Multiplications](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_1_'></a>[Dot Product](#toc0_)\n",
        "\n",
        "- The dot product, also called the **scalar product** or **inner product** in Euclidean space, measures the **similarity** between two vectors.\n",
        "- It is used to compute each element of a matrix product and in correlation analysis to assess similarity between signals.\n",
        "\n",
        "üî¢ **Formula:**\n",
        "\n",
        "- For two n-dimensional column vectors:\n",
        "  $$\\mathbf{x} = [x_1, x_2, \\dots, x_n], \\quad \\mathbf{y} = [y_1, y_2, \\dots, y_n]$$\n",
        "\n",
        "- The dot product is:\n",
        "  $$\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^{n} x_i y_i = \\mathbf{x}^T \\mathbf{y}$$\n",
        "\n",
        "üìê **Geometric Interpretation:**\n",
        "\n",
        "- The dot product can be expressed in terms of vector norms and the angle $\\theta$ between them:\n",
        "  $$\\mathbf{x} \\cdot \\mathbf{y} = \\|\\mathbf{x}\\| \\|\\mathbf{y}\\| \\cos \\theta$$\n",
        "\n",
        "- Where the norm is:\n",
        "  $$\\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + \\dots + a_n^2}$$\n",
        "\n",
        "- Key cases:\n",
        "  - If $\\theta = 0^\\circ$ (vectors align), then $\\cos \\theta = 1$, maximizing the dot product.\n",
        "  - If $\\theta = 90^\\circ$ (vectors are orthogonal), then $\\cos \\theta = 0$, and the dot product is zero.\n",
        "  - If $\\theta = 180^\\circ$ (vectors point opposite), then $\\cos \\theta = -1$, and the dot product is negative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.array([1, 2, 3])\n",
        "y = np.array([4, 5, 6])\n",
        "\n",
        "# dot product\n",
        "dot = np.dot(x, y)\n",
        "\n",
        "# log\n",
        "print(f\"dot: {dot}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.array([0, 255, 255, 0])\n",
        "y = np.array([0, 255, 255, 0])\n",
        "\n",
        "# dot product\n",
        "dot = np.dot(x, y)\n",
        "\n",
        "# 2-norm\n",
        "norms = np.linalg.norm(x) * np.linalg.norm(y)\n",
        "\n",
        "# angle between vectors\n",
        "cos_theta = dot / norms\n",
        "theta = np.arccos(cos_theta)\n",
        "\n",
        "# log\n",
        "print(f\"cos_theta : {cos_theta}\")\n",
        "print(f\"theta     : {theta}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# orthogonal vectors\n",
        "x = np.array([1, 0])\n",
        "y = np.array([0, 1])\n",
        "\n",
        "# dot product\n",
        "dot = x @ y\n",
        "\n",
        "# log\n",
        "print(f\"dot: {dot}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3x3 image patch\n",
        "patch = np.array(\n",
        "    [\n",
        "        [100, 150, 100],\n",
        "        [150, 200, 150],\n",
        "        [100, 150, 100],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# simple averaging filter\n",
        "kernel = np.ones((3, 3)) / 9\n",
        "\n",
        "# image filtering (correlation)\n",
        "response = np.tensordot(patch, kernel)\n",
        "\n",
        "# log\n",
        "print(f\"response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_2_'></a>[Outer Product](#toc0_)\n",
        "\n",
        "- The outer product takes two vectors and produces a matrix, unlike the dot product which results in a scalar.\n",
        "- It is used to construct matrices from vectors and is essential in matrix decompositions like Singular Value Decomposition (SVD), where matrices are expressed as sums of outer products.\n",
        "- The outer product forms a matrix whose columns are scaled copies of one vector, scaled by elements of the other.\n",
        "\n",
        "üî¢ **Formula:**\n",
        "\n",
        "- For two n-dimensional column vectors:\n",
        "  $$\\mathbf{x} = [x_1, x_2, \\dots, x_n], \\quad \\mathbf{y} = [y_1, y_2, \\dots, y_n]$$\n",
        "\n",
        "- Their outer product is defined as:\n",
        "  $$\\mathbf{x} \\otimes \\mathbf{y} = \\mathbf{x} \\mathbf{y}^T$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.array([1, 2, 3])\n",
        "y = np.array([4, 5, 6])\n",
        "\n",
        "# outer product\n",
        "outer = np.outer(x, y)\n",
        "\n",
        "# log\n",
        "print(f\"outer:\\n{outer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1D sobel kernels (separable)\n",
        "sobel_vertical = np.array([1, 2, 1])  # column kernel (for smoothing)\n",
        "sobel_horizontal = np.array([-1, 0, 1])  # row kernel (for edge detection)\n",
        "\n",
        "# construct 2D sobel filter (rank-1 matrix)\n",
        "sobel_kernel = np.outer(sobel_vertical, sobel_horizontal)\n",
        "\n",
        "# log\n",
        "print(f\"sobel_kernel:\\n{sobel_kernel}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVD reconstruction (rank-1 approximation)\n",
        "sigma = 10\n",
        "u = np.array([0.6, 0.8])\n",
        "v = np.array([0.3, 0.4, 0.5])\n",
        "\n",
        "# suppose from SVD: A ‚âà œÉ * u @ v^T\n",
        "rank1_matrix = sigma * np.outer(u, v)\n",
        "\n",
        "# log\n",
        "print(f\"rank1_matrix:\\n{rank1_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_3_'></a>[Matrix-Vector Multiplication](#toc0_)\n",
        "\n",
        "- Matrix-vector multiplication is a fundamental operation in linear algebra, especially in transforms and machine learning algorithms.\n",
        "- It maps a vector from one space to another through multiplication by a matrix.\n",
        "\n",
        "üî¢ **Formula:**\n",
        "\n",
        "- For a matrix $\\mathbf{A}$ of size $m \\times n$ and a vector $\\mathbf{x}$ of size $n \\times 1$, the multiplication is defined as:\n",
        "  $$\\mathbf{A} \\mathbf{x} = \\mathbf{y}$$\n",
        "  \n",
        "  $$\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RGB pixel vector\n",
        "x = np.array([255, 0, 0])\n",
        "\n",
        "# color transformation matrix (swap red and blue channels)\n",
        "A = np.array(\n",
        "    [\n",
        "        [0, 0, 1],\n",
        "        [0, 1, 0],\n",
        "        [1, 0, 0],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# matrix-vector multiplication\n",
        "y = A @ x\n",
        "\n",
        "# log\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_4_'></a>[Matrix-Matrix Multiplication](#toc0_)\n",
        "\n",
        "- Matrix-matrix multiplication is an extension of matrix-vector multiplication.\n",
        "\n",
        "üî¢ **Formula:**\n",
        "\n",
        "- For matrices $\\mathbf{A}$ and $\\mathbf{B}$ with dimensions $m \\times n$ and $n \\times p$ respectively, the multiplication is defined as:\n",
        "  $$\\mathbf{A} \\mathbf{B} = \\mathbf{C}$$\n",
        "\n",
        "  $$\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}_{m \\times n}, \\quad \\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1p} \\\\ b_{21} & b_{22} & \\dots & b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{n1} & b_{n2} & \\dots & b_{np} \\end{bmatrix}_{n \\times p}, \\quad \\mathbf{C} = \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1p} \\\\ c_{21} & c_{22} & \\dots & c_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{m1} & c_{m2} & \\dots & c_{mp} \\end{bmatrix}_{m \\times p}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matrix A: 3x3 color transformation matrix\n",
        "A = np.array(\n",
        "    [\n",
        "        [0, 0, 1],\n",
        "        [0, 1, 0],\n",
        "        [1, 0, 0],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# matrix B: 3x4 matrix representing 4 RGB pixels\n",
        "B = np.array(\n",
        "    [\n",
        "        [255, 0, 0, 128],\n",
        "        [0, 255, 0, 128],\n",
        "        [0, 0, 255, 128],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# multiply A and B to transform all pixels\n",
        "C = A @ B\n",
        "\n",
        "# log\n",
        "print(f\"C:\\n{C}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_1_5_'></a>[Element-wise Multiplication (Hadamard Product)](#toc0_)\n",
        "\n",
        "- Element-wise multiplication, also known as the Hadamard product, involves multiplying two matrices of the same dimensions element by element.\n",
        "\n",
        "üî¢ **Formula:**\n",
        "\n",
        "- For two matrices $\\mathbf{A}$ and $\\mathbf{B}$, both of size $m \\times n$, the element-wise multiplication is defined as:\n",
        "  $$\\mathbf{A} \\circ \\mathbf{B} = \\mathbf{C}$$\n",
        "\n",
        "  $$\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}_{m \\times n}, \\quad \\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{m1} & b_{m2} & \\dots & b_{mn} \\end{bmatrix}_{m \\times n}, \\quad \\mathbf{C} = \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1n} \\\\ c_{21} & c_{22} & \\dots & c_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{m1} & c_{m2} & \\dots & c_{mn} \\end{bmatrix}_{m \\times n}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# grayscale image patch\n",
        "image_patch = np.array(\n",
        "    [\n",
        "        [100, 150, 200, 250],\n",
        "        [80, 120, 160, 200],\n",
        "        [60, 90, 120, 150],\n",
        "        [40, 60, 80, 100],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# binary mask\n",
        "mask = np.array(\n",
        "    [\n",
        "        [0, 0, 0, 0],\n",
        "        [0, 1, 1, 0],\n",
        "        [0, 1, 1, 0],\n",
        "        [0, 0, 0, 0],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# element-wise multiplication\n",
        "masked_patch = image_patch * mask\n",
        "\n",
        "# log\n",
        "print(f\"masked_patch:\\n{masked_patch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_2_'></a>[Fundamental Matrix Properties](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_1_'></a>[Linear Independence](#toc0_)\n",
        "\n",
        "- **Linear independence** is a fundamental concept in linear algebra that describes a set of vectors that do not depend on each other through linear combinations.\n",
        "- A set of vectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k\\}$ in an n-dimensional vector space is **linearly independent** if the only solution to the equation\n",
        "  $$c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\dots + c_k \\mathbf{v}_k = \\mathbf{0}$$\n",
        "  is the trivial solution\n",
        "  $$c_1 = c_2 = \\dots = c_k = 0.$$\n",
        "\n",
        "- If there exists a non-trivial solution (some $c_i \\neq 0$), the vectors are **linearly dependent**, meaning at least one vector can be expressed as a linear combination of the others.\n",
        "\n",
        "üîç **Significance:**\n",
        "\n",
        "- Linear independence indicates the vectors contribute uniquely to the vector space without redundancy.\n",
        "- The maximum number of linearly independent vectors in a space defines its **dimension**.\n",
        "- In the context of matrices, the **rank** is the number of linearly independent rows or columns.\n",
        "\n",
        "üìà **Example:**\n",
        "\n",
        "- In $\\mathbb{R}^2$, vectors $\\mathbf{v}_1 = [1, 0]^T$ and $\\mathbf{v}_2 = [0, 1]^T$ are linearly independent.\n",
        "- However, $\\mathbf{v}_3 = [2, 0]^T$ is linearly dependent on $\\mathbf{v}_1$ because $\\mathbf{v}_3 = 2 \\mathbf{v}_1$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define vectors as columns in a matrix\n",
        "vectors = np.array(\n",
        "    [\n",
        "        [1, 0, 2],\n",
        "        [0, 1, 0],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# compute pairwise dot products\n",
        "num_vectors = vectors.shape[1]\n",
        "dot_product_matrix = np.zeros((num_vectors, num_vectors))\n",
        "for i in range(num_vectors):\n",
        "    for j in range(i, num_vectors):\n",
        "        dot_product_matrix[i, j] = np.dot(vectors[:, i], vectors[:, j])\n",
        "\n",
        "# log\n",
        "print(f\"dot_product_matrix:\\n{dot_product_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_2_'></a>[Determinant](#toc0_)\n",
        "\n",
        "- The **determinant** is a scalar value that can be computed from a square matrix and encodes important properties of the matrix.\n",
        "- For a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, the determinant is denoted as $\\det(\\mathbf{A})$ or $|\\mathbf{A}|$.\n",
        "\n",
        "üî¢ **Key properties:**\n",
        "\n",
        "- The determinant gives information about:\n",
        "  - **Invertibility:** $\\mathbf{A}$ is invertible if and only if $\\det(\\mathbf{A}) \\neq 0$.\n",
        "  - **Volume scaling:** The absolute value of the determinant represents the scaling factor of the volume when the matrix is viewed as a linear transformation.\n",
        "  - **Orientation:** The sign of the determinant indicates whether the transformation preserves or reverses orientation.\n",
        "\n",
        "üìê **Examples:**\n",
        "\n",
        "- For a $2 \\times 2$ matrix\n",
        "  $$\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix},$$\n",
        "  the determinant is\n",
        "  $$\\det(\\mathbf{A}) = ad - bc.$$\n",
        "\n",
        "- For higher dimensions, the determinant is computed recursively via minors and cofactors or using efficient algorithms like **LU decomposition**.\n",
        "\n",
        "üîÑ **Geometric interpretation:**\n",
        "\n",
        "- Applying the matrix $\\mathbf{A}$ to a unit square (in 2D) or cube (in 3D) scales its volume by $|\\det(\\mathbf{A})|$.\n",
        "- If $\\det(\\mathbf{A}) = 0$, the transformation squashes the space into a lower dimension, indicating linear dependence among rows or columns.\n",
        "\n",
        "‚ö†Ô∏è **Important notes:**\n",
        "\n",
        "- The determinant is only defined for square matrices.\n",
        "- Determinants multiply under matrix multiplication:\n",
        "  $$\\det(\\mathbf{AB}) = \\det(\\mathbf{A}) \\det(\\mathbf{B}).$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.array(\n",
        "    [\n",
        "        [2, 3],\n",
        "        [1, 4],\n",
        "    ]\n",
        ")\n",
        "\n",
        "det_A = np.linalg.det(A)\n",
        "\n",
        "# log\n",
        "print(f\"det_A: {det_A}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "B = np.array(\n",
        "    [\n",
        "        [1, 2, 3],\n",
        "        [0, 1, 4],\n",
        "        [5, 6, 0],\n",
        "    ]\n",
        ")\n",
        "\n",
        "det_B = np.linalg.det(B)\n",
        "\n",
        "# log\n",
        "print(f\"det_B: {det_B}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_3_'></a>[Invertibility](#toc0_)\n",
        "\n",
        "- A square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **invertible** (or **nonsingular**) if there exists a matrix $\\mathbf{A}^{-1}$ such that:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n\n",
        "  $$\n",
        "\n",
        "- where $\\mathbf{I}_n$ is the $n \\times n$ identity matrix.\n",
        "\n",
        "ü™ú **Conditions for Invertibility:**\n",
        "\n",
        "- **Nonzero Determinant:**\n",
        "  $$\n",
        "  \\det(\\mathbf{A}) \\neq 0\n",
        "  $$\n",
        "\n",
        "- **Full Rank:**\n",
        "  - $\\mathbf{A}$ has linearly independent rows and columns.\n",
        "  - $\\operatorname{rank}(\\mathbf{A}) = n$.\n",
        "\n",
        "- **Non-Singular:**\n",
        "  - No redundant (linearly dependent) rows or columns exist.\n",
        "\n",
        "- **Eigenvalues are Nonzero:**\n",
        "  - If any eigenvalue $\\lambda_i = 0$, then $\\mathbf{A}$ is singular (not invertible)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a square matrix A\n",
        "A = np.array(\n",
        "    [\n",
        "        [4, 7],\n",
        "        [2, 6],\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check determinant\n",
        "det_A = np.linalg.det(A)\n",
        "print(f\"det_A: {det_A:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check rank\n",
        "rank_A = np.linalg.matrix_rank(A)\n",
        "print(f\"rank_A: {rank_A}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute eigenvalues\n",
        "eigvals_A = np.linalg.eigvals(A)\n",
        "print(f\"eigvals_A: {eigvals_A}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check invertibility\n",
        "is_invertible = (det_A != 0) and (rank_A == A.shape[0]) and not np.isclose(eigvals_A, 0).any()\n",
        "print(f\"is_invertible: {is_invertible}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# inverse of A\n",
        "A_inv = np.linalg.inv(A)\n",
        "print(f\"A_inv:\\n{A_inv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# verify identity matrix\n",
        "identity_approx = A @ A_inv\n",
        "print(f\"identity_approx:\\n{identity_approx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_4_'></a>[Moore-Penrose Pseudoinverse (Generalized Inverse)](#toc0_)\n",
        "\n",
        "- The **Moore-Penrose pseudoinverse** $\\mathbf{A}^+$ generalizes the matrix inverse for matrices that are **non-square** or **singular** (non-invertible).\n",
        "- It provides the **optimal least-squares solution** to linear systems that may not have exact or unique solutions.\n",
        "\n",
        "üî¢ **Brief Recap of Singular Value Decomposition (SVD):**\n",
        "\n",
        "- Any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ can be factorized as\n",
        "  $$\n",
        "  \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n",
        "  $$\n",
        "  where $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices and $\\mathbf{\\Sigma}$ is a diagonal matrix containing singular values.\n",
        "\n",
        "  *(For a detailed explanation, see [Singular Value Decomposition (SVD)](#toc2_5_2_).)*\n",
        "\n",
        "üîÑ **Constructing the Pseudoinverse of $\\mathbf{\\Sigma}$:**\n",
        "\n",
        "- Form $\\mathbf{\\Sigma}^+$ by taking the reciprocal of each nonzero singular value $\\sigma_i$:\n",
        "  $$\n",
        "  \\sigma_i^+ = \\begin{cases}\n",
        "  \\frac{1}{\\sigma_i}, & \\sigma_i \\neq 0 \\\\\n",
        "  0, & \\sigma_i = 0\n",
        "  \\end{cases}\n",
        "  $$\n",
        "- Zeros remain zero, and the shape of $\\mathbf{\\Sigma}^+$ is $n \\times m$ (transposed relative to $\\mathbf{\\Sigma}$).\n",
        "\n",
        "üßÆ **Moore-Penrose Pseudoinverse Formula:**\n",
        "\n",
        "- The pseudoinverse of $\\mathbf{A}$ is\n",
        "  $$\n",
        "  \\mathbf{A}^+ = \\mathbf{V} \\mathbf{\\Sigma}^+ \\mathbf{U}^T\n",
        "  $$\n",
        "\n",
        "üìê **Interpretation and Properties:**\n",
        "\n",
        "- $\\mathbf{A}^+$ yields the **minimum-norm least-squares solution** $\\mathbf{x} = \\mathbf{A}^+ \\mathbf{b}$ to the system $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$.\n",
        "- If $\\mathbf{A}$ is invertible and square, then $\\mathbf{A}^+$ equals the regular inverse:\n",
        "  $$\n",
        "  \\mathbf{A}^+ = \\mathbf{A}^{-1}\n",
        "  $$\n",
        "- The pseudoinverse is essential in applications such as statistics, machine learning, and signal processing, especially for solving over- or under-determined systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a non-square matrix A\n",
        "A = np.array(\n",
        "    [\n",
        "        [1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# compute the Moore-Penrose pseudoinverse\n",
        "A_pinv = np.linalg.pinv(A)\n",
        "\n",
        "# verify identity matrix\n",
        "identity_approx = A @ A_pinv\n",
        "\n",
        "# log\n",
        "print(f\"A_pinv:\\n{A_pinv}\\n\")\n",
        "print(f\"identity_approx:\\n{identity_approx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_5_'></a>[Trace](#toc0_)\n",
        "\n",
        "- The **trace** of a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, denoted as $\\operatorname{tr}(\\mathbf{A})$, is the sum of its diagonal elements:\n",
        "  $$\n",
        "  \\operatorname{tr}(\\mathbf{A}) = \\sum_{i=1}^n A_{ii}\n",
        "  $$\n",
        "\n",
        "üî¢ **Key Properties:**\n",
        "\n",
        "- The trace is a **linear operator**:\n",
        "  $$\n",
        "  \\operatorname{tr}(\\mathbf{A} + \\mathbf{B}) = \\operatorname{tr}(\\mathbf{A}) + \\operatorname{tr}(\\mathbf{B})\n",
        "  $$\n",
        "  $$\n",
        "  \\operatorname{tr}(c \\mathbf{A}) = c \\operatorname{tr}(\\mathbf{A}), \\quad c \\in \\mathbb{R}\n",
        "  $$\n",
        "\n",
        "- The trace of a product is invariant under cyclic permutations:\n",
        "  $$\n",
        "  \\operatorname{tr}(\\mathbf{A} \\mathbf{B}) = \\operatorname{tr}(\\mathbf{B} \\mathbf{A})\n",
        "  $$\n",
        "\n",
        "- This property generalizes to multiple matrices:\n",
        "  $$\n",
        "  \\operatorname{tr}(\\mathbf{A} \\mathbf{B} \\mathbf{C}) = \\operatorname{tr}(\\mathbf{C} \\mathbf{A} \\mathbf{B}) = \\operatorname{tr}(\\mathbf{B} \\mathbf{C} \\mathbf{A})\n",
        "  $$\n",
        "\n",
        "üìê **Interpretation:**\n",
        "\n",
        "- The trace equals the sum of the eigenvalues of $\\mathbf{A}$ (counted with multiplicity).\n",
        "- It is often used in matrix calculus, statistics (e.g., sum of variances), and physics to summarize diagonal dominance or total effect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.array(\n",
        "    [\n",
        "        [1, 2, 3],\n",
        "        [0, 4, 5],\n",
        "        [0, 0, 6],\n",
        "    ]\n",
        ")\n",
        "\n",
        "B = np.array(\n",
        "    [\n",
        "        [7, 8, 9],\n",
        "        [0, 1, 2],\n",
        "        [0, 0, 3],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# compute traces\n",
        "trace_A = np.trace(A)\n",
        "trace_B = np.trace(B)\n",
        "\n",
        "# trace linearity: tr(A + B) == tr(A) + tr(B)\n",
        "trace_sum = np.trace(A + B)\n",
        "\n",
        "# scalar multiplication: tr(cA) == c * tr(A)\n",
        "c = 2.5\n",
        "trace_scaled = np.trace(c * A)\n",
        "\n",
        "# cyclic permutation property: tr(AB) == tr(BA)\n",
        "trace_AB = np.trace(A @ B)\n",
        "trace_BA = np.trace(B @ A)\n",
        "\n",
        "# log\n",
        "print(f\"trace_A      : {trace_A}\")\n",
        "print(f\"trace_B      : {trace_B}\")\n",
        "print(f\"trace_sum    : {trace_sum}\")\n",
        "print(f\"trace_scaled : {trace_scaled}\")\n",
        "print(f\"trace_AB     : {trace_AB}\")\n",
        "print(f\"trace_BA     : {trace_BA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_2_6_'></a>[Matrix Rank](#toc0_)\n",
        "\n",
        "- The **rank** of a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is defined as:\n",
        "  - The maximum number of **linearly independent columns** of $\\mathbf{A}$, or equivalently,\n",
        "  - The maximum number of **linearly independent rows** of $\\mathbf{A}$.\n",
        "\n",
        "üî¢ **Key Properties:**\n",
        "\n",
        "- $\\operatorname{rank}(\\mathbf{A}) \\leq \\min(m, n)$.\n",
        "- The rank equals the number of **non-zero singular values** of $\\mathbf{A}$ (from its Singular Value Decomposition).\n",
        "- The **column rank** and **row rank** are always equal (Fundamental Theorem of Linear Algebra).\n",
        "- Rank measures the dimension of the vector space spanned by the columns (or rows) of $\\mathbf{A}$.\n",
        "- A **full rank** matrix has rank equal to $\\min(m, n)$, meaning its columns (or rows) are linearly independent.\n",
        "- For square matrices, full rank implies the matrix is **invertible** (nonsingular).\n",
        "\n",
        "üìê **Interpretation:**\n",
        "\n",
        "- The rank indicates how much information or \"degrees of freedom\" the matrix carries.\n",
        "- Low-rank matrices represent data with redundancies or dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a full rank 3x3 matrix\n",
        "A = np.array(\n",
        "    [\n",
        "        [1, 2, 3],\n",
        "        [4, 5, 6],\n",
        "        [7, 8, 10],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# compute rank\n",
        "rank_A = np.linalg.matrix_rank(A)\n",
        "\n",
        "# log\n",
        "print(f\"rank_A: {rank_A}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_3_'></a>[Orthogonality & Unitarity](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_3_1_'></a>[Orthogonality](#toc0_)\n",
        "\n",
        "- **Orthogonality** describes a notion of **perpendicularity** between vectors or matrix rows/columns in a vector space.\n",
        "- Two vectors $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n$ are **orthogonal** if their **dot product** (inner product) is zero:\n",
        "  $$\n",
        "  \\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^\\top \\mathbf{b} = 0\n",
        "  $$\n",
        "\n",
        "- A set of vectors $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_k\\}$ is **orthonormal** if they are mutually orthogonal and each has unit norm:\n",
        "  $$\n",
        "  \\mathbf{u}_i^\\top \\mathbf{u}_j =\n",
        "  \\begin{cases}\n",
        "  1 & \\text{if } i = j \\\\\n",
        "  0 & \\text{if } i \\neq j\n",
        "  \\end{cases}\n",
        "  $$\n",
        "\n",
        "- This means the vectors are not only **perpendicular** but also **normalized** (length equals 1).\n",
        "\n",
        "- A square matrix $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ is called **orthogonal** if its columns (and rows) form an orthonormal set, satisfying:\n",
        "  $$\n",
        "  \\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{Q} \\mathbf{Q}^\\top = \\mathbf{I}\n",
        "  $$\n",
        "  which implies:\n",
        "  $$\n",
        "  \\mathbf{Q}^{-1} = \\mathbf{Q}^\\top\n",
        "  $$\n",
        "\n",
        "- Orthogonal matrices **preserve vector norms and angles**, making them essential in many linear transformations such as Fourier transforms and Singular Value Decomposition (SVD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = np.array([1, 0, 0])\n",
        "b = np.array([0, 1, 0])\n",
        "\n",
        "# normalize vectors to make them unit length (orthonormal)\n",
        "a_norm = a / np.linalg.norm(a)\n",
        "b_norm = b / np.linalg.norm(b)\n",
        "\n",
        "# check orthogonality (dot product)\n",
        "dot_ab = np.dot(a, b)\n",
        "dot_norm = np.dot(a_norm, b_norm)\n",
        "\n",
        "# log\n",
        "print(f\"dot_ab   : {dot_ab}\")\n",
        "print(f\"dot_norm : {dot_norm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Q = np.array(\n",
        "    [\n",
        "        [1, 0, 0],\n",
        "        [0, 1, 0],\n",
        "        [0, 0, 1],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# verify orthogonality: Q.T @ Q = I\n",
        "identity_check = Q.T @ Q\n",
        "\n",
        "# log\n",
        "print(f\"identity_check:\\n{identity_check}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_3_2_'></a>[Unitary](#toc0_)\n",
        "\n",
        "- **Unitary matrices** are the complex-valued generalization of orthogonal matrices.\n",
        "- They preserve vector lengths and orthogonality, but in the **complex domain**.\n",
        "- A matrix $\\mathbf{U}$ is **unitary** if it satisfies:\n",
        "  $$\n",
        "  \\mathbf{U} \\mathbf{U}^* = \\mathbf{U}^* \\mathbf{U} = \\mathbf{I}\n",
        "  $$\n",
        "  which implies:\n",
        "  $$\n",
        "  \\mathbf{U}^* = \\mathbf{U}^{-1}\n",
        "  $$\n",
        "  where:\n",
        "  - $\\mathbf{U}^*$ (also written $\\mathbf{U}^H$) is the **conjugate transpose** (Hermitian transpose) of $\\mathbf{U}$.\n",
        "\n",
        "‚úçÔ∏è **Notes:**\n",
        "\n",
        "- For real-valued matrices, unitary matrices reduce to **orthogonal matrices**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a 2x2 complex unitary matrix U\n",
        "theta = np.pi / 4\n",
        "U = np.array(\n",
        "    [\n",
        "        [np.exp(1j * theta), 0],\n",
        "        [0, np.exp(-1j * theta)],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# compute conjugate transpose (hermitian transpose)\n",
        "U_H = np.conjugate(U.T)\n",
        "\n",
        "# log\n",
        "print(f\"U:\\n{U}\\n\")\n",
        "print(f\"U_H:\\n{U_H}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# verify unitary properties\n",
        "UUH = U @ U_H\n",
        "UHU = U_H @ U\n",
        "\n",
        "# log\n",
        "print(f\"UUH:\\n{UUH}\\n\")\n",
        "print(f\"UHU:\\n{UHU}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_3_3_'></a>[Hermitian](#toc0_)\n",
        "\n",
        "- A **Hermitian matrix** $\\mathbf{H}$ is a square matrix (possibly with complex entries) that satisfies the condition:\n",
        "  $$\n",
        "  \\mathbf{H} = \\mathbf{H}^*\n",
        "  $$\n",
        "  where $\\mathbf{H}^*$ is the **conjugate transpose** (Hermitian transpose) of $\\mathbf{H}$.\n",
        "\n",
        "- This means the element at position $(i, j)$ is the complex conjugate of the element at $(j, i)$:\n",
        "  $$\n",
        "  H_{ij} = \\overline{H_{ji}}\n",
        "  $$\n",
        "\n",
        "‚úçÔ∏è **Notes:**\n",
        "\n",
        "- Hermitian matrices are the complex analogue of real symmetric matrices.\n",
        "- All eigenvalues of a Hermitian matrix are **real**.\n",
        "- Hermitian matrices play a key role in quantum mechanics and signal processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a 2x2 hermitian matrix H\n",
        "H = np.array(\n",
        "    [\n",
        "        [2 + 0j, 1 - 1j],\n",
        "        [1 + 1j, 3 + 0j],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# compute conjugate transpose (hermitian transpose)\n",
        "H_H = np.conjugate(H.T)\n",
        "\n",
        "# log\n",
        "print(f\"H:\\n{H}\\n\")\n",
        "print(f\"Hermitian transpose (H^*):\\n{H_H}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_4_'></a>[Matrix Structures](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_4_1_'></a>[Symmetric Matrices](#toc0_)\n",
        "\n",
        "- A matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **symmetric** if it equals its transpose:\n",
        "  $$\n",
        "  \\mathbf{A}^\\top = \\mathbf{A}\n",
        "  $$\n",
        "\n",
        "- Symmetric matrices have several important properties:\n",
        "  - All eigenvalues of $\\mathbf{A}$ are **real**.\n",
        "  - $\\mathbf{A}$ is **diagonalizable** by an orthogonal matrix; that is, there exists an orthogonal matrix $\\mathbf{Q}$ such that\n",
        "    $$\n",
        "    \\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^\\top\n",
        "    $$\n",
        "    where $\\mathbf{\\Lambda}$ is a diagonal matrix containing the eigenvalues.\n",
        "\n",
        "- These properties make symmetric matrices fundamental in optimization, physics, and numerical methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a symmetric matrix A (real-valued)\n",
        "A = np.array(\n",
        "    [\n",
        "        [4, 1, 2],\n",
        "        [1, 3, 0],\n",
        "        [2, 0, 5],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# compute transpose of A\n",
        "A_T = A.T\n",
        "\n",
        "# log\n",
        "print(f\"A:\\n{A}\\n\")\n",
        "print(f\"Transpose of A:\\n{A_T}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_4_2_'></a>[Positive Semi-Definite (PSD)](#toc0_)\n",
        "\n",
        "- A symmetric matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **positive semi-definite (PSD)** if:\n",
        "  $$\n",
        "  \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} \\geq 0 \\quad \\forall \\mathbf{x} \\in \\mathbb{R}^n\n",
        "  $$\n",
        "\n",
        "- Key properties of PSD matrices:\n",
        "  - All eigenvalues of $\\mathbf{A}$ are **non-negative** (i.e., $\\lambda_i \\geq 0$).\n",
        "  - PSD matrices are symmetric by definition.\n",
        "  - For any matrix $\\mathbf{B}$, the matrix $\\mathbf{B}^\\top \\mathbf{B}$ is always symmetric PSD.\n",
        "\n",
        "- PSD matrices frequently appear in optimization, covariance matrices in statistics, and kernel methods in machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a symmetric matrix A\n",
        "A = np.array(\n",
        "    [\n",
        "        [2, -1, 0],\n",
        "        [-1, 2, -1],\n",
        "        [0, -1, 2],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# check symmetry\n",
        "is_symmetric = np.array_equal(A, A.T)\n",
        "\n",
        "# check PSD property via eigenvalues\n",
        "eigenvalues = np.linalg.eigvalsh(A)  # eigvalsh for symmetric matrices\n",
        "is_psd = np.all(eigenvalues >= 0)\n",
        "\n",
        "# Log\n",
        "print(f\"A:\\n{A}\")\n",
        "print(f\"is_symmetric : {is_symmetric}\")\n",
        "print(f\"eigenvalues  : {eigenvalues}\")\n",
        "print(f\"is_psd       : {is_psd}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_4_3_'></a>[Vandermonde Structure](#toc0_)\n",
        "\n",
        "- A **Vandermonde matrix** is a special structured matrix where each row consists of successive powers of a given set of numbers.\n",
        "\n",
        "üî¢ **Mathematical Definition:**\n",
        "\n",
        "- Given scalars $x_1, x_2, \\dots, x_n$, the Vandermonde matrix $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ is defined as:\n",
        "  $$\n",
        "  \\mathbf{V} = \\begin{bmatrix}\n",
        "  1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} \\\\\n",
        "  1 & x_2 & x_2^2 & \\cdots & x_2^{n-1} \\\\\n",
        "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "  1 & x_n & x_n^2 & \\cdots & x_n^{n-1}\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "üìê **Properties:**\n",
        "\n",
        "- The determinant of a Vandermonde matrix has a closed form:\n",
        "  $$\n",
        "  \\det(\\mathbf{V}) = \\prod_{1 \\leq i < j \\leq n} (x_j - x_i)\n",
        "  $$\n",
        "  which is nonzero if all $x_i$ are distinct, implying $\\mathbf{V}$ is invertible.\n",
        "\n",
        "**Applications in Digital Image Processing (DIP) and Transforms:**\n",
        "\n",
        "- Vandermonde structures appear in polynomial interpolation, signal processing, and coding theory.\n",
        "- They are related to transformation matrices in the Discrete Fourier Transform (DFT), Chebyshev polynomial transforms, and other orthogonal polynomial expansions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_5_'></a>[Matrix Decompositions (Factorization)](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_5_1_'></a>[Eigen-decomposition](#toc0_)\n",
        "\n",
        "Given a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, an **eigenvalue** $\\lambda \\in \\mathbb{R}$ and a corresponding **eigenvector** $\\mathbf{v} \\neq \\mathbf{0}$ satisfy:\n",
        "$$\n",
        "\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "$$\n",
        "\n",
        "- Geometrically, $\\mathbf{A}$ scales the vector $\\mathbf{v}$ by $\\lambda$ without changing its direction.\n",
        "- The **spectral theorem** states that symmetric matrices have **real eigenvalues** and a complete set of **orthonormal eigenvectors**.\n",
        "\n",
        "üî¢ **Computing Eigenvalues and Eigenvectors:**\n",
        "\n",
        "- Eigenvalues $\\lambda$ are found by solving the characteristic equation:\n",
        "  $$\n",
        "  \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\n",
        "  $$\n",
        "- For each eigenvalue $\\lambda$, the corresponding eigenvectors $\\mathbf{v}$ satisfy:\n",
        "  $$\n",
        "  (\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{v} = \\mathbf{0}\n",
        "  $$\n",
        "\n",
        "üìê **Significance:**\n",
        "\n",
        "- Eigenvalues indicate the scaling factor applied to eigenvectors during the transformation by $\\mathbf{A}$.\n",
        "- Eigenvectors represent directions invariant under the transformation.\n",
        "- Eigen-decomposition is fundamental in techniques like Principal Component Analysis (PCA) for dimensionality reduction.\n",
        "- In signal processing, Fourier transform basis functions can be viewed as eigenfunctions of the shift operator, revealing frequency components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a symmetric matrix A\n",
        "A = np.array(\n",
        "    [\n",
        "        [4, 1, 2],\n",
        "        [1, 2, 0],\n",
        "        [2, 0, 3],\n",
        "    ]\n",
        ")\n",
        "\n",
        "# compute eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(A)\n",
        "\n",
        "# log\n",
        "print(f\"eigenvalues:\\n{eigenvalues}\\n\")\n",
        "print(f\"eigenvectors:\\n{eigenvectors}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# form diagonal matrix of eigenvalues\n",
        "Lambda = np.diag(eigenvalues)\n",
        "\n",
        "# reconstruct A: A ‚âà Q Œõ Q·µó\n",
        "A_reconstructed = eigenvectors @ Lambda @ eigenvectors.T\n",
        "\n",
        "# log\n",
        "print(f\"A_reconstructed:\\n{A_reconstructed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc2_5_2_'></a>[Singular Value Decomposition (SVD)](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc2_5_2_1_'></a>[Definition](#toc0_)\n",
        "\n",
        "For any real matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the **Singular Value Decomposition (SVD)** expresses it as:\n",
        "\n",
        "$$\n",
        "\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix (left singular vectors),\n",
        "- $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with non-negative real values (singular values),\n",
        "- $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix (right singular vectors).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  3x3 Laplacian filter kernel\n",
        "A = np.array([\n",
        "    [0,  1, 0],\n",
        "    [1, -4, 1],\n",
        "    [0,  1, 0]\n",
        "])\n",
        "\n",
        "# compute SVD\n",
        "U, S, Vt = np.linalg.svd(A)\n",
        "\n",
        "# construct Œ£ with same shape as A\n",
        "sigma = np.zeros_like(A, dtype=np.float32)\n",
        "np.fill_diagonal(sigma, S)\n",
        "\n",
        "# log\n",
        "print(f\"A:\\n{A}\\n\")\n",
        "print(f\"U:\\n{U}\\n\")\n",
        "print(f\"Œ£:\\n{sigma}\\n\")\n",
        "print(f\"V·µó:\\n{Vt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc2_5_2_2_'></a>[General Rank-$r$ SVD](#toc0_)\n",
        "\n",
        "If $\\mathbf{A}$ has rank $r$:\n",
        "\n",
        "$$\n",
        "\\mathbf{A} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\n",
        "$$\n",
        "\n",
        "Each term is a **rank-1 matrix**. The sum reconstructs $\\mathbf{A}$ exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reconstruct A\n",
        "A_reconstructed = U @ sigma @ Vt\n",
        "\n",
        "# log\n",
        "print(\"A_reconstructed:\\n\", A_reconstructed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc2_5_2_3_'></a>[Connection to Eigen-Decomposition](#toc0_)\n",
        "\n",
        "- $\\mathbf{A}^\\top \\mathbf{A}$ and $\\mathbf{A} \\mathbf{A}^\\top$ are symmetric and positive semi-definite matrices.\n",
        "- Their eigenvalues are non-negative: $\\lambda_i \\geq 0$.\n",
        "- The singular values of $\\mathbf{A}$ are given by $\\sigma_i = \\sqrt{\\lambda_i}$, where $\\lambda_i$ are the eigenvalues of $\\mathbf{A}^\\top \\mathbf{A}$.\n",
        "- The columns of $\\mathbf{V}$ (right singular vectors) are the eigenvectors of $\\mathbf{A}^\\top \\mathbf{A}$.\n",
        "- The columns of $\\mathbf{U}$ (left singular vectors) are the eigenvectors of $\\mathbf{A} \\mathbf{A}^\\top$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define a non-square matrix A (m √ó n)\n",
        "A = np.array([\n",
        "    [3, 1],\n",
        "    [1, 3],\n",
        "    [2, 2]\n",
        "])\n",
        "\n",
        "# perform SVD: A = U @ Œ£ @ Vt\n",
        "U, S, Vt = np.linalg.svd(A)\n",
        "\n",
        "# construct Œ£ (m √ó n) with singular values on the diagonal\n",
        "Sigma = np.zeros_like(A, dtype=np.float64)\n",
        "np.fill_diagonal(Sigma, S)\n",
        "\n",
        "# compute A^T A and A A^T\n",
        "AtA = A.T @ A\n",
        "AAt = A @ A.T\n",
        "\n",
        "# eigen-decomposition\n",
        "eigvals_AtA, eigvecs_AtA = np.linalg.eigh(AtA)\n",
        "eigvals_AAt, eigvecs_AAt = np.linalg.eigh(AAt)\n",
        "\n",
        "# sort eigenvalues and eigenvectors in descending order\n",
        "idx_AtA = np.argsort(eigvals_AtA)[::-1]\n",
        "eigvals_AtA = eigvals_AtA[idx_AtA]\n",
        "eigvecs_AtA = eigvecs_AtA[:, idx_AtA]\n",
        "\n",
        "idx_AAt = np.argsort(eigvals_AAt)[::-1]\n",
        "eigvals_AAt = eigvals_AAt[idx_AAt]\n",
        "eigvecs_AAt = eigvecs_AAt[:, idx_AAt]\n",
        "\n",
        "# singular values from eigendecomposition of A^T A\n",
        "singular_values_from_eig = np.sqrt(eigvals_AtA)\n",
        "\n",
        "# log\n",
        "print(f\"S                        : {S}\")\n",
        "print(f\"singular_values_from_eig : {singular_values_from_eig}\\n\")\n",
        "print(f\"Vt.T:\\n{Vt.T}\")\n",
        "print(f\"\\neigvecs_AtA:\\n{eigvecs_AtA}\\n\")\n",
        "print(f\"\\nU:\\n{U}\")\n",
        "print(f\"\\neigvecs_AAt:\\n{eigvecs_AAt}\")"
      ]
    }
  ],
  "metadata": {
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "author_name": "Amirhossein Heydari",
    "kernelspec": {
      "display_name": "media-processing-workshop-py3.13 (3.13.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
