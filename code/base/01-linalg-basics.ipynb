{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/pytorch-workshop](https://github.com/mr-pylin/pytorch-workshop)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Linear Algebra](#toc2_)    \n",
    "  - [Vector & Matrix Multiplications](#toc2_1_)    \n",
    "    - [Dot Product](#toc2_1_1_)    \n",
    "    - [Outer Product](#toc2_1_2_)    \n",
    "    - [Matrix-Vector Multiplication](#toc2_1_3_)    \n",
    "    - [Matrix-Matrix Multiplication](#toc2_1_4_)    \n",
    "    - [Element-wise Multiplication (Hadamard Product)](#toc2_1_5_)    \n",
    "    - [Kronecker Product](#toc2_1_6_)    \n",
    "    - [Einstein Summation](#toc2_1_7_)    \n",
    "  - [Fundamental Matrix Properties](#toc2_2_)    \n",
    "    - [Linear Independence](#toc2_2_1_)    \n",
    "    - [Determinant](#toc2_2_2_)    \n",
    "    - [Invertibility](#toc2_2_3_)    \n",
    "    - [Moore-Penrose Pseudoinverse (Generalized Inverse)](#toc2_2_4_)    \n",
    "    - [Trace](#toc2_2_5_)    \n",
    "    - [Matrix Rank](#toc2_2_6_)    \n",
    "  - [Orthogonality & Unitarity](#toc2_3_)    \n",
    "    - [Orthogonality](#toc2_3_1_)    \n",
    "    - [Unitary](#toc2_3_2_)    \n",
    "    - [Hermitian](#toc2_3_3_)    \n",
    "  - [Matrix Structures](#toc2_4_)    \n",
    "    - [Symmetric Matrices](#toc2_4_1_)    \n",
    "    - [Positive Semi-Definite (PSD)](#toc2_4_2_)    \n",
    "    - [Vandermonde Structure](#toc2_4_3_)    \n",
    "  - [Matrix Decompositions (Factorization)](#toc2_5_)    \n",
    "    - [Eigen-decomposition](#toc2_5_1_)    \n",
    "    - [Singular Value Decomposition (SVD)](#toc2_5_2_)    \n",
    "      - [Definition](#toc2_5_2_1_)    \n",
    "      - [General Rank-$r$ SVD](#toc2_5_2_2_)    \n",
    "      - [Connection to Eigen-Decomposition](#toc2_5_2_3_)    \n",
    "    - [QR Decomposition](#toc2_5_3_)    \n",
    "    - [Cholesky Decomposition](#toc2_5_4_)    \n",
    "  - [Solving Linear Systems](#toc2_6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Linear Algebra](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Vector & Matrix Multiplications](#toc0_)\n",
    "\n",
    "<table style=\"margin: 0 auto;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"text-align: center;\">Function</th>\n",
    "      <th style=\"text-align: center;\">Operator</th>\n",
    "      <th style=\"text-align: center;\">Description</th>\n",
    "      <th style=\"text-align: center;\">Details</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><code>np.dot</code></td>\n",
    "      <td style=\"text-align: center;\">-</td>\n",
    "      <td>Dot product of two arrays</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.dot.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.vdot</code></td>\n",
    "      <td style=\"text-align: center;\">-</td>\n",
    "      <td>Return the dot product of two vectors</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.vdot.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.vecdot</code></td>\n",
    "      <td style=\"text-align: center;\">-</td>\n",
    "      <td>Vector dot product of two arrays</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.vecdot.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.inner</code></td>\n",
    "      <td style=\"text-align: center;\">-</td>\n",
    "      <td>Inner product of two arrays</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.inner.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.outer</code></td>\n",
    "      <td style=\"text-align: center;\">-</td>\n",
    "      <td>Compute the outer product of two vectors</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.outer.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.matmul</code></td>\n",
    "      <td style=\"text-align: center;\"><code>@</code></td>\n",
    "      <td>Matrix product of two arrays</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.matmul.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.tensordot</code></td>\n",
    "      <td style=\"text-align: center;\">-</td>\n",
    "      <td>Compute tensor dot product along specified axes</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.einsum</code></td>\n",
    "      <td style=\"text-align: center;\">-</td>\n",
    "      <td>Evaluates the Einstein summation convention on the operands</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.einsum.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.kron</code></td>\n",
    "      <td style=\"text-align: center;\">-</td>\n",
    "      <td>Kronecker product of two arrays</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.kron.html\">link</a></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Dot Product](#toc0_)\n",
    "\n",
    "- The dot product, also called the **scalar product** or **inner product** in Euclidean space, measures the **similarity** between two vectors.\n",
    "- It is used to compute each element of a matrix product and in correlation analysis to assess similarity between signals.\n",
    "\n",
    "üî¢ **Formula:**\n",
    "\n",
    "- For two n-dimensional column vectors:\n",
    "  $$\\mathbf{x} = [x_1, x_2, \\dots, x_n], \\quad \\mathbf{y} = [y_1, y_2, \\dots, y_n]$$\n",
    "\n",
    "- The dot product is:\n",
    "  $$\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^{n} x_i y_i = \\mathbf{x}^T \\mathbf{y}$$\n",
    "\n",
    "üìê **Geometric Interpretation:**\n",
    "\n",
    "- The dot product can be expressed in terms of vector norms and the angle $\\theta$ between them:\n",
    "  $$\\mathbf{x} \\cdot \\mathbf{y} = \\|\\mathbf{x}\\| \\|\\mathbf{y}\\| \\cos \\theta$$\n",
    "\n",
    "- Where the norm is:\n",
    "  $$\\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + \\dots + a_n^2}$$\n",
    "\n",
    "- Key cases:\n",
    "  - If $\\theta = 0^\\circ$ (vectors align), then $\\cos \\theta = 1$, maximizing the dot product.\n",
    "  - If $\\theta = 90^\\circ$ (vectors are orthogonal), then $\\cos \\theta = 0$, and the dot product is zero.\n",
    "  - If $\\theta = 180^\\circ$ (vectors point opposite), then $\\cos \\theta = -1$, and the dot product is negative.\n",
    "\n",
    "üíª **NumPy Implementation:**\n",
    "\n",
    "- **`np.dot(a, b)`**: Standard dot product without complex conjugation\n",
    "  - Uses complex numbers as-is: $\\sum_{i=1}^{n} x_i y_i$\n",
    "  - Suitable for general matrix operations and real vector spaces\n",
    "\n",
    "- **`np.vdot(a, b)`**: Vector dot product with proper complex handling\n",
    "  - Uses complex conjugate of first argument: $\\sum_{i=1}^{n} \\overline{x_i} y_i$\n",
    "  - Mathematically correct inner product for complex vector spaces\n",
    "  - Always flattens arrays to 1D vectors first\n",
    "  - Essential for quantum mechanics, signal processing applications\n",
    "\n",
    "- **`np.tensordot(a, b, axes)`**: Generalized dot product for higher-dimensional arrays\n",
    "  - Computes tensor dot product along specified axes\n",
    "  - `axes` parameter controls which axes to sum over\n",
    "  - More flexible than `dot` for multi-dimensional array operations\n",
    "  - Example: `np.tensordot(A, B, axes=1)` sums over last axis of A and first axis of B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "\n",
    "# dot product\n",
    "dot = np.dot(x, y)\n",
    "\n",
    "# log\n",
    "print(f\"dot: {dot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 255, 255, 0])\n",
    "y = np.array([0, 255, 255, 0])\n",
    "\n",
    "# dot product\n",
    "dot = np.dot(x, y)\n",
    "\n",
    "# 2-norm\n",
    "norms = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "\n",
    "# angle between vectors\n",
    "cos_theta = dot / norms\n",
    "theta = np.arccos(cos_theta)\n",
    "\n",
    "# log\n",
    "print(f\"cos_theta : {cos_theta}\")\n",
    "print(f\"theta     : {theta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orthogonal vectors\n",
    "x = np.array([1, 0])\n",
    "y = np.array([0, 1])\n",
    "\n",
    "# dot product\n",
    "dot = x @ y\n",
    "\n",
    "# log\n",
    "print(f\"dot: {dot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 image patch\n",
    "patch = np.array(\n",
    "    [\n",
    "        [100, 150, 100],\n",
    "        [150, 200, 150],\n",
    "        [100, 150, 100],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# simple averaging filter\n",
    "kernel = np.ones((3, 3)) / 9\n",
    "\n",
    "# image filtering (correlation)\n",
    "response = np.tensordot(patch, kernel)\n",
    "\n",
    "# log\n",
    "print(f\"response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[Outer Product](#toc0_)\n",
    "\n",
    "- The outer product takes two vectors and produces a matrix, unlike the dot product which results in a scalar.\n",
    "- It is used to construct matrices from vectors and is essential in matrix decompositions like Singular Value Decomposition (SVD), where matrices are expressed as sums of outer products.\n",
    "- The outer product forms a matrix whose columns are scaled copies of one vector, scaled by elements of the other.\n",
    "\n",
    "üî¢ **Formula:**\n",
    "\n",
    "- For two n-dimensional column vectors:\n",
    "  $$\\mathbf{x} = [x_1, x_2, \\dots, x_n], \\quad \\mathbf{y} = [y_1, y_2, \\dots, y_n]$$\n",
    "\n",
    "- Their outer product is defined as:\n",
    "  $$\\mathbf{x} \\otimes \\mathbf{y} = \\mathbf{x} \\mathbf{y}^T$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "\n",
    "# outer product\n",
    "outer = np.outer(x, y)\n",
    "\n",
    "# log\n",
    "print(f\"outer:\\n{outer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D sobel kernels (separable)\n",
    "sobel_vertical = np.array([1, 2, 1])  # column kernel (for smoothing)\n",
    "sobel_horizontal = np.array([-1, 0, 1])  # row kernel (for edge detection)\n",
    "\n",
    "# construct 2D sobel filter (rank-1 matrix)\n",
    "sobel_kernel = np.outer(sobel_vertical, sobel_horizontal)\n",
    "\n",
    "# log\n",
    "print(f\"sobel_kernel:\\n{sobel_kernel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD reconstruction (rank-1 approximation)\n",
    "sigma = 10\n",
    "u = np.array([0.6, 0.8])\n",
    "v = np.array([0.3, 0.4, 0.5])\n",
    "\n",
    "# suppose from SVD: A ‚âà œÉ * u @ v^T\n",
    "rank1_matrix = sigma * np.outer(u, v)\n",
    "\n",
    "# log\n",
    "print(f\"rank1_matrix:\\n{rank1_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[Matrix-Vector Multiplication](#toc0_)\n",
    "\n",
    "- Matrix-vector multiplication is a fundamental operation in linear algebra, especially in transforms and machine learning algorithms.\n",
    "- It maps a vector from one space to another through multiplication by a matrix.\n",
    "\n",
    "üî¢ **Formula:**\n",
    "\n",
    "- For a matrix $\\mathbf{A}$ of size $m \\times n$ and a vector $\\mathbf{x}$ of size $n \\times 1$, the multiplication is defined as:\n",
    "  $$\\mathbf{A} \\mathbf{x} = \\mathbf{y}$$\n",
    "  \n",
    "  $$\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB pixel vector\n",
    "x = np.array([255, 0, 0])\n",
    "\n",
    "# color transformation matrix (swap red and blue channels)\n",
    "a = np.array(\n",
    "    [\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# matrix-vector multiplication\n",
    "y = a @ x\n",
    "\n",
    "# log\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[Matrix-Matrix Multiplication](#toc0_)\n",
    "\n",
    "- Matrix-matrix multiplication is an extension of matrix-vector multiplication.\n",
    "\n",
    "üî¢ **Formula:**\n",
    "\n",
    "- For matrices $\\mathbf{A}$ and $\\mathbf{B}$ with dimensions $m \\times n$ and $n \\times p$ respectively, the multiplication is defined as:\n",
    "  $$\\mathbf{A} \\mathbf{B} = \\mathbf{C}$$\n",
    "\n",
    "  $$\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}_{m \\times n}, \\quad \\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1p} \\\\ b_{21} & b_{22} & \\dots & b_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{n1} & b_{n2} & \\dots & b_{np} \\end{bmatrix}_{n \\times p}, \\quad \\mathbf{C} = \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1p} \\\\ c_{21} & c_{22} & \\dots & c_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{m1} & c_{m2} & \\dots & c_{mp} \\end{bmatrix}_{m \\times p}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix A: 3x3 color transformation matrix\n",
    "a = np.array(\n",
    "    [\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# matrix B: 3x4 matrix representing 4 RGB pixels\n",
    "b = np.array(\n",
    "    [\n",
    "        [255, 0, 0, 128],\n",
    "        [0, 255, 0, 128],\n",
    "        [0, 0, 255, 128],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# multiply A and B to transform all pixels\n",
    "c = a @ b\n",
    "\n",
    "# log\n",
    "print(f\"C:\\n{c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_5_'></a>[Element-wise Multiplication (Hadamard Product)](#toc0_)\n",
    "\n",
    "- Element-wise multiplication, also known as the Hadamard product, involves multiplying two matrices of the same dimensions element by element.\n",
    "\n",
    "üî¢ **Formula:**\n",
    "\n",
    "- For two matrices $\\mathbf{A}$ and $\\mathbf{B}$, both of size $m \\times n$, the element-wise multiplication is defined as:\n",
    "  $$\\mathbf{A} \\circ \\mathbf{B} = \\mathbf{C}$$\n",
    "\n",
    "  $$\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}_{m \\times n}, \\quad \\mathbf{B} = \\begin{bmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{m1} & b_{m2} & \\dots & b_{mn} \\end{bmatrix}_{m \\times n}, \\quad \\mathbf{C} = \\begin{bmatrix} c_{11} & c_{12} & \\dots & c_{1n} \\\\ c_{21} & c_{22} & \\dots & c_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c_{m1} & c_{m2} & \\dots & c_{mn} \\end{bmatrix}_{m \\times n}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grayscale image patch\n",
    "image_patch = np.array(\n",
    "    [\n",
    "        [100, 150, 200, 250],\n",
    "        [80, 120, 160, 200],\n",
    "        [60, 90, 120, 150],\n",
    "        [40, 60, 80, 100],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# binary mask\n",
    "mask = np.array(\n",
    "    [\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 1, 1, 0],\n",
    "        [0, 1, 1, 0],\n",
    "        [0, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# element-wise multiplication\n",
    "masked_patch = image_patch * mask\n",
    "\n",
    "# log\n",
    "print(f\"masked_patch:\\n{masked_patch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_6_'></a>[Kronecker Product](#toc0_)\n",
    "\n",
    "- The Kronecker product, denoted as $\\mathbf{A} \\otimes \\mathbf{B}$, is a **block matrix operation** that creates a larger matrix by scaling each element of the first matrix by the entire second matrix.\n",
    "- It is fundamental in quantum mechanics, signal processing, and multilinear algebra for representing tensor products of vector spaces.\n",
    "\n",
    "üî¢ **Formula:**\n",
    "\n",
    "- For matrices $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{B} \\in \\mathbb{R}^{p \\times q}$:\n",
    " $$\\mathbf{A} \\otimes \\mathbf{B} = \\begin{bmatrix}\n",
    " a_{11}\\mathbf{B} & a_{12}\\mathbf{B} & \\cdots & a_{1n}\\mathbf{B} \\\\\n",
    " a_{21}\\mathbf{B} & a_{22}\\mathbf{B} & \\cdots & a_{2n}\\mathbf{B} \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{m1}\\mathbf{B} & a_{m2}\\mathbf{B} & \\cdots & a_{mn}\\mathbf{B}\n",
    " \\end{bmatrix}_{(mp) \\times (nq)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two matrices\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[0, 5], [6, 7]])\n",
    "\n",
    "# kronecker product\n",
    "kronecker = np.kron(a, b)\n",
    "\n",
    "# log\n",
    "print(f\"a:\\n{a}\\n\")\n",
    "print(f\"b:\\n{b}\\n\")\n",
    "print(f\"kronecker:\\n{kronecker}\\n\")\n",
    "print(f\"kronecker.shape: {kronecker.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_7_'></a>[Einstein Summation](#toc0_)\n",
    "\n",
    "- Einstein summation notation is a **concise mathematical convention** for expressing sums over repeated indices without explicitly writing the summation symbol.\n",
    "- It eliminates the need for $\\sum$ symbols, making complex tensor operations more readable and less cluttered.\n",
    "\n",
    "üî¢ **Convention Rules:**\n",
    "\n",
    "- **Repeated index rule**: Any index that appears **twice** in a term is automatically summed over\n",
    "- **Free index rule**: Indices that appear only **once** represent the dimensions of the result\n",
    "- **Range convention**: Each repeated index sums over its full range (typically 1 to n or 0 to n-1)\n",
    "\n",
    "üìù **Examples:**\n",
    "\n",
    "- **Dot product**: $\\sum_{i=1}^{n} x_i y_i = x_i y_i$\n",
    "- **Matrix-vector multiplication**: $\\sum_{j=1}^{n} A_{ij} x_j = c_i = A_{ij} x_j$\n",
    "- **Matrix-matrix multiplication**: $\\sum_{k=1}^{n} A_{ik} B_{kj} = C_{ij} = A_{ik} B_{kj}$\n",
    "- **Trace of matrix**: $\\text{tr}(\\mathbf{A}) = A_{ii}$ (sum over diagonal)\n",
    "- **Frobenius norm**: $\\|\\mathbf{A}\\|_F^2 = A_{ij} A_{ij}$\n",
    "\n",
    "üíª **NumPy Implementation:**\n",
    "\n",
    "- **`np.einsum('subscripts', operands)`**: Evaluates Einstein summation convention\n",
    " - `'i,i->'`: Dot product (sum over i)\n",
    " - `'ij,j->i'`: Matrix-vector multiplication\n",
    " - `'ij,jk->ik'`: Matrix-matrix multiplication\n",
    " - `'ii->'`: Trace (sum over diagonal)\n",
    " - `'ij,ij->'`: Frobenius inner product\n",
    "- **Flexible and powerful**: Can express virtually any tensor operation\n",
    "- **Optimized**: Often faster than equivalent operations using other NumPy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define matrices\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([[7, 8], [9, 10], [11, 12]])\n",
    "c = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "d = np.array([1, 2, 3])\n",
    "e = np.array([4, 5, 6])\n",
    "\n",
    "# matrix multiplication using einsum\n",
    "matrix_mult = np.einsum(\"ij,jk->ik\", a, b)\n",
    "\n",
    "# trace of a matrix\n",
    "trace = np.einsum(\"ii\", c)\n",
    "\n",
    "# element-wise multiplication and sum\n",
    "element_wise_sum = np.einsum(\"ij,ij\", a, a)\n",
    "\n",
    "# outer product\n",
    "outer = np.einsum(\"i,j->ij\", d, e)\n",
    "\n",
    "# log\n",
    "print(f\"a:\\n{a}\\n\")\n",
    "print(f\"b:\\n{b}\\n\")\n",
    "print(f\"c:\\n{c}\\n\")\n",
    "print(f\"matrix_mult:\\n{matrix_mult}\\n\")\n",
    "print(f\"trace:\\n{trace}\\n\")\n",
    "print(f\"element_wise_sum:\\n{element_wise_sum}\\n\")\n",
    "print(f\"outer:\\n{outer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Fundamental Matrix Properties](#toc0_)\n",
    "\n",
    "<table style=\"margin: 0 auto;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"text-align: center;\">Function</th>\n",
    "      <th style=\"text-align: center;\">Description</th>\n",
    "      <th style=\"text-align: center;\">Details</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.norm</code></td>\n",
    "      <td>Matrix or vector norm</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.det</code></td>\n",
    "      <td>Compute the determinant of an array</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.det.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.matrix_rank</code></td>\n",
    "      <td>Return matrix rank of array using SVD method</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_rank.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.trace</code></td>\n",
    "      <td>Return the sum along diagonals of the array</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.trace.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.inv</code></td>\n",
    "      <td>Compute the (multiplicative) inverse of a matrix</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.pinv</code></td>\n",
    "      <td>Compute the (Moore-Penrose) pseudo-inverse of a matrix</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html\">link</a></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[Linear Independence](#toc0_)\n",
    "\n",
    "- **Linear independence** is a fundamental concept in linear algebra that describes a set of vectors that do not depend on each other through linear combinations.\n",
    "- A set of vectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k\\}$ in an n-dimensional vector space is **linearly independent** if the only solution to the equation\n",
    "  $$c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\dots + c_k \\mathbf{v}_k = \\mathbf{0}$$\n",
    "  is the trivial solution\n",
    "  $$c_1 = c_2 = \\dots = c_k = 0.$$\n",
    "\n",
    "- If there exists a non-trivial solution (some $c_i \\neq 0$), the vectors are **linearly dependent**, meaning at least one vector can be expressed as a linear combination of the others.\n",
    "\n",
    "üîç **Significance:**\n",
    "\n",
    "- Linear independence indicates the vectors contribute uniquely to the vector space without redundancy.\n",
    "- The maximum number of linearly independent vectors in a space defines its **dimension**.\n",
    "- In the context of matrices, the **rank** is the number of linearly independent rows or columns.\n",
    "\n",
    "üìà **Example:**\n",
    "\n",
    "- In $\\mathbb{R}^2$, vectors $\\mathbf{v}_1 = [1, 0]^T$ and $\\mathbf{v}_2 = [0, 1]^T$ are linearly independent.\n",
    "- However, $\\mathbf{v}_3 = [2, 0]^T$ is linearly dependent on $\\mathbf{v}_1$ because $\\mathbf{v}_3 = 2 \\mathbf{v}_1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectors as columns in a matrix\n",
    "vectors = np.array(\n",
    "    [\n",
    "        [1, 0, 2],\n",
    "        [0, 1, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute pairwise dot products\n",
    "num_vectors = vectors.shape[1]\n",
    "dot_product_matrix = np.zeros((num_vectors, num_vectors))\n",
    "for i in range(num_vectors):\n",
    "    for j in range(i, num_vectors):\n",
    "        dot_product_matrix[i, j] = np.dot(vectors[:, i], vectors[:, j])\n",
    "\n",
    "# log\n",
    "print(f\"dot_product_matrix:\\n{dot_product_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[Determinant](#toc0_)\n",
    "\n",
    "- The **determinant** is a scalar value that can be computed from a square matrix and encodes important properties of the matrix.\n",
    "- For a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, the determinant is denoted as $\\det(\\mathbf{A})$ or $|\\mathbf{A}|$.\n",
    "\n",
    "üî¢ **Key properties:**\n",
    "\n",
    "- The determinant gives information about:\n",
    "  - **Invertibility:** $\\mathbf{A}$ is invertible if and only if $\\det(\\mathbf{A}) \\neq 0$.\n",
    "  - **Volume scaling:** The absolute value of the determinant represents the scaling factor of the volume when the matrix is viewed as a linear transformation.\n",
    "  - **Orientation:** The sign of the determinant indicates whether the transformation preserves or reverses orientation.\n",
    "\n",
    "üìê **Examples:**\n",
    "\n",
    "- For a $2 \\times 2$ matrix\n",
    "  $$\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix},$$\n",
    "  the determinant is\n",
    "  $$\\det(\\mathbf{A}) = ad - bc.$$\n",
    "\n",
    "- For higher dimensions, the determinant is computed recursively via minors and cofactors or using efficient algorithms like **LU decomposition**.\n",
    "\n",
    "üîÑ **Geometric interpretation:**\n",
    "\n",
    "- Applying the matrix $\\mathbf{A}$ to a unit square (in 2D) or cube (in 3D) scales its volume by $|\\det(\\mathbf{A})|$.\n",
    "- If $\\det(\\mathbf{A}) = 0$, the transformation squashes the space into a lower dimension, indicating linear dependence among rows or columns.\n",
    "\n",
    "‚ö†Ô∏è **Important notes:**\n",
    "\n",
    "- The determinant is only defined for square matrices.\n",
    "- Determinants multiply under matrix multiplication:\n",
    "  $$\\det(\\mathbf{AB}) = \\det(\\mathbf{A}) \\det(\\mathbf{B}).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(\n",
    "    [\n",
    "        [2, 3],\n",
    "        [1, 4],\n",
    "    ]\n",
    ")\n",
    "\n",
    "det_A = np.linalg.det(a)\n",
    "\n",
    "# log\n",
    "print(f\"det_A: {det_A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [0, 1, 4],\n",
    "        [5, 6, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "det_B = np.linalg.det(b)\n",
    "\n",
    "# log\n",
    "print(f\"det_B: {det_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[Invertibility](#toc0_)\n",
    "\n",
    "- A square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **invertible** (or **nonsingular**) if there exists a matrix $\\mathbf{A}^{-1}$ such that:\n",
    "\n",
    "  $$\n",
    "  \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_n\n",
    "  $$\n",
    "\n",
    "- where $\\mathbf{I}_n$ is the $n \\times n$ identity matrix.\n",
    "\n",
    "ü™ú **Conditions for Invertibility:**\n",
    "\n",
    "- **Nonzero Determinant:**\n",
    "  $$\n",
    "  \\det(\\mathbf{A}) \\neq 0\n",
    "  $$\n",
    "\n",
    "- **Full Rank:**\n",
    "  - $\\mathbf{A}$ has linearly independent rows and columns.\n",
    "  - $\\operatorname{rank}(\\mathbf{A}) = n$.\n",
    "\n",
    "- **Non-Singular:**\n",
    "  - No redundant (linearly dependent) rows or columns exist.\n",
    "\n",
    "- **Eigenvalues are Nonzero:**\n",
    "  - If any eigenvalue $\\lambda_i = 0$, then $\\mathbf{A}$ is singular (not invertible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a square matrix A\n",
    "a = np.array(\n",
    "    [\n",
    "        [4, 7],\n",
    "        [2, 6],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check determinant\n",
    "det_A = np.linalg.det(a)\n",
    "print(f\"det_A: {det_A:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rank\n",
    "rank_A = np.linalg.matrix_rank(a)\n",
    "print(f\"rank_A: {rank_A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigenvalues\n",
    "eigvals_A = np.linalg.eigvals(a)\n",
    "print(f\"eigvals_A: {eigvals_A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check invertibility\n",
    "is_invertible = (det_A != 0) and (rank_A == a.shape[0]) and not np.isclose(eigvals_A, 0).any()\n",
    "print(f\"is_invertible: {is_invertible}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse of A\n",
    "A_inv = np.linalg.inv(a)\n",
    "print(f\"A_inv:\\n{A_inv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify identity matrix\n",
    "identity_approx = a @ A_inv\n",
    "print(f\"identity_approx:\\n{identity_approx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_4_'></a>[Moore-Penrose Pseudoinverse (Generalized Inverse)](#toc0_)\n",
    "\n",
    "- The **Moore-Penrose pseudoinverse** $\\mathbf{A}^+$ generalizes the matrix inverse for matrices that are **non-square** or **singular** (non-invertible).\n",
    "- It provides the **optimal least-squares solution** to linear systems that may not have exact or unique solutions.\n",
    "\n",
    "üî¢ **Brief Recap of Singular Value Decomposition (SVD):**\n",
    "\n",
    "- Any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ can be factorized as\n",
    "  $$\n",
    "  \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n",
    "  $$\n",
    "  where $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices and $\\mathbf{\\Sigma}$ is a diagonal matrix containing singular values.\n",
    "\n",
    "  *(For a detailed explanation, see [Singular Value Decomposition (SVD)](#toc2_5_2_).)*\n",
    "\n",
    "üîÑ **Constructing the Pseudoinverse of $\\mathbf{\\Sigma}$:**\n",
    "\n",
    "- Form $\\mathbf{\\Sigma}^+$ by taking the reciprocal of each nonzero singular value $\\sigma_i$:\n",
    "  $$\n",
    "  \\sigma_i^+ = \\begin{cases}\n",
    "  \\frac{1}{\\sigma_i}, & \\sigma_i \\neq 0 \\\\\n",
    "  0, & \\sigma_i = 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- Zeros remain zero, and the shape of $\\mathbf{\\Sigma}^+$ is $n \\times m$ (transposed relative to $\\mathbf{\\Sigma}$).\n",
    "\n",
    "üßÆ **Moore-Penrose Pseudoinverse Formula:**\n",
    "\n",
    "- The pseudoinverse of $\\mathbf{A}$ is\n",
    "  $$\n",
    "  \\mathbf{A}^+ = \\mathbf{V} \\mathbf{\\Sigma}^+ \\mathbf{U}^T\n",
    "  $$\n",
    "\n",
    "üìê **Interpretation and Properties:**\n",
    "\n",
    "- $\\mathbf{A}^+$ yields the **minimum-norm least-squares solution** $\\mathbf{x} = \\mathbf{A}^+ \\mathbf{b}$ to the system $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$.\n",
    "- If $\\mathbf{A}$ is invertible and square, then $\\mathbf{A}^+$ equals the regular inverse:\n",
    "  $$\n",
    "  \\mathbf{A}^+ = \\mathbf{A}^{-1}\n",
    "  $$\n",
    "- The pseudoinverse is essential in applications such as statistics, machine learning, and signal processing, especially for solving over- or under-determined systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a non-square matrix A\n",
    "a = np.array(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute the Moore-Penrose pseudoinverse\n",
    "A_pinv = np.linalg.pinv(a)\n",
    "\n",
    "# verify identity matrix\n",
    "identity_approx = a @ A_pinv\n",
    "\n",
    "# log\n",
    "print(f\"A_pinv:\\n{A_pinv}\\n\")\n",
    "print(f\"identity_approx:\\n{identity_approx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_5_'></a>[Trace](#toc0_)\n",
    "\n",
    "- The **trace** of a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, denoted as $\\operatorname{tr}(\\mathbf{A})$, is the sum of its diagonal elements:\n",
    "  $$\n",
    "  \\operatorname{tr}(\\mathbf{A}) = \\sum_{i=1}^n A_{ii}\n",
    "  $$\n",
    "\n",
    "üî¢ **Key Properties:**\n",
    "\n",
    "- The trace is a **linear operator**:\n",
    "  $$\n",
    "  \\operatorname{tr}(\\mathbf{A} + \\mathbf{B}) = \\operatorname{tr}(\\mathbf{A}) + \\operatorname{tr}(\\mathbf{B})\n",
    "  $$\n",
    "  $$\n",
    "  \\operatorname{tr}(c \\mathbf{A}) = c \\operatorname{tr}(\\mathbf{A}), \\quad c \\in \\mathbb{R}\n",
    "  $$\n",
    "\n",
    "- The trace of a product is invariant under cyclic permutations:\n",
    "  $$\n",
    "  \\operatorname{tr}(\\mathbf{A} \\mathbf{B}) = \\operatorname{tr}(\\mathbf{B} \\mathbf{A})\n",
    "  $$\n",
    "\n",
    "- This property generalizes to multiple matrices:\n",
    "  $$\n",
    "  \\operatorname{tr}(\\mathbf{A} \\mathbf{B} \\mathbf{C}) = \\operatorname{tr}(\\mathbf{C} \\mathbf{A} \\mathbf{B}) = \\operatorname{tr}(\\mathbf{B} \\mathbf{C} \\mathbf{A})\n",
    "  $$\n",
    "\n",
    "üìê **Interpretation:**\n",
    "\n",
    "- The trace equals the sum of the eigenvalues of $\\mathbf{A}$ (counted with multiplicity).\n",
    "- It is often used in matrix calculus, statistics (e.g., sum of variances), and physics to summarize diagonal dominance or total effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [0, 4, 5],\n",
    "        [0, 0, 6],\n",
    "    ]\n",
    ")\n",
    "\n",
    "b = np.array(\n",
    "    [\n",
    "        [7, 8, 9],\n",
    "        [0, 1, 2],\n",
    "        [0, 0, 3],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute traces\n",
    "trace_A = np.trace(a)\n",
    "trace_B = np.trace(b)\n",
    "\n",
    "# trace linearity: tr(A + B) == tr(A) + tr(B)\n",
    "trace_sum = np.trace(a + b)\n",
    "\n",
    "# scalar multiplication: tr(cA) == c * tr(A)\n",
    "c = 2.5\n",
    "trace_scaled = np.trace(c * a)\n",
    "\n",
    "# cyclic permutation property: tr(AB) == tr(BA)\n",
    "trace_AB = np.trace(a @ b)\n",
    "trace_BA = np.trace(b @ a)\n",
    "\n",
    "# log\n",
    "print(f\"trace_A      : {trace_A}\")\n",
    "print(f\"trace_B      : {trace_B}\")\n",
    "print(f\"trace_sum    : {trace_sum}\")\n",
    "print(f\"trace_scaled : {trace_scaled}\")\n",
    "print(f\"trace_AB     : {trace_AB}\")\n",
    "print(f\"trace_BA     : {trace_BA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_6_'></a>[Matrix Rank](#toc0_)\n",
    "\n",
    "- The **rank** of a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is defined as:\n",
    "  - The maximum number of **linearly independent columns** of $\\mathbf{A}$, or equivalently,\n",
    "  - The maximum number of **linearly independent rows** of $\\mathbf{A}$.\n",
    "\n",
    "üî¢ **Key Properties:**\n",
    "\n",
    "- $\\operatorname{rank}(\\mathbf{A}) \\leq \\min(m, n)$.\n",
    "- The rank equals the number of **non-zero singular values** of $\\mathbf{A}$ (from its Singular Value Decomposition).\n",
    "- The **column rank** and **row rank** are always equal (Fundamental Theorem of Linear Algebra).\n",
    "- Rank measures the dimension of the vector space spanned by the columns (or rows) of $\\mathbf{A}$.\n",
    "- A **full rank** matrix has rank equal to $\\min(m, n)$, meaning its columns (or rows) are linearly independent.\n",
    "- For square matrices, full rank implies the matrix is **invertible** (nonsingular).\n",
    "\n",
    "üìê **Interpretation:**\n",
    "\n",
    "- The rank indicates how much information or \"degrees of freedom\" the matrix carries.\n",
    "- Low-rank matrices represent data with redundancies or dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a full rank 3x3 matrix\n",
    "a = np.array(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 10],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute rank\n",
    "rank_A = np.linalg.matrix_rank(a)\n",
    "\n",
    "# log\n",
    "print(f\"rank_A: {rank_A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Orthogonality & Unitarity](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_1_'></a>[Orthogonality](#toc0_)\n",
    "\n",
    "- **Orthogonality** describes a notion of **perpendicularity** between vectors or matrix rows/columns in a vector space.\n",
    "- Two vectors $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n$ are **orthogonal** if their **dot product** (inner product) is zero:\n",
    "  $$\n",
    "  \\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^\\top \\mathbf{b} = 0\n",
    "  $$\n",
    "\n",
    "- A set of vectors $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_k\\}$ is **orthonormal** if they are mutually orthogonal and each has unit norm:\n",
    "  $$\n",
    "  \\mathbf{u}_i^\\top \\mathbf{u}_j =\n",
    "  \\begin{cases}\n",
    "  1 & \\text{if } i = j \\\\\n",
    "  0 & \\text{if } i \\neq j\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- This means the vectors are not only **perpendicular** but also **normalized** (length equals 1).\n",
    "\n",
    "- A square matrix $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ is called **orthogonal** if its columns (and rows) form an orthonormal set, satisfying:\n",
    "  $$\n",
    "  \\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{Q} \\mathbf{Q}^\\top = \\mathbf{I}\n",
    "  $$\n",
    "  which implies:\n",
    "  $$\n",
    "  \\mathbf{Q}^{-1} = \\mathbf{Q}^\\top\n",
    "  $$\n",
    "\n",
    "- Orthogonal matrices **preserve vector norms and angles**, making them essential in many linear transformations such as Fourier transforms and Singular Value Decomposition (SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 0, 0])\n",
    "b = np.array([0, 1, 0])\n",
    "\n",
    "# normalize vectors to make them unit length (orthonormal)\n",
    "a_norm = a / np.linalg.norm(a)\n",
    "b_norm = b / np.linalg.norm(b)\n",
    "\n",
    "# check orthogonality (dot product)\n",
    "dot_ab = np.dot(a, b)\n",
    "dot_norm = np.dot(a_norm, b_norm)\n",
    "\n",
    "# log\n",
    "print(f\"dot_ab   : {dot_ab}\")\n",
    "print(f\"dot_norm : {dot_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array(\n",
    "    [\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# verify orthogonality: Q.T @ Q = I\n",
    "identity_check = Q.T @ Q\n",
    "\n",
    "# log\n",
    "print(f\"identity_check:\\n{identity_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_2_'></a>[Unitary](#toc0_)\n",
    "\n",
    "- **Unitary matrices** are the complex-valued generalization of orthogonal matrices.\n",
    "- They preserve vector lengths and orthogonality, but in the **complex domain**.\n",
    "- A matrix $\\mathbf{U}$ is **unitary** if it satisfies:\n",
    "  $$\n",
    "  \\mathbf{U} \\mathbf{U}^* = \\mathbf{U}^* \\mathbf{U} = \\mathbf{I}\n",
    "  $$\n",
    "  which implies:\n",
    "  $$\n",
    "  \\mathbf{U}^* = \\mathbf{U}^{-1}\n",
    "  $$\n",
    "  where:\n",
    "  - $\\mathbf{U}^*$ (also written $\\mathbf{U}^H$) is the **conjugate transpose** (Hermitian transpose) of $\\mathbf{U}$.\n",
    "\n",
    "‚úçÔ∏è **Notes:**\n",
    "\n",
    "- For real-valued matrices, unitary matrices reduce to **orthogonal matrices**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a 2x2 complex unitary matrix U\n",
    "theta = np.pi / 4\n",
    "U = np.array(\n",
    "    [\n",
    "        [np.exp(1j * theta), 0],\n",
    "        [0, np.exp(-1j * theta)],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute conjugate transpose (hermitian transpose)\n",
    "U_H = np.conjugate(U.T)\n",
    "\n",
    "# log\n",
    "print(f\"U:\\n{U}\\n\")\n",
    "print(f\"U_H:\\n{U_H}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify unitary properties\n",
    "UUH = U @ U_H\n",
    "UHU = U_H @ U\n",
    "\n",
    "# log\n",
    "print(f\"UUH:\\n{UUH}\\n\")\n",
    "print(f\"UHU:\\n{UHU}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_3_'></a>[Hermitian](#toc0_)\n",
    "\n",
    "- A **Hermitian matrix** $\\mathbf{H}$ is a square matrix (possibly with complex entries) that satisfies the condition:\n",
    "  $$\n",
    "  \\mathbf{H} = \\mathbf{H}^*\n",
    "  $$\n",
    "  where $\\mathbf{H}^*$ is the **conjugate transpose** (Hermitian transpose) of $\\mathbf{H}$.\n",
    "\n",
    "- This means the element at position $(i, j)$ is the complex conjugate of the element at $(j, i)$:\n",
    "  $$\n",
    "  H_{ij} = \\overline{H_{ji}}\n",
    "  $$\n",
    "\n",
    "‚úçÔ∏è **Notes:**\n",
    "\n",
    "- Hermitian matrices are the complex analogue of real symmetric matrices.\n",
    "- All eigenvalues of a Hermitian matrix are **real**.\n",
    "- Hermitian matrices play a key role in quantum mechanics and signal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a 2x2 hermitian matrix H\n",
    "H = np.array(\n",
    "    [\n",
    "        [2 + 0j, 1 - 1j],\n",
    "        [1 + 1j, 3 + 0j],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute conjugate transpose (hermitian transpose)\n",
    "H_H = np.conjugate(H.T)\n",
    "\n",
    "# log\n",
    "print(f\"H:\\n{H}\\n\")\n",
    "print(f\"Hermitian transpose (H^*):\\n{H_H}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Matrix Structures](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_1_'></a>[Symmetric Matrices](#toc0_)\n",
    "\n",
    "- A matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **symmetric** if it equals its transpose:\n",
    "  $$\n",
    "  \\mathbf{A}^\\top = \\mathbf{A}\n",
    "  $$\n",
    "\n",
    "- Symmetric matrices have several important properties:\n",
    "  - All eigenvalues of $\\mathbf{A}$ are **real**.\n",
    "  - $\\mathbf{A}$ is **diagonalizable** by an orthogonal matrix; that is, there exists an orthogonal matrix $\\mathbf{Q}$ such that\n",
    "    $$\n",
    "    \\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^\\top\n",
    "    $$\n",
    "    where $\\mathbf{\\Lambda}$ is a diagonal matrix containing the eigenvalues.\n",
    "\n",
    "- These properties make symmetric matrices fundamental in optimization, physics, and numerical methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a symmetric matrix A (real-valued)\n",
    "a = np.array(\n",
    "    [\n",
    "        [4, 1, 2],\n",
    "        [1, 3, 0],\n",
    "        [2, 0, 5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute transpose of A\n",
    "A_T = a.T\n",
    "\n",
    "# log\n",
    "print(f\"A:\\n{a}\\n\")\n",
    "print(f\"Transpose of A:\\n{A_T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_2_'></a>[Positive Semi-Definite (PSD)](#toc0_)\n",
    "\n",
    "- A symmetric matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is **positive semi-definite (PSD)** if:\n",
    "  $$\n",
    "  \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} \\geq 0 \\quad \\forall \\mathbf{x} \\in \\mathbb{R}^n\n",
    "  $$\n",
    "\n",
    "- Key properties of PSD matrices:\n",
    "  - All eigenvalues of $\\mathbf{A}$ are **non-negative** (i.e., $\\lambda_i \\geq 0$).\n",
    "  - PSD matrices are symmetric by definition.\n",
    "  - For any matrix $\\mathbf{B}$, the matrix $\\mathbf{B}^\\top \\mathbf{B}$ is always symmetric PSD.\n",
    "\n",
    "- PSD matrices frequently appear in optimization, covariance matrices in statistics, and kernel methods in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a symmetric matrix A\n",
    "a = np.array(\n",
    "    [\n",
    "        [2, -1, 0],\n",
    "        [-1, 2, -1],\n",
    "        [0, -1, 2],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# check symmetry\n",
    "is_symmetric = np.array_equal(a, a.T)\n",
    "\n",
    "# check PSD property via eigenvalues\n",
    "eigenvalues = np.linalg.eigvalsh(a)  # eigvalsh for symmetric matrices\n",
    "is_psd = np.all(eigenvalues >= 0)\n",
    "\n",
    "# Log\n",
    "print(f\"A:\\n{a}\")\n",
    "print(f\"is_symmetric : {is_symmetric}\")\n",
    "print(f\"eigenvalues  : {eigenvalues}\")\n",
    "print(f\"is_psd       : {is_psd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_3_'></a>[Vandermonde Structure](#toc0_)\n",
    "\n",
    "- A **Vandermonde matrix** is a special structured matrix where each row consists of successive powers of a given set of numbers.\n",
    "\n",
    "üî¢ **Mathematical Definition:**\n",
    "\n",
    "- Given scalars $x_1, x_2, \\dots, x_n$, the Vandermonde matrix $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ is defined as:\n",
    "  $$\n",
    "  \\mathbf{V} = \\begin{bmatrix}\n",
    "  1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} \\\\\n",
    "  1 & x_2 & x_2^2 & \\cdots & x_2^{n-1} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  1 & x_n & x_n^2 & \\cdots & x_n^{n-1}\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "üìê **Properties:**\n",
    "\n",
    "- The determinant of a Vandermonde matrix has a closed form:\n",
    "  $$\n",
    "  \\det(\\mathbf{V}) = \\prod_{1 \\leq i < j \\leq n} (x_j - x_i)\n",
    "  $$\n",
    "  which is nonzero if all $x_i$ are distinct, implying $\\mathbf{V}$ is invertible.\n",
    "\n",
    "**Applications in Digital Image Processing (DIP) and Transforms:**\n",
    "\n",
    "- Vandermonde structures appear in polynomial interpolation, signal processing, and coding theory.\n",
    "- They are related to transformation matrices in the Discrete Fourier Transform (DFT), Chebyshev polynomial transforms, and other orthogonal polynomial expansions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[Matrix Decompositions (Factorization)](#toc0_)\n",
    "\n",
    "<table style=\"margin: 0 auto;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"text-align: center;\">Function</th>\n",
    "      <th style=\"text-align: center;\">Description</th>\n",
    "      <th style=\"text-align: center;\">Details</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.eig</code></td>\n",
    "      <td>Compute the eigenvalues and right eigenvectors of a square array</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.eigh</code></td>\n",
    "      <td>Return the eigenvalues and eigenvectors of a complex Hermitian (conjugate symmetric) or a real symmetric matrix</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.eigvals</code></td>\n",
    "      <td>Compute the eigenvalues of a general matrix</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigvals.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.eigvalsh</code></td>\n",
    "      <td>Compute the eigenvalues of a complex Hermitian or real symmetric matrix</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigvalsh.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.cholesky</code></td>\n",
    "      <td>Cholesky decomposition</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.qr</code></td>\n",
    "      <td>Compute the qr factorization of a matrix</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.qr.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.svd</code></td>\n",
    "      <td>Returns the singular values of a matrix (or a stack of matrices) <code>x</code></td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\">link</a></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_1_'></a>[Eigen-decomposition](#toc0_)\n",
    "\n",
    "Given a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, an **eigenvalue** $\\lambda \\in \\mathbb{R}$ and a corresponding **eigenvector** $\\mathbf{v} \\neq \\mathbf{0}$ satisfy:\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "- Geometrically, $\\mathbf{A}$ scales the vector $\\mathbf{v}$ by $\\lambda$ without changing its direction.\n",
    "- The **spectral theorem** states that symmetric matrices have **real eigenvalues** and a complete set of **orthonormal eigenvectors**.\n",
    "\n",
    "üî¢ **Computing Eigenvalues and Eigenvectors:**\n",
    "\n",
    "- Eigenvalues $\\lambda$ are found by solving the characteristic equation:\n",
    "  $$\n",
    "  \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\n",
    "  $$\n",
    "- For each eigenvalue $\\lambda$, the corresponding eigenvectors $\\mathbf{v}$ satisfy:\n",
    "  $$\n",
    "  (\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{v} = \\mathbf{0}\n",
    "  $$\n",
    "\n",
    "üìê **Significance:**\n",
    "\n",
    "- Eigenvalues indicate the scaling factor applied to eigenvectors during the transformation by $\\mathbf{A}$.\n",
    "- Eigenvectors represent directions invariant under the transformation.\n",
    "- Eigen-decomposition is fundamental in techniques like Principal Component Analysis (PCA) for dimensionality reduction.\n",
    "- In signal processing, Fourier transform basis functions can be viewed as eigenfunctions of the shift operator, revealing frequency components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a symmetric matrix A\n",
    "a = np.array(\n",
    "    [\n",
    "        [4, 1, 2],\n",
    "        [1, 2, 0],\n",
    "        [2, 0, 3],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(a)\n",
    "\n",
    "# log\n",
    "print(f\"eigenvalues:\\n{eigenvalues}\\n\")\n",
    "print(f\"eigenvectors:\\n{eigenvectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form diagonal matrix of eigenvalues\n",
    "Lambda = np.diag(eigenvalues)\n",
    "\n",
    "# reconstruct A: A ‚âà Q Œõ Q·µó\n",
    "A_reconstructed = eigenvectors @ Lambda @ eigenvectors.T\n",
    "\n",
    "# log\n",
    "print(f\"A_reconstructed:\\n{A_reconstructed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_2_'></a>[Singular Value Decomposition (SVD)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_5_2_1_'></a>[Definition](#toc0_)\n",
    "\n",
    "For any real matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the **Singular Value Decomposition (SVD)** expresses it as:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix (left singular vectors),\n",
    "- $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with non-negative real values (singular values),\n",
    "- $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix (right singular vectors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3x3 Laplacian filter kernel\n",
    "a = np.array([\n",
    "    [0,  1, 0],\n",
    "    [1, -4, 1],\n",
    "    [0,  1, 0]\n",
    "])\n",
    "\n",
    "# compute SVD\n",
    "U, S, Vt = np.linalg.svd(a)\n",
    "\n",
    "# construct Œ£ with same shape as A\n",
    "sigma = np.zeros_like(a, dtype=np.float32)\n",
    "np.fill_diagonal(sigma, S)\n",
    "\n",
    "# log\n",
    "print(f\"A:\\n{a}\\n\")\n",
    "print(f\"U:\\n{U}\\n\")\n",
    "print(f\"Œ£:\\n{sigma}\\n\")\n",
    "print(f\"V·µó:\\n{Vt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_5_2_2_'></a>[General Rank-$r$ SVD](#toc0_)\n",
    "\n",
    "If $\\mathbf{A}$ has rank $r$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\n",
    "$$\n",
    "\n",
    "Each term is a **rank-1 matrix**. The sum reconstructs $\\mathbf{A}$ exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct A\n",
    "A_reconstructed = U @ sigma @ Vt\n",
    "\n",
    "# log\n",
    "print(\"A_reconstructed:\\n\", A_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_5_2_3_'></a>[Connection to Eigen-Decomposition](#toc0_)\n",
    "\n",
    "- $\\mathbf{A}^\\top \\mathbf{A}$ and $\\mathbf{A} \\mathbf{A}^\\top$ are symmetric and positive semi-definite matrices.\n",
    "- Their eigenvalues are non-negative: $\\lambda_i \\geq 0$.\n",
    "- The singular values of $\\mathbf{A}$ are given by $\\sigma_i = \\sqrt{\\lambda_i}$, where $\\lambda_i$ are the eigenvalues of $\\mathbf{A}^\\top \\mathbf{A}$.\n",
    "- The columns of $\\mathbf{V}$ (right singular vectors) are the eigenvectors of $\\mathbf{A}^\\top \\mathbf{A}$.\n",
    "- The columns of $\\mathbf{U}$ (left singular vectors) are the eigenvectors of $\\mathbf{A} \\mathbf{A}^\\top$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a non-square matrix A (m √ó n)\n",
    "a = np.array([\n",
    "    [3, 1],\n",
    "    [1, 3],\n",
    "    [2, 2]\n",
    "])\n",
    "\n",
    "# perform SVD: A = U @ Œ£ @ Vt\n",
    "U, S, Vt = np.linalg.svd(a)\n",
    "\n",
    "# construct Œ£ (m √ó n) with singular values on the diagonal\n",
    "Sigma = np.zeros_like(a, dtype=np.float64)\n",
    "np.fill_diagonal(Sigma, S)\n",
    "\n",
    "# compute A^T A and A A^T\n",
    "AtA = a.T @ a\n",
    "AAt = a @ a.T\n",
    "\n",
    "# eigen-decomposition\n",
    "eigvals_AtA, eigvecs_AtA = np.linalg.eigh(AtA)\n",
    "eigvals_AAt, eigvecs_AAt = np.linalg.eigh(AAt)\n",
    "\n",
    "# sort eigenvalues and eigenvectors in descending order\n",
    "idx_AtA = np.argsort(eigvals_AtA)[::-1]\n",
    "eigvals_AtA = eigvals_AtA[idx_AtA]\n",
    "eigvecs_AtA = eigvecs_AtA[:, idx_AtA]\n",
    "\n",
    "idx_AAt = np.argsort(eigvals_AAt)[::-1]\n",
    "eigvals_AAt = eigvals_AAt[idx_AAt]\n",
    "eigvecs_AAt = eigvecs_AAt[:, idx_AAt]\n",
    "\n",
    "# singular values from eigendecomposition of A^T A\n",
    "singular_values_from_eig = np.sqrt(eigvals_AtA)\n",
    "\n",
    "# log\n",
    "print(f\"S                        : {S}\")\n",
    "print(f\"singular_values_from_eig : {singular_values_from_eig}\\n\")\n",
    "print(f\"Vt.T:\\n{Vt.T}\")\n",
    "print(f\"\\neigvecs_AtA:\\n{eigvecs_AtA}\\n\")\n",
    "print(f\"\\nU:\\n{U}\")\n",
    "print(f\"\\neigvecs_AAt:\\n{eigvecs_AAt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_3_'></a>[QR Decomposition](#toc0_)\n",
    "\n",
    "- QR decomposition factors any matrix $\\mathbf{A}$ into the product of an **orthogonal matrix** $\\mathbf{Q}$ and an **upper triangular matrix** $\\mathbf{R}$.\n",
    "- It is fundamental for solving linear systems, least squares problems, and eigenvalue computations with superior numerical stability.\n",
    "\n",
    "üî¢ **Mathematical Definition:**\n",
    "\n",
    "- For any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ (with $m \\geq n$):\n",
    "  $$\\mathbf{A} = \\mathbf{QR}$$\n",
    "\n",
    "- Where:\n",
    "  - $\\mathbf{Q} \\in \\mathbb{R}^{m \\times m}$: orthogonal matrix ($\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}$)\n",
    "  - $\\mathbf{R} \\in \\mathbb{R}^{m \\times n}$: upper triangular matrix\n",
    "\n",
    "üìä **Two Forms:**\n",
    "\n",
    "- **Full (thick) QR**:\n",
    "  $$\\mathbf{A}_{m \\times n} = \\mathbf{Q}_{m \\times m} \\mathbf{R}_{m \\times n}$$\n",
    "\n",
    "- **Reduced (thin) QR**:\n",
    "  $$\\mathbf{A}_{m \\times n} = \\mathbf{Q}_{m \\times n} \\mathbf{R}_{n \\times n}$$\n",
    "\n",
    "- The reduced form is more memory-efficient when $m \\gg n$\n",
    "\n",
    "üîß **Key Applications:**\n",
    "\n",
    "- **Solving Linear Systems**: For $\\mathbf{Ax} = \\mathbf{b}$:\n",
    "  1. Factor: $\\mathbf{A} = \\mathbf{QR}$\n",
    "  1. Substitute: $\\mathbf{QR}\\mathbf{x} = \\mathbf{b}$\n",
    "  1. Multiply by $\\mathbf{Q}^T$: $\\mathbf{R}\\mathbf{x} = \\mathbf{Q}^T\\mathbf{b}$\n",
    "  1. Back-substitute to solve triangular system\n",
    "\n",
    "- **Least Squares**: Minimizes $\\|\\mathbf{Ax} - \\mathbf{b}\\|^2$ efficiently:\n",
    "  $$\\mathbf{x} = \\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{b}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3x3 Laplacian filter kernel\n",
    "a = np.array([\n",
    "    [0,  1, 0],\n",
    "    [1, -4, 1],\n",
    "    [0,  1, 0]\n",
    "])\n",
    "\n",
    "# compute QR decomposition\n",
    "Q, R = np.linalg.qr(a)\n",
    "\n",
    "# log\n",
    "print(f\"A:\\n{a}\\n\")\n",
    "print(f\"Q:\\n{Q}\\n\")\n",
    "print(f\"R:\\n{R}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_4_'></a>[Cholesky Decomposition](#toc0_)\n",
    "\n",
    "- Cholesky decomposition factors a **positive definite matrix** into the product of a lower triangular matrix and its transpose.\n",
    "- It is the most efficient method for solving linear systems with symmetric positive definite matrices, requiring only half the operations of LU decomposition.\n",
    "\n",
    "üî¢ **Mathematical Definition:**\n",
    "\n",
    "- For a positive definite matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$:\n",
    "  $$\\mathbf{A} = \\mathbf{LL}^T$$\n",
    "\n",
    "- Where:\n",
    "  - $\\mathbf{L}$: lower triangular matrix with positive diagonal elements\n",
    "  - $\\mathbf{L}^T$: transpose of $\\mathbf{L}$ (upper triangular)\n",
    "\n",
    "üìã **Requirements:**\n",
    "\n",
    "- **Symmetric**: $\\mathbf{A} = \\mathbf{A}^T$\n",
    "- **Positive definite**: $\\mathbf{x}^T\\mathbf{A}\\mathbf{x} > 0$ for all non-zero $\\mathbf{x}$\n",
    "- **Equivalently**: All eigenvalues are positive\n",
    "\n",
    "üîß **Solving Linear Systems:**\n",
    "\n",
    "- For $\\mathbf{Ax} = \\mathbf{b}$ with positive definite $\\mathbf{A}$:\n",
    "  1. **Factor**: $\\mathbf{A} = \\mathbf{LL}^T$\n",
    "  1. **Forward substitution**: Solve $\\mathbf{Ly} = \\mathbf{b}$ for $\\mathbf{y}$\n",
    "  1. **Backward substitution**: Solve $\\mathbf{L}^T\\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$\n",
    "\n",
    "  $$\\mathbf{LL}^T\\mathbf{x} = \\mathbf{b}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive definite matrix\n",
    "a = np.array([\n",
    "    [4,  12, -16],\n",
    "    [12, 37, -43],\n",
    "    [-16, -43, 98]\n",
    "])\n",
    "\n",
    "# psd check\n",
    "is_positive_definite = np.all(np.linalg.eigvals(a) > 0)\n",
    "\n",
    "# compute Cholesky decomposition\n",
    "L = np.linalg.cholesky(a)\n",
    "\n",
    "# log\n",
    "print(f\"is_positive_definite: {is_positive_definite}\")\n",
    "print(f\"A:\\n{a}\\n\")\n",
    "print(f\"L:\\n{L}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_6_'></a>[Solving Linear Systems](#toc0_)\n",
    "\n",
    "- Solving linear systems of equations $\\mathbf{Ax} = \\mathbf{b}$ is a fundamental problem in linear algebra with applications across science and engineering.\n",
    "- NumPy provides efficient methods for both exact solutions and least squares approximations.\n",
    "\n",
    "üéØ **Problem Setup:**\n",
    "\n",
    "- **System of equations**: $\\mathbf{Ax} = \\mathbf{b}$ where:\n",
    "  - $\\mathbf{A}$: coefficient matrix\n",
    "  - $\\mathbf{x}$: unknown vector (to solve for) \n",
    "  - $\\mathbf{b}$: right-hand side vector\n",
    "\n",
    "üîß **Solutions:**\n",
    "\n",
    "- **Exact**:\n",
    "  - Solves $\\mathbf{Ax} = \\mathbf{b}$ for **square, invertible** matrices\n",
    "  - Uses LU decomposition with partial pivoting\n",
    "  - **Requirements**: $\\mathbf{A}$ must be square ($n \\times n$) and non-singular\n",
    "  - **Returns**: Exact solution vector $\\mathbf{x}$\n",
    "\n",
    "  $$\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$$\n",
    "\n",
    "- **Least Squares**:\n",
    "  - Solves $\\mathbf{Ax} = \\mathbf{b}$ in the **least squares sense**\n",
    "  - Works for any matrix dimensions (square, overdetermined, underdetermined)\n",
    "  - Uses SVD decomposition for robust computation\n",
    "  - **Returns**: solution, residuals, rank, singular values\n",
    "  - **Cases:**\n",
    "    - **Overdetermined** ($m > n$): Minimizes $\\|\\mathbf{Ax} - \\mathbf{b}\\|^2$\n",
    "    - **Underdetermined** ($m < n$): Returns minimum norm solution\n",
    "    - **Square**: Same as `solve()` if matrix is invertible\n",
    "\n",
    "  $$\\mathbf{x} = \\arg\\min_{\\mathbf{x}} \\|\\mathbf{Ax} - \\mathbf{b}\\|^2$$\n",
    "\n",
    "üíª **Usage Guidelines:**\n",
    "\n",
    "- **Use `np.linalg.solve()`** when:\n",
    "  - Matrix is square and you expect exact solution\n",
    "  - System is well-conditioned\n",
    "  - Speed is important\n",
    "\n",
    "- **Use `np.linalg.lstsq()`** when:\n",
    "  - Matrix is not square\n",
    "  - System might be inconsistent\n",
    "  - You need robust handling of singular/near-singular matrices\n",
    "  - You want residual information\n",
    "\n",
    "<table style=\"margin: 0 auto;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"text-align: center;\">Function</th>\n",
    "      <th style=\"text-align: center;\">Description</th>\n",
    "      <th style=\"text-align: center;\">Details</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.solve</code></td>\n",
    "      <td>Solve a linear matrix equation, or system of linear scalar equations</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html\">link</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><code>np.linalg.lstsq</code></td>\n",
    "      <td>Return the least-squares solution to a linear matrix equation</td>\n",
    "      <td style=\"text-align: center;\"><a href=\"https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\">link</a></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient matrix\n",
    "A = np.array([\n",
    "    [3,  1, -1],\n",
    "    [1,  4,  2],\n",
    "    [2, -1,  5]\n",
    "])\n",
    "\n",
    "# right-hand side vector\n",
    "b = np.array([4, 15, 19])\n",
    "\n",
    "# solve the system Ax = b\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "# verify solution\n",
    "b_reconstructed = A @ x\n",
    "\n",
    "# log\n",
    "print(f\"A:\\n{A}\\n\")\n",
    "print(f\"b     : {b}\")\n",
    "print(f\"x     : {x}\")\n",
    "print(f\"A @ x : {b_reconstructed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design matrix (overdetermined system)\n",
    "A = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6],\n",
    "    [7, 8]\n",
    "])\n",
    "\n",
    "# observation vector\n",
    "b = np.array([3, 7, 11, 15])\n",
    "\n",
    "# solve using least squares\n",
    "x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "\n",
    "# verify solution\n",
    "b_estimated = A @ x\n",
    "residual_norm = np.linalg.norm(b - b_estimated)\n",
    "\n",
    "# log\n",
    "print(f\"A:\\n{A}\\n\")\n",
    "print(f\"b               : {b}\")\n",
    "print(f\"x               : {x}\")\n",
    "print(f\"A @ x           : {b_estimated}\")\n",
    "print(f\"Residuals       : {residuals}\")\n",
    "print(f\"Rank of A       : {rank}\")\n",
    "print(f\"Singular values : {s}\")\n",
    "print(f\"Residual norm   : {residual_norm}\")"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
