{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/pytorch-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"text-align: right; flex: 1;\">\n",
    "        <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../assets/images/pytorch/logo/pytorch-logo-dark.svg\" \n",
    "                 alt=\"PyTorch Logo\"\n",
    "                 style=\"max-height: 48px; width: auto; background-color: #ffffff; border-radius: 8px;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Convolutional Neural Network (CNN)](#toc2_)    \n",
    "  - [Convolution vs. Correlation](#toc2_1_)    \n",
    "  - [Basic Concepts](#toc2_2_)    \n",
    "    - [Padding](#toc2_2_1_)    \n",
    "    - [Stride](#toc2_2_2_)    \n",
    "    - [Dilation](#toc2_2_3_)    \n",
    "  - [Popular CNN Architectures](#toc2_3_)    \n",
    "    - [Classic / Foundational](#toc2_3_1_)    \n",
    "    - [Deeper & Structured CNNs](#toc2_3_2_)    \n",
    "    - [Efficient / Modern CNNs](#toc2_3_3_)    \n",
    "  - [Convolution in PyTorch](#toc2_4_)    \n",
    "    - [1D Correlation](#toc2_4_1_)    \n",
    "    - [2D Correlation](#toc2_4_2_)    \n",
    "  - [CNN Implementation](#toc2_5_)    \n",
    "    - [Using PyTorch](#toc2_5_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable automatic figure display (plt.show() required)  \n",
    "# this ensures consistency with .py scripts and gives full control over when plots appear\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for deterministic results\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# log\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Convolutional Neural Network (CNN)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Convolution vs. Correlation](#toc0_)\n",
    "\n",
    "- Convolution and correlation are both operations used in signal processing and image analysis\n",
    "\n",
    "**[Convolution](https://en.wikipedia.org/wiki/Convolution)**:\n",
    "\n",
    "- Convolution measures how one function (the kernel) modifies the other function (the signal or image).\n",
    "- In the context of image processing, it's used to apply a filter or kernel to an image.\n",
    "- Mathematical Formulation (discrete signals):\n",
    "   $$[f * g](i) = \\sum_{j} f[j] \\cdot g[i - j]$$\n",
    "\n",
    "**[Correlation](https://en.wikipedia.org/wiki/Correlation)**:\n",
    "\n",
    "- Correlation measures the similarity between two signals as one is shifted over the other.\n",
    "- In image processing, it's used to detect patterns by sliding a filter over an image.\n",
    "- Mathematical Formulation (discrete signals):\n",
    "   $$[f \\star g](i) = \\sum_{j} f[j] \\cdot g[i + j]$$\n",
    "\n",
    "<div style=\"text-align: center; padding-top: 10px;\">\n",
    "    <img src=\"../assets/images/original/lti/corr-vs-conv.svg\" alt=\"corr-vs-conv.svg\" style=\"min-width: 512px; width: 70%; height: auto; border-radius: 16px;\">\n",
    "    <p><em>Figure 1: Correlation vs. Convolution</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Basic Concepts](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[Padding](#toc0_)\n",
    "\n",
    "- It refers to adding extra values (usually zeros) around the input tensor (signal or image) before applying the convolution operation\n",
    "- Padding is used to control the size of the output and to allow the kernel to process the edges of the input\n",
    "- `padding='same'`\n",
    "  - To ensure that the output of the convolution operation has the same spatial dimensions (width and height for 2D convolutions, length for 1D convolutions) as the input\n",
    "    $$p = \\left\\lceil \\frac{k - 1}{2} \\right\\rceil$$\n",
    "- `padding='valid'`\n",
    "  - Means no padding is applied to the input\n",
    "    $$\\text{Output Size} = \\left\\lfloor \\frac{\\text{Input Size} - k}{s} + 1 \\right\\rfloor$$\n",
    "\n",
    "<div style=\"text-align: center; padding-top: 10px;\">\n",
    "    <img src=\"../assets/images/original/lti/padding.svg\" alt=\"padding.svg\" style=\"min-width: 512px; width: 70%; height: auto; border-radius: 16px;\">\n",
    "    <p><em>Figure 2: Padding for Convolution</em></p>\n",
    "</div>\n",
    "\n",
    "üìù **More details**:\n",
    "\n",
    "- https://medium.com/analytics-vidhya/convolution-padding-stride-and-pooling-in-cnn-13dc1f3ada26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[Stride](#toc0_)\n",
    "\n",
    "- It defines how much the kernel moves over the input tensor during the convolution\n",
    "- A stride of `1` means the kernel moves one step at a time, fully overlapping with each adjacent position\n",
    "- A stride of `2` means the kernel skips one element at a time, leading to downsampling (reducing the size of the output)\n",
    "\n",
    "üìù **More details**:\n",
    "\n",
    "- https://medium.com/analytics-vidhya/convolution-padding-stride-and-pooling-in-cnn-13dc1f3ada26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[Dilation](#toc0_)\n",
    "\n",
    "- It introduces gaps between the elements of the kernel, effectively \"spreading out\" the kernel\n",
    "- This allows the kernel to cover a larger area of the input without increasing the number of parameters (kernel size)\n",
    "- Dilation is useful for capturing long-range dependencies in the input.\n",
    "\n",
    "<div style=\"text-align: center; padding-top: 10px;\">\n",
    "    <img src=\"../assets/images/original/lti/dilation.svg\" alt=\"dilation.svg\" style=\"min-width: 512px; width: 70%; height: auto; border-radius: 16px;\">\n",
    "    <p><em>Figure 3: Dilation for Convolution</em></p>\n",
    "</div>\n",
    "\n",
    "üìù **More details**:\n",
    "\n",
    "- https://medium.com/@akp83540/dilation-rate-in-a-convolution-operation-a7143e437654\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Popular CNN Architectures](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_1_'></a>[Classic / Foundational](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LeNet-5 (1998)**\n",
    "\n",
    "- Proposed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) at [AT&T Bell Labs](https://en.wikipedia.org/wiki/Bell_Labs).\n",
    "- **LeNet-5** is one of the earliest successful Convolutional Neural Networks (CNNs), designed for handwritten digit recognition.\n",
    "- It introduced core CNN principles such as local receptive fields, weight sharing, and spatial pooling, forming the foundation of modern convolutional architectures.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [Gradient-Based Learning Applied to Document Recognition [paper]](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "- Check detailed info in [**lenet5.ipynb**](./models/cnn/lenet5.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AlexNet (2012)**\n",
    "\n",
    "- Proposed by [Alex Krizhevsky](https://en.wikipedia.org/wiki/Alex_Krizhevsky), [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever), and [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) at the [University of Toronto](https://en.wikipedia.org/wiki/University_of_Toronto).\n",
    "- **AlexNet** is a deep Convolutional Neural Network that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 by a large margin.\n",
    "- It demonstrated the effectiveness of deep CNNs trained on GPUs and introduced key techniques such as ReLU activation, Dropout for regularization, and data augmentation.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [ImageNet Classification with Deep Convolutional Neural Networks [paper]](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "- Check detailed info in [**alexnet.ipynb**](./models/cnn/alexnet.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_2_'></a>[Deeper & Structured CNNs](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGGNet (2014)**\n",
    "\n",
    "- Proposed by [Karen Simonyan](https://dblp.uni-trier.de/search/author?author=Karen%20Simonyan) and [Andrew Zisserman](https://dblp.uni-trier.de/pid/z/AndrewZisserman.html?q=Andrew%20Zisserman) at [University of Oxford](https://en.wikipedia.org/wiki/University_of_Oxford).\n",
    "- **VGGNet** is a deep Convolutional Neural Network known for its simple and uniform architecture using only $3\\times3$ convolutional layers stacked on top of each other.\n",
    "- It demonstrated that depth is critical for good performance and became a standard benchmark in image classification tasks.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [Very Deep Convolutional Networks for Large-Scale Image Recognition [paper]](https://arxiv.org/abs/1409.1556)\n",
    "- Check detailed info in [**vggnet.ipynb**](./models/cnn/vggnet.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GoogLeNet / Inception v1 (2014)**\n",
    "\n",
    "- Proposed by [Christian Szegedy](https://scholar.google.com/citations?user=bnQMuzgAAAAJ) et al. at [Google Research](https://research.google/) as part of the Inception project.\n",
    "- **GoogLeNet** introduced the Inception module, combining multiple convolutional filters (1√ó1, 3√ó3, 5√ó5) and pooling in parallel to capture multi-scale features efficiently.\n",
    "- It achieved state-of-the-art performance on ImageNet while keeping computational cost low, and popularized deeper, more efficient CNN architectures.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [Going Deeper with Convolutions [paper]](https://arxiv.org/abs/1409.4842)\n",
    "- Check detailed info in [**googlenet.ipynb**](./models/cnn/googlenet.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNet (2015)**\n",
    "\n",
    "- Proposed by [Kaiming He](https://scholar.google.com/citations?user=DhtAFkwAAAAJ&hl=en&oi=sra), [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en&oi=sra), [Shaoqing Ren](https://scholar.google.com/citations?user=AUhj438AAAAJ&hl=en&oi=sra), and [Jian Sun](https://scholar.google.com/citations?user=ALVSZAYAAAAJ&hl=en&oi=sra) at [Microsoft Research](https://www.microsoft.com/en-us/research/).\n",
    "- **ResNet** introduced residual (skip) connections, allowing training of extremely deep networks (up to hundreds of layers) without suffering from vanishing gradients.\n",
    "- It became a foundational architecture for modern CNNs, influencing image recognition, detection, and segmentation tasks.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [Deep Residual Learning for Image Recognition [paper]](https://arxiv.org/abs/1512.03385)\n",
    "- Check detailed info in [**resnet.ipynb**](./models/cnn/resnet.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_3_'></a>[Efficient / Modern CNNs](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DenseNet (2017)**\n",
    "\n",
    "- Proposed by [Gao Huang](https://scholar.google.com.hk/citations?user=-P9LwcgAAAAJ&hl), [Zhuang Liu](https://unknown.org), et al. at [Cornell University](https://www.cornell.edu/).\n",
    "- **DenseNet** introduced dense connectivity, where each layer receives inputs from all preceding layers, improving gradient flow, feature reuse, and parameter efficiency.\n",
    "- It achieved state-of-the-art performance on image classification benchmarks while being more compact than traditional deep networks.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [Densely Connected Convolutional Networks [paper]](https://arxiv.org/abs/1608.06993)\n",
    "- Check detailed info in [**densenet.ipynb**](./models/cnn/densenet.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MobileNet (2017)**\n",
    "\n",
    "- Proposed by [Andrew G. Howard](https://scholar.google.com/citations?user=_9l8vD8AAAAJ&hl=en&oi=sra) et al. at [Google Research](https://research.google/).\n",
    "- **MobileNet** introduced depthwise separable convolutions, drastically reducing the number of parameters and computational cost while maintaining competitive accuracy.\n",
    "- Designed for mobile and embedded devices, it enables efficient real-time image classification and vision tasks.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications [paper]](https://arxiv.org/abs/1704.04861)\n",
    "<!-- - Check detailed info in [**mobilenet.ipynb**](./models/cnn/mobilenet.ipynb) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xception (2017)**\n",
    "\n",
    "- Proposed by [Fran√ßois Chollet](https://scholar.google.com/citations?user=VfYhf2wAAAAJ&hl=en&oi=sra) at [Google Research](https://research.google/).  \n",
    "- **Xception** stands for ‚ÄúExtreme Inception‚Äù and replaces Inception modules with **depthwise separable convolutions**, improving efficiency and performance.  \n",
    "- Designed for modern CNN applications, it achieves high accuracy while reducing model parameters and computational cost compared to conventional Inception architectures.  \n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [Xception: Deep Learning with Depthwise Separable Convolutions [paper]](https://arxiv.org/abs/1610.02357)  \n",
    "<!-- - Check detailed info in [**xception.ipynb**](./models/cnn/xception.ipynb) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EfficientNet (2019)**\n",
    "\n",
    "- Proposed by [Mingxing Tan](https://scholar.google.com/citations?user=6POeyBoAAAAJ&hl=en&oi=sra) and [Quoc V. Le](https://scholar.google.com/citations?user=vfT6-XIAAAAJ&hl=en&oi=sra) at [Google Research](https://research.google/).\n",
    "- **EfficientNet** introduced a compound scaling method that uniformly scales network depth, width, and input resolution to achieve better accuracy with fewer parameters.\n",
    "- It achieved state-of-the-art performance on ImageNet while being highly efficient, inspiring a family of models from EfficientNet-B0 to B7.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks [paper]](https://arxiv.org/abs/1905.11946)\n",
    "- Check detailed info in [**efficientnet.ipynb**](./models/cnn/efficientnet.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvNeXt (2022)**\n",
    "\n",
    "- Proposed by [Zhuang Liu](https://scholar.google.com/citations?user=7OTD-LEAAAAJ&hl=en&oi=sra) et al. at [Facebook AI Research (FAIR)](https://ai.meta.com/).\n",
    "- **ConvNeXt** modernized CNN design by adopting architectural ideas from Vision Transformers while retaining standard convolutional layers, achieving competitive performance with simpler and more efficient models.\n",
    "- It demonstrates that carefully redesigned CNNs can match or outperform Transformers on image classification tasks with less computational cost.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- [A ConvNet for the 2020s [paper]](https://arxiv.org/abs/2201.03545)\n",
    "<!-- - Check detailed info in [**convnext.ipynb**](./models/cnn/convnext.ipynb) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Convolution in PyTorch](#toc0_)\n",
    "\n",
    "- Convolution operations (e.g. `nn.Conv1d`, `nn.Conv2d`) in PyTorch (and most deep learning frameworks) technically performs **correlation, not convolution!**\n",
    "- Although the operation is named e.g. `Conv2d`, the correlation operation is preferred in practice for a few reasons\n",
    "  1. **Simplicity**:\n",
    "      - Correlation is easier to implement and understand since it doesn't require flipping the kernel\n",
    "  1. **Equivalence in Learning**:\n",
    "      - In the context of CNNs, the kernel weights are learned during training\n",
    "      - Since the kernels are learned, whether you use convolution or cross-correlation doesn't matter\n",
    "      - The network can learn equivalent filters regardless of whether the kernel is flipped or not\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `torch.nn.Conv1d`: [docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "- `torch.nn.Conv2d`: [docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "- `torch.nn.Conv3d`: [docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html)\n",
    "- `torch.nn.functional.conv1d`: [docs.pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html)\n",
    "- `torch.nn.functional.conv2d`: [docs.pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html)\n",
    "- `torch.nn.functional.conv3d`: [docs.pytorch.org/docs/stable/generated/torch.nn.functional.conv3d.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.conv3d.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_1_'></a>[1D Correlation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 1D signal and a kernel\n",
    "signal_1d = torch.arange(1, 10).reshape(1, 1, -1)  # shape: [1, 1, 10] -> (batch_size, num_channels, signal_length)\n",
    "kernel_1d = torch.tensor([2, 1, 2]).reshape(1, 1, -1)  # shape: [1, 1,  3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies convolution with \"same\" padding, output size is the same as input size\n",
    "conv_1d_1 = F.conv1d(\n",
    "    signal_1d,\n",
    "    kernel_1d,\n",
    "    padding=\"same\",\n",
    ")\n",
    "\n",
    "# applies convolution with \"valid\" padding, no padding is added, so the output size is reduced\n",
    "conv_1d_2 = F.conv1d(\n",
    "    signal_1d,\n",
    "    kernel_1d,\n",
    "    padding=\"valid\",\n",
    ")\n",
    "\n",
    "# applies convolution with a padding of 2 and a stride of 2, which results in downsampling the output\n",
    "conv_1d_3 = F.conv1d(\n",
    "    signal_1d,\n",
    "    kernel_1d,\n",
    "    padding=2,\n",
    "    stride=2,\n",
    ")\n",
    "\n",
    "# log\n",
    "print(f\"conv_1d_1 : {conv_1d_1}\")\n",
    "print(f\"conv_1d_2 : {conv_1d_2}\")\n",
    "print(f\"conv_1d_3 : {conv_1d_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 4), layout=\"compressed\")\n",
    "\n",
    "axs[0].plot(signal_1d.squeeze(), marker=\"o\", label=\"Original Signal\")\n",
    "axs[0].plot(kernel_1d.squeeze(), marker=\"o\", color=\"purple\", label=\"Kernel\")\n",
    "axs[0].set_title(\"Original Signal\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(conv_1d_1.squeeze(), marker=\"o\", color=\"orange\")\n",
    "axs[1].set_title('Convolution with \"Same\" Padding')\n",
    "axs[2].plot(conv_1d_2.squeeze(), marker=\"o\", color=\"green\")\n",
    "axs[2].set_title('Convolution with \"Valid\" Padding')\n",
    "axs[3].plot(conv_1d_3.squeeze(), marker=\"o\", color=\"red\")\n",
    "axs[3].set_title(\"Convolution with Custom Padding and Stride\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_2_'></a>[2D Correlation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2D signal (image) and a kernel\n",
    "signal_2d = torch.arange(1, 26, dtype=torch.float32).reshape(1, 1, 5, 5)  # (batch_size, num_channels, signal_length)\n",
    "kernel_2d = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=torch.float32).reshape(1, 1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies convolution with \"same\" padding, output size is the same as input size\n",
    "conv_2d_1 = F.conv2d(signal_2d, kernel_2d, padding=\"same\")\n",
    "\n",
    "# applies convolution with \"valid\" padding, no padding is added, so the output size is reduced\n",
    "conv_2d_2 = F.conv2d(signal_2d, kernel_2d, padding=\"valid\")\n",
    "\n",
    "# applies convolution with a padding of 1 and a stride of 2, which results in downsampling the output\n",
    "conv_2d_3 = F.conv2d(signal_2d, kernel_2d, padding=1, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(20, 4), layout=\"compressed\")\n",
    "\n",
    "axs[0].imshow(signal_2d.squeeze(), cmap=\"gray\")\n",
    "axs[0].set(title=\"Original Signal (Image)\", xticks=range(signal_2d.shape[3]), yticks=range(signal_2d.shape[2]))\n",
    "axs[1].imshow(kernel_2d.squeeze(), cmap=\"gray\")\n",
    "axs[1].set(title=\"Kernel\", xticks=range(kernel_2d.shape[3]), yticks=range(kernel_2d.shape[2]))\n",
    "axs[2].imshow(conv_2d_1.squeeze(), cmap=\"gray\")\n",
    "axs[2].set(title='Convolution with \"Same\" Padding', xticks=range(conv_2d_1.shape[3]), yticks=range(conv_2d_1.shape[2]))\n",
    "axs[3].imshow(conv_2d_2.squeeze(), cmap=\"gray\")\n",
    "axs[3].set(title='Convolution with \"Valid\" Padding', xticks=range(conv_2d_2.shape[3]), yticks=range(conv_2d_2.shape[2]))\n",
    "axs[4].imshow(conv_2d_3.squeeze(), cmap=\"gray\")\n",
    "axs[4].set(\n",
    "    title=\"Convolution with Custom Padding and Stride\",\n",
    "    xticks=range(conv_2d_3.shape[3]),\n",
    "    yticks=range(conv_2d_3.shape[2]),\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[CNN Implementation](#toc0_)\n",
    "\n",
    "- CNNs are a class of deep learning models specifically designed for processing structured grid-like data, such as images, videos, and certain types of sequential data.\n",
    "\n",
    "**Key Components of CNNs**\n",
    "\n",
    "1. **Feature Extraction**\n",
    "   - **Convolutional Layers**\n",
    "     - Core building block of a CNN.\n",
    "     - Slide a filter (kernel) over the input data to produce a feature map.\n",
    "   - **Pooling Layers**\n",
    "     - Reduce spatial dimensions of feature maps.\n",
    "     - Help make the model invariant to small translations and reduce computation.\n",
    "     - Types:\n",
    "       - **Max Pooling:** Takes the maximum value from each patch.\n",
    "       - **Average Pooling:** Takes the average value from each patch.\n",
    "1. **Classification**\n",
    "   - Flatten the features extracted by convolution/pooling layers and pass through fully connected layers (MLP).\n",
    "   - Performs final classification or regression task based on extracted features.\n",
    "   - See [**Multi-Layer Perceptron (MLP)**](./05-multi-layer-perceptrons.ipynb).\n",
    "\n",
    "<div style=\"text-align: center; padding-top: 10px;\">\n",
    "    <img src=\"../assets/images/original/cnn/cnn-general.svg\" alt=\"cnn-general.svg\" style=\"min-width: 512px; max-width: 100%; height: auto; border-radius: 16px;\">\n",
    "    <p><em>Figure 4: Convolutional Neural Networks Model</em></p>\n",
    "</div>\n",
    "\n",
    "**Calculating the number of parameters**:\n",
    "\n",
    "<table style=\"margin: 0 auto; text-align:center;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th colspan=\"4\">Feature Extraction (Convolutional Layers)</th>\n",
    "      <th colspan=\"4\">Classification (Fully Connected Layers)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th colspan=\"2\">Convolution<sub>1</sub></th>\n",
    "      <th colspan=\"2\">Convolution<sub>L</sub></th>\n",
    "      <th colspan=\"2\">Hidden<sub>1</sub></th>\n",
    "      <th colspan=\"2\">OUtput (Logits)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>(c<sub>0</sub> √ó s<sub>1</sub> √ó s<sub>2</sub>) √ó c<sub>1</sub></td>\n",
    "      <td>c<sub>1</sub></td>\n",
    "      <td>(c<sub>L-1</sub> √ó s<sub>1</sub> √ó s<sub>2</sub>) √ó c<sub>L</sub></td>\n",
    "      <td>c<sub>L</sub></td>\n",
    "      <td>d<sub>0</sub> √ó h<sub>1</sub></td>\n",
    "      <td>h<sub>1</sub></td>\n",
    "      <td>h<sub>L-1</sub> √ó o</td>\n",
    "      <td>o</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "  <tfoot>\n",
    "    <tr>\n",
    "      <td colspan=\"2\">(c<sub>0</sub> √ó s<sub>1</sub> √ó s<sub>2</sub> + 1) √ó c<sub>1</sub></td>\n",
    "      <td colspan=\"2\">(c<sub>L-1</sub> √ó s<sub>1</sub> √ó s<sub>2</sub> + 1) √ó c<sub>L</sub></td>\n",
    "      <td colspan=\"2\">(d<sub>0</sub> + 1) √ó h<sub>1</sub></td>\n",
    "      <td colspan=\"2\">(h<sub>L-1</sub> + 1) √ó o</td>\n",
    "    </tr>\n",
    "  </tfoot>\n",
    "</table>\n",
    "\n",
    "**Training a CNN**:\n",
    "\n",
    "- **Forward Pass:** Compute outputs using current weights and biases.\n",
    "- **Loss Function:** E.g., Cross-Entropy Loss for classification, Mean Squared Error for regression.\n",
    "- **Backward Pass (Backpropagation):** Compute gradients of the loss w.r.t. weights and biases.\n",
    "- **Weight Update:** Update parameters using optimizers like Gradient Descent or Adam.\n",
    "- **Regularization:** Use Dropout, Batch Normalization, etc., to prevent overfitting and stabilize training.\n",
    "\n",
    "**Applications of CNNs**:\n",
    "\n",
    "- Image Classification\n",
    "- Object Detection\n",
    "- Segmentation\n",
    "- Face Recognition\n",
    "\n",
    "‚úçÔ∏è **Notes**:\n",
    "\n",
    "- **`torch.nn.Conv2d`**\n",
    "    - **Loss functions:**\n",
    "        - Multi-class classification: `torch.nn.CrossEntropyLoss` = `LogSoftmax` + `NLLLoss`\n",
    "        - [CrossEntropyLoss docs](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "        - [NLLLoss docs](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)\n",
    "    - **Activation for last layer:**\n",
    "        - When using `CrossEntropyLoss`, no activation is needed; it internally computes LogSoftmax + NLLLoss.\n",
    "        - [Softmax docs](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
    "        - [LogSoftmax docs](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html)\n",
    "    - **Weights:**\n",
    "        - Kaiming/He initialization\n",
    "        - Uniform: $W \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_\\text{in}}}, \\sqrt{\\frac{6}{n_\\text{in}}}\\right)$\n",
    "        - Normal: $W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_\\text{in}}\\right)$\n",
    "    - **Biases:** Initialized to zero\n",
    "    - [Initialization docs](https://pytorch.org/docs/stable/nn.init.html)\n",
    "    - Paper: [Delving deep into rectifiers: Surpassing human-level performance on ImageNet - He et al., 2015](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)\n",
    "\n",
    "üõù **Playgrounds**:\n",
    "\n",
    "- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n",
    "- [Image Similarity Search](https://convnetplayground.fastforwardlabs.com/)\n",
    "- [NN-SVG](https://alexlenail.me/NN-SVG/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_5_1_'></a>[Using PyTorch](#toc0_)\n",
    "\n",
    "<div style=\"text-align: center; padding-top: 10px;\">\n",
    "    <img src=\"../assets/images/original/cnn/cnn-example.svg\" alt=\"cnn-example.svg\" style=\"min-width: 512px; max-width: 100%; height: auto; border-radius: 16px;\">\n",
    "    <p><em>Figure 5: A Simple Example using Convolutional Neural Networks</em></p>\n",
    "</div>\n",
    "\n",
    "**Calculating the number of parameters**:\n",
    "\n",
    "<table style=\"margin: 0 auto; text-align:center;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th colspan=\"4\">Feature Extraction (Convolutional Layers)</th>\n",
    "      <th colspan=\"4\">Classification (Fully Connected Layers)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th colspan=\"2\">Convolution<sub>1</sub></th>\n",
    "      <th colspan=\"2\">Convolution<sub>L</sub></th>\n",
    "      <th colspan=\"2\">Hidden<sub>1</sub></th>\n",
    "      <th colspan=\"2\">OUtput (Logits)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>(3 √ó 3 √ó 3) √ó 8</td>\n",
    "      <td>8</td>\n",
    "      <td>(8 √ó 3 √ó 3) √ó 16</td>\n",
    "      <td>16</td>\n",
    "      <td>1024 √ó 32</td>\n",
    "      <td>32</td>\n",
    "      <td>32 √ó 10</td>\n",
    "      <td>10</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "  <tfoot>\n",
    "    <tr>\n",
    "      <td colspan=\"2\">(3 √ó 3 √ó 3 + 1) √ó 8</td>\n",
    "      <td colspan=\"2\">(8 √ó 3 √ó 3 + 1) √ó 16</td>\n",
    "      <td colspan=\"2\">(1024 + 1) √ó 32</td>\n",
    "      <td colspan=\"2\">(32 + 1) √ó 10</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-top: 2px solid; font-weight: bold;\">\n",
    "      <td colspan=\"8\">Total Parameters: 224 + 1168 + 32800 + 330 = <strong>34522</strong></td>\n",
    "    </tr>\n",
    "  </tfoot>\n",
    "</table>\n",
    "\n",
    "- Refer to [**cifar10-classification.ipynb**](./projects/cifar-classification/cifar-10/implementation-1/cifar10-classification.ipynb) for a comprehensive example on the CNN concept.\n",
    "\n",
    "üìö **Tutorials**:\n",
    "\n",
    "- Neural Networks: [pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial)\n",
    "- Training a Classifier: [pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        conv_channels: list[int],\n",
    "        kernel_size: int,\n",
    "        hidden_sizes: list[int],\n",
    "        n_output: int,\n",
    "        input_height: int,\n",
    "        input_width: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        c_in = in_channels\n",
    "        h, w = input_height, input_width\n",
    "\n",
    "        # convolutional feature extractor\n",
    "        for c_out in conv_channels:\n",
    "            layers.append(nn.Conv2d(c_in, c_out, kernel_size=kernel_size, padding=kernel_size // 2))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "            # update spatial size after pooling\n",
    "            h //= 2\n",
    "            w //= 2\n",
    "            c_in = c_out\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # compute flattened dimension\n",
    "        d_flat = c_in * h * w\n",
    "\n",
    "        # fully connected classifier\n",
    "        fc_layers = []\n",
    "        in_features = d_flat\n",
    "        for h_size in hidden_sizes:\n",
    "            fc_layers.append(nn.Linear(in_features, h_size))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "            in_features = h_size\n",
    "\n",
    "        fc_layers.append(nn.Linear(in_features, n_output))\n",
    "        self.classifier = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 100\n",
    "n_channels = 3          # e.g., RGB images\n",
    "height, width = 32, 32  # image size\n",
    "n_output = 10           # number of classes\n",
    "\n",
    "# random input data\n",
    "X = torch.randn(batch_size, n_channels, height, width)\n",
    "\n",
    "# random labels for classification\n",
    "y_true = torch.randint(0, n_output, (batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "pytorch_model = PytorchCNN(\n",
    "    in_channels=3,\n",
    "    conv_channels=[8, 16],\n",
    "    kernel_size=3,\n",
    "    hidden_sizes=[32],\n",
    "    n_output=10,\n",
    "    input_height=32,\n",
    "    input_width=32,\n",
    ")\n",
    "\n",
    "pytorch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(pytorch_model, input_size=(batch_size, n_channels, height, width), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "logits = pytorch_model(X)\n",
    "\n",
    "# log\n",
    "print(f\"Logits:\\n{logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()  # expects logits + integer labels\n",
    "\n",
    "# compute loss\n",
    "loss = criterion(logits, y_true)\n",
    "print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "loss.backward()  # computes gradients for all parameters\n",
    "\n",
    "# log\n",
    "print(f\"gradients for first layer weights:\\n{pytorch_model.features[0].weight.grad}\")"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
