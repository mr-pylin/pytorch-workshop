{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/pytorch-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"text-align: right; flex: 1;\">\n",
    "        <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../assets/images/pytorch/logo/pytorch-logo-dark.svg\" \n",
    "                 alt=\"PyTorch Logo\"\n",
    "                 style=\"max-height: 48px; width: auto; background-color: #ffffff; border-radius: 8px;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Multilayer Perceptron (MLP)](#toc2_)    \n",
    "  - [Forward and Backward Propagation](#toc2_1_)    \n",
    "    - [Forward Propagation](#toc2_1_1_)    \n",
    "      - [Input Layer $\\rightarrow$ First Hidden Layer](#toc2_1_1_1_)    \n",
    "      - [Hidden Layer $l$ $\\rightarrow$ Hidden Layer $l+1$](#toc2_1_1_2_)    \n",
    "      - [Last Hidden Layer $\\rightarrow$ Output Layer](#toc2_1_1_3_)    \n",
    "    - [Backward Propagation](#toc2_1_2_)    \n",
    "      - [Output Layer (Last Layer `L`)](#toc2_1_2_1_)    \n",
    "      - [Hidden Layers `l = L-1, ..., 1`](#toc2_1_2_2_)    \n",
    "  - [Limitations](#toc2_2_)    \n",
    "    - [MLPs vs. Other Architectures](#toc2_2_1_)    \n",
    "  - [Parameter Initialization](#toc2_3_)    \n",
    "    - [Weight](#toc2_3_1_)    \n",
    "    - [Bias](#toc2_3_2_)    \n",
    "  - [MLP Implementation](#toc2_4_)    \n",
    "    - [Manual](#toc2_4_1_)    \n",
    "    - [Using PyTorch](#toc2_4_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for deterministic results\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# log\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Multilayer Perceptron (MLP)](#toc0_)\n",
    "\n",
    "- A [**Multilayer Perceptron (MLP)**](https://en.wikipedia.org/wiki/Multilayer_perceptron) is a type of feedforward artificial neural network, also known as a **Fully-Connected Network** or **Dense Network**.\n",
    "- It consists of at least three layers of nodes: an **input layer**, one or more **hidden layers**, and an **output layer**.\n",
    "\n",
    "üß¨ **Key Characteristics**:\n",
    "\n",
    "- **Fully Connected**: Every node (neuron) in one layer is connected to every node in the next layer.\n",
    "- **[Non-Linear Activations](./utils/activation.ipynb)**: Each neuron applies a non-linear activation function, enabling the network to model complex patterns.\n",
    "- **[Feedforward](https://en.wikipedia.org/wiki/Feedforward_neural_network)**: Data flows in a single direction, from input to output, with no cycles or loops.\n",
    "\n",
    "üèõÔ∏è **Basic Architecture**:\n",
    "\n",
    "- **Input Layer**: Receives input features. The number of neurons equals the number of features in the dataset.\n",
    "- **Hidden Layers**: These layers contain neurons that compute weighted sums and apply activation functions.\n",
    "- **Output Layer**: Produces the final output, which could be a single value or a set of values for different tasks e.g. [**Regression**](https://en.wikipedia.org/wiki/Regression_analysis), and [**Classification**](https://en.wikipedia.org/wiki/Classification).\n",
    "\n",
    "<div style=\"text-align: center; padding-top: 10px;\">\n",
    "    <img src=\"../assets/images/original/mlp/mlp-general.svg\" alt=\"mlp-general.svg\" style=\"min-width: 512px; width: 80%; height: auto;; border-radius: 16px;\">\n",
    "    <p><em>Figure 1: Multi-Layer-Perceptron (aka fully connected layers)</em></p>\n",
    "</div>\n",
    "\n",
    "**Calculating the number of parameters**:\n",
    "\n",
    "<table style=\"margin: 0 auto; text-align:center;\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th colspan=\"2\">hidden<sub>1</sub> parameters</th>\n",
    "      <th colspan=\"2\">hidden<sub>2</sub> parameters</th>\n",
    "      <th colspan=\"2\">hidden<sub>L-1</sub> parameters</th>\n",
    "      <th colspan=\"2\">output parameters</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "      <td>Weights</td>\n",
    "      <td>Biases</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>n √ó h<sub>1</sub></td>\n",
    "      <td>h<sub>1</sub></td>\n",
    "      <td>h<sub>1</sub> √ó h<sub>2</sub></td>\n",
    "      <td>h<sub>2</sub></td>\n",
    "      <td>h<sub>L-2</sub> √ó h<sub>L-1</sub></td>\n",
    "      <td>h<sub>L-1</sub></td>\n",
    "      <td>h<sub>L-1</sub> √ó o</td>\n",
    "      <td>o</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "  <tfoot>\n",
    "    <tr>\n",
    "      <td colspan=\"2\">(n + 1) √ó h<sub>1</sub></td>\n",
    "      <td colspan=\"2\">(h<sub>1</sub> + 1) √ó h<sub>2</sub></td>\n",
    "      <td colspan=\"2\">(h<sub>L-2</sub> + 1) √ó h<sub>L-1</sub></td>\n",
    "      <td colspan=\"2\">(h<sub>L-1</sub> + 1) √ó o</td>\n",
    "    </tr>\n",
    "  </tfoot>\n",
    "</table>\n",
    "\n",
    "üõù **Playgrounds**:\n",
    "\n",
    "- [deeperplayground.org](https://deeperplayground.org/)\n",
    "- [alexlenail.me/NN-SVG](https://alexlenail.me/NN-SVG/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Forward and Backward Propagation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Forward Propagation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_1_'></a>[Input Layer $\\rightarrow$ First Hidden Layer](#toc0_)\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}^{[1]} = \\mathbf{X} \\mathbf{W}^{[1]} + \\mathbf{1}_m \\mathbf{b}^{[1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{[1]} = \\sigma(\\mathbf{Z}^{[1]})\n",
    "$$\n",
    "\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$\n",
    "- $\\mathbf{W}^{[1]} \\in \\mathbb{R}^{n \\times h_1}$\n",
    "- $\\mathbf{1}_m \\in \\mathbb{R}^{m \\times 1}$ is a column of ones to broadcast the bias.\n",
    "- $\\mathbf{1}_m \\mathbf{b}^{[1]}, \\mathbf{Z}^{[1]}, \\mathbf{A}^{[1]} \\in \\mathbb{R}^{m \\times h_1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_2_'></a>[Hidden Layer $l$ $\\rightarrow$ Hidden Layer $l+1$](#toc0_)\n",
    "\n",
    "For `l = 1, ..., L-1` (except last layer):\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}^{[l+1]} = \\mathbf{A}^{[l]} \\mathbf{W}^{[l+1]} + \\mathbf{1}_m \\mathbf{b}^{[l+1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^{[l+1]} = \\sigma(\\mathbf{Z}^{[l+1]})\n",
    "$$\n",
    "\n",
    "- $\\mathbf{W}^{[l+1]} \\in \\mathbb{R}^{h_l \\times h_{l+1}}$\n",
    "- $\\mathbf{1}_m \\mathbf{b}^{[l+1]}, \\mathbf{Z}^{[l+1]}, \\mathbf{A}^{[l+1]} \\in \\mathbb{R}^{m \\times h_{l+1}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_3_'></a>[Last Hidden Layer $\\rightarrow$ Output Layer](#toc0_)\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}^{[L]} = \\mathbf{A}^{[L-1]} \\mathbf{W}^{[L]} + \\mathbf{1}_m \\mathbf{b}^{[L]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}_{\\text{logits}} = \\mathbf{Z}^{[L]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}_{\\text{pred}} = \\text{softmax}(\\mathbf{Y}_{\\text{logits}})\n",
    "$$\n",
    "\n",
    "- $\\mathbf{W}^{[L]} \\in \\mathbb{R}^{h_{L-1} \\times o}$\n",
    "- $\\mathbf{Z}^{[l]}, \\mathbf{Y}_{\\text{logits}}, \\mathbf{Y}_{\\text{pred}} \\in \\mathbb{R}^{m \\times o}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[Backward Propagation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_2_1_'></a>[Output Layer (Last Layer `L`)](#toc0_)\n",
    "\n",
    "$$\n",
    "\\Delta^{[L]} = \\mathbf{A}^{[L]} - \\mathbf{Y} \\in \\mathbb{R}^{m \\times o}\n",
    "$$\n",
    "\n",
    "**Gradients:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{[L]}} = (\\mathbf{A}^{[L-1]})^\\top \\Delta^{[L]} \\in \\mathbb{R}^{h_{L-1} \\times o}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{[L]}} = \\sum_{j=1}^{m} \\Delta^{[L](j)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_2_2_'></a>[Hidden Layers `l = L-1, ..., 1`](#toc0_)\n",
    "\n",
    "$$\n",
    "\\Delta^{[l]} = \\Delta^{[l+1]} (\\mathbf{W}^{[l+1]})^\\top \\odot \\sigma'(\\mathbf{Z}^{[l]}) \\in \\mathbb{R}^{m \\times h_l}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{[l]}} = (\\mathbf{A}^{[l-1]})^\\top \\Delta^{[l]} \\in \\mathbb{R}^{h_{l-1} \\times h_l}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{[l]}} = \\sum_{j=1}^{m} \\Delta^{[l](j)}\n",
    "$$\n",
    "\n",
    "- $\\mathbf{A}^{[0]} = \\mathbf{X}$ (input matrix)  \n",
    "- $\\sigma'(\\mathbf{Z}^{[l]})$ = element-wise derivative of activation function  \n",
    "- $\\odot$ = element-wise (Hadamard) product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Limitations](#toc0_)\n",
    "\n",
    "- **Fixed Input and Output Sizes**:\n",
    "  - Standard MLPs require fixed-size input and output tensors.\n",
    "  - This makes them less directly applicable to variable-length data without preprocessing (e.g., padding, pooling, or embedding).\n",
    "\n",
    "- **No Built-in Structure Awareness**:\n",
    "  - MLPs treat inputs as flat vectors.\n",
    "  - They do not inherently exploit spatial structure (images), temporal structure (sequences), or relational structure (graphs).\n",
    "  - This often makes them less parameter-efficient compared to specialized architectures.\n",
    "\n",
    "- **Scalability Issues**:\n",
    "  - The number of parameters grows rapidly with input size:\n",
    "    $$\n",
    "    \\#\\text{params} \\propto d_{\\text{in}} \\times d_{\\text{out}}\n",
    "    $$\n",
    "  - This leads to:\n",
    "    - higher memory usage\n",
    "    - higher computational cost\n",
    "    - increased risk of overfitting\n",
    "\n",
    "- **Stateless Nature**:\n",
    "  - MLPs are stateless: the output depends only on the current input.\n",
    "  - They do not maintain internal memory across multiple inputs.\n",
    "  - This limits their ability to model sequential or dynamic processes directly.\n",
    "\n",
    "- **Limited Inductive Bias**:\n",
    "  - MLPs do not assume any structure in the data.\n",
    "  - This makes them highly general but often less efficient than architectures designed for specific data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[MLPs vs. Other Architectures](#toc0_)\n",
    "\n",
    "- **MLP vs. [CNN](./07-convolutional-neural-networks.ipynb)**  \n",
    "  - CNNs exploit spatial locality and weight sharing.\n",
    "  - This makes CNNs far more parameter-efficient and effective for image data.\n",
    "\n",
    "- **MLP vs. [RNN](./11-recurrent-neural-networks.ipynb)**  \n",
    "  - RNNs maintain hidden state across time steps.\n",
    "  - This enables modeling temporal dependencies in sequential data.\n",
    "\n",
    "- **MLP vs. Transformer**\n",
    "  - Transformers use attention mechanisms to model relationships between all input elements.\n",
    "  - They handle sequential and structured data more effectively than standard MLPs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Parameter Initialization](#toc0_)\n",
    "\n",
    "- Initialization occurs once when the layer is created.\n",
    "- Parameters are updated during training by the optimizer.\n",
    "- Initialization is defined in `reset_parameters()` of `nn.Linear`.\n",
    "- More Details about Initialization: [**hyperparameter.ipynb**](./utils/hyperparameter.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example dimensions\n",
    "fan_in = 3\n",
    "fan_out = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_1_'></a>[Weight](#toc0_)\n",
    "\n",
    "- By default, `nn.Linear` initializes weights using **Kaiming (He) uniform initialization**.\n",
    "- This initialization is well suited for layers followed by ReLU or similar activation functions (see [**activation.ipynb**](./utils/activation.ipynb) for more info).\n",
    "- The weights are sampled from a uniform distribution:\n",
    "\n",
    "  $$\n",
    "  W_{ij} \\sim \\mathcal{U}\\left(\n",
    "  -\\sqrt{\\frac{6}{n_{\\text{in}}}},\n",
    "  \\sqrt{\\frac{6}{n_{\\text{in}}}}\n",
    "  \\right)\n",
    "  $$\n",
    "\n",
    "- where:\n",
    "  - $n_{\\text{in}}$ is the number of input features (fan-in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty tensors (uninitialized memory)\n",
    "W = torch.empty((fan_in, fan_out))\n",
    "\n",
    "# default distribution for <nn.Linear> parameters\n",
    "nn.init.kaiming_uniform_(W, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "# log\n",
    "print(f\"W:\\n{W}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_3_2_'></a>[Bias](#toc0_)\n",
    "\n",
    "- Biases are initialized from a uniform distribution:\n",
    "\n",
    "  $$\n",
    "  b_i \\sim \\mathcal{U}\\left(\n",
    "  -\\frac{1}{\\sqrt{n_{\\text{in}}}},\n",
    "  \\frac{1}{\\sqrt{n_{\\text{in}}}}\n",
    "  \\right)\n",
    "  $$\n",
    "\n",
    "- Biases are **not initialized to zero** by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty tensors (uninitialized memory)\n",
    "b = torch.empty((fan_in))\n",
    "\n",
    "# default distribution for <nn.Linear> parameters\n",
    "nn.init.uniform_(b, -1 / fan_in**0.5, 1 / fan_in**0.5)\n",
    "\n",
    "# log\n",
    "print(f\"b:\\n{b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[MLP Implementation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLP(torch.nn.Module):\n",
    "    def __init__(self, n_input: int, hidden_sizes: list[int], n_output: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # layer sizes\n",
    "        self.layer_sizes = [n_input] + hidden_sizes + [n_output]\n",
    "        self.L = len(self.layer_sizes) - 1  # number of layers\n",
    "\n",
    "        # create weight and bias parameters manually\n",
    "        self.weights = torch.nn.ParameterList()\n",
    "        self.biases = torch.nn.ParameterList()\n",
    "        for l in range(self.L):\n",
    "            W = torch.nn.Parameter(torch.empty(self.layer_sizes[l], self.layer_sizes[l + 1]))\n",
    "            b = torch.nn.Parameter(torch.zeros(1, self.layer_sizes[l + 1]))\n",
    "            # Kaiming initialization for weights\n",
    "            torch.nn.init.kaiming_uniform_(W, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_all: bool = False):\n",
    "        \"\"\"Forward pass storing pre-activations and activations\"\"\"\n",
    "        a = x\n",
    "        activations = [a]  # a^{[0]} = X\n",
    "        pre_acts = []\n",
    "\n",
    "        for l in range(self.L - 1):\n",
    "            z = torch.matmul(a, self.weights[l]) + self.biases[l]\n",
    "            pre_acts.append(z)\n",
    "            a = torch.relu(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # output layer\n",
    "        z = torch.matmul(a, self.weights[-1]) + self.biases[-1]\n",
    "        pre_acts.append(z)\n",
    "        y_pred = torch.softmax(z, dim=1)\n",
    "        activations.append(y_pred)\n",
    "\n",
    "        if return_all:\n",
    "            return y_pred, pre_acts, activations\n",
    "        return y_pred\n",
    "\n",
    "    def backward(self, x: torch.Tensor, y_true: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Manual backward propagation for cross-entropy loss + softmax\n",
    "        x: input batch (m x n)\n",
    "        y_true: one-hot labels (m x o)\n",
    "        \"\"\"\n",
    "        m = x.shape[0]\n",
    "\n",
    "        # forward pass and store intermediate values\n",
    "        y_pred, pre_acts, activations = self.forward(x, return_all=True)\n",
    "\n",
    "        # initialize gradient containers\n",
    "        dW = [torch.zeros_like(W) for W in self.weights]\n",
    "        db = [torch.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # output layer error (softmax + cross-entropy)\n",
    "        delta = (y_pred - y_true) / m  # shape: (m x o)\n",
    "        dW[-1] = torch.matmul(activations[-2].T, delta)\n",
    "        db[-1] = delta.sum(dim=0, keepdim=True)\n",
    "\n",
    "        # backprop through hidden layers\n",
    "        for l in reversed(range(self.L - 1)):\n",
    "            # derivative of ReLU\n",
    "            dz = delta.matmul(self.weights[l + 1].T) * (pre_acts[l] > 0).float()\n",
    "            delta = dz\n",
    "            dW[l] = torch.matmul(activations[l].T, delta)\n",
    "            db[l] = delta.sum(dim=0, keepdim=True)\n",
    "\n",
    "        return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_input = 4            # number of input features\n",
    "hidden_sizes = [5, 3]  # two hidden layers: 5 and 3 nodes\n",
    "n_output = 2           # number of classes\n",
    "batch_size = 6\n",
    "\n",
    "# create random input data\n",
    "X = torch.randn(batch_size, n_input)\n",
    "\n",
    "# create random one-hot labels\n",
    "y_indices = torch.randint(0, n_output, (batch_size,))\n",
    "y_onehot = torch.zeros(batch_size, n_output)\n",
    "y_onehot[torch.arange(batch_size), y_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the MLP\n",
    "custom_model = CustomMLP(n_input=n_input, hidden_sizes=hidden_sizes, n_output=n_output)\n",
    "custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "summary(custom_model, input_size=(batch_size, n_input), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "y_pred = custom_model.forward(X, return_all=False)\n",
    "\n",
    "# log\n",
    "print(f\"predictions:\\n{y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "dW, db = custom_model.backward(X, y_onehot)\n",
    "\n",
    "# log\n",
    "for l in range(len(dW)):\n",
    "    print(f\"layer {l+1} gradients:\")\n",
    "    print(f\"dW:\\n{dW[l]}\")\n",
    "    print(f\"db:\\n{db[l]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_4_2_'></a>[Using PyTorch](#toc0_)\n",
    "\n",
    "- Refer to this [**mnist-classification.ipynb**](./projects/mnist-classification/implementation-1/mnist-classification.ipynb) for a comprehensive example on the MLP concept.\n",
    "\n",
    "üìö **Tutorials**:\n",
    "\n",
    "- Neural Networks: [pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial)\n",
    "- Training a Classifier: [pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchMLP(nn.Module):\n",
    "    def __init__(self, n_input: int, hidden_sizes: list[int], n_output: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # build layers\n",
    "        layers = []\n",
    "        in_features = n_input\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = h\n",
    "            \n",
    "        # output layer\n",
    "        layers.append(nn.Linear(in_features, n_output))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_input = 4\n",
    "hidden_sizes = [3]\n",
    "n_output = 2\n",
    "batch_size = 6\n",
    "\n",
    "# random input data\n",
    "X = torch.randn(batch_size, n_input)\n",
    "\n",
    "# random label indices for classification\n",
    "y_true = torch.randint(0, n_output, (batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "pytorch_model = PytorchMLP(n_input, hidden_sizes, n_output)\n",
    "pytorch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(pytorch_model, input_size=(batch_size, n_input), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "logits = pytorch_model(X)\n",
    "\n",
    "# log\n",
    "print(\"Logits:\\n\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss()  # expects logits + integer labels\n",
    "\n",
    "# compute loss\n",
    "loss = criterion(logits, y_true)\n",
    "print(f\"loss: {loss.item()}\")\n",
    "\n",
    "# backward pass\n",
    "loss.backward()  # computes gradients for all parameters\n",
    "\n",
    "# log\n",
    "print(f\"gradients for first layer weights:\\n{pytorch_model.model[0].weight.grad}\")"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
