{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/pytorch-workshop](https://github.com/mr-pylin/pytorch-workshop)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [VGGNet](#toc2_)    \n",
    "  - [Custom VGGNet](#toc2_1_)    \n",
    "    - [VGG11](#toc2_1_1_)    \n",
    "      - [Initialize the Model](#toc2_1_1_1_)    \n",
    "      - [Model Summary](#toc2_1_1_2_)    \n",
    "    - [VGG13](#toc2_1_2_)    \n",
    "      - [Initialize the Model](#toc2_1_2_1_)    \n",
    "      - [Model Summary](#toc2_1_2_2_)    \n",
    "    - [VGG16](#toc2_1_3_)    \n",
    "      - [Initialize the Model](#toc2_1_3_1_)    \n",
    "      - [Model Summary](#toc2_1_3_2_)    \n",
    "    - [VGG19](#toc2_1_4_)    \n",
    "      - [Initialize the Model](#toc2_1_4_1_)    \n",
    "      - [Model Summary](#toc2_1_4_2_)    \n",
    "  - [PyTorch VGGNet](#toc2_2_)    \n",
    "    - [VGG11](#toc2_2_1_)    \n",
    "      - [Initialize the Model](#toc2_2_1_1_)    \n",
    "      - [Model Summary](#toc2_2_1_2_)    \n",
    "    - [VGG13](#toc2_2_2_)    \n",
    "      - [Initialize the Model](#toc2_2_2_1_)    \n",
    "      - [Model Summary](#toc2_2_2_2_)    \n",
    "    - [VGG16](#toc2_2_3_)    \n",
    "      - [Initialize the Model](#toc2_2_3_1_)    \n",
    "      - [Model Summary](#toc2_2_3_2_)    \n",
    "    - [VGG19](#toc2_2_4_)    \n",
    "      - [Initialize the Model](#toc2_2_4_1_)    \n",
    "      - [Model Summary](#toc2_2_4_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n",
    "\n",
    "- torchvision models:\n",
    "  - class\n",
    "    - brings in the model class directly\n",
    "    - Allows more control and customization since you are dealing directly with the class. You can override methods, customize initialization, etc.\n",
    "  - function\n",
    "    - This import brings in a function that returns an instance of the model\n",
    "    - Easier and quicker to use, especially for standard models\n",
    "- [docs.pytorch.org/vision/stable/models.html](https://docs.pytorch.org/vision/stable/models.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torchvision.models import VGG, vgg11, vgg13, vgg16, vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[VGGNet](#toc0_)\n",
    "\n",
    "- Developed in 2014 by [Karen Simonyan](https://dblp.uni-trier.de/search/author?author=Karen%20Simonyan) and [Andrew Zisserman](https://dblp.uni-trier.de/pid/z/AndrewZisserman.html?q=Andrew%20Zisserman) from the Visual Geometry Group ([VGG](https://www.robots.ox.ac.uk/~vgg/index.html)) at the University of [Oxford](https://www.ox.ac.uk/).\n",
    "- It is based on the [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) paper\n",
    "- It was trained on the [ImageNet](https://www.image-net.org/) dataset (first resized to 256x256 then center cropped to 224x224) [[ImageNet viewer](https://navigu.net/#imagenet)]\n",
    "- Known for its simple and uniform architecture, using small `3x3` convolutional filters consistently throughout the network\n",
    "- It comes in several variants, primarily `VGG11`, `VGG13`, `VGG16` and `VGG19`, indicating the total number of layers\n",
    "- The `runner-up` of the ImageNet Large Scale Visual Recognition Challenge ([ILSVRC](https://image-net.org/challenges/LSVRC/2014/)) in 2014\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"../../../assets/images/original/cnn/architectures/vgg16.svg\" alt=\"vgg16-architecture.svg\" style=\"width: 100%;\">\n",
    "  <figcaption>VGG16 Architecture</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Custom VGGNet](#toc0_)\n",
    "\n",
    "- `Softmax` is missing due to internal implementation of `LogSoftmax` in the `CrossEntropyLoss` function.\n",
    "- there is an extension of VGGNet which also contains `nn.BatchNorm2d` before `nn.ReLU` in the `feature_extractor` section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVGGNet(nn.Module):\n",
    "    def __init__(self, feature_layers: list, num_classes: int = 1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(*self._make_layers(feature_layers))\n",
    "\n",
    "        # 512x7x7 -> 512x7x7\n",
    "        # trainable params: 0\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
    "\n",
    "        # flatten : 512x7x7 -> 25088\n",
    "        # 25088 -> 1000\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 25088 -> 4096\n",
    "            # trainable params: (25088 + 1) * 4096 = 102,764,544\n",
    "            nn.Linear(25088, 4096),\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: 0\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: 0\n",
    "            nn.Dropout(),\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: (4096 + 1) * 4096 = 16,781,312\n",
    "            nn.Linear(4096, 4096),\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: 0\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 4096 -> 4096\n",
    "            # trainable params: 0\n",
    "            nn.Dropout(),\n",
    "            # 4096 -> 1000\n",
    "            # trainable params: (4096 + 1) * 1000 = 4,097,000\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # feature extractor\n",
    "        x = self.features(x)\n",
    "\n",
    "        # adaptive average pooling\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        # flatten : 512x7x7 -> 25088\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # classifier\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layers(self, cfg: list) -> list:\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "\n",
    "        for x in cfg:\n",
    "            if x == \"M\":\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[VGG11](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_1_'></a>[Initialize the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_1 = CustomVGGNet(feature_layers=[64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_2_'></a>[Model Summary](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg11_1, (1, 3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[VGG13](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_2_1_'></a>[Initialize the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg13_1 = CustomVGGNet(feature_layers=[64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg13_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_2_2_'></a>[Model Summary](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg13_1, (1, 3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[VGG16](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_3_1_'></a>[Initialize the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_1 = CustomVGGNet(\n",
    "    feature_layers=[64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_3_2_'></a>[Model Summary](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg16_1, (1, 3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[VGG19](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_4_1_'></a>[Initialize the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_1 = CustomVGGNet(\n",
    "    feature_layers=[\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_4_2_'></a>[Model Summary](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg19_1, (1, 3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[PyTorch VGGNet](#toc0_)\n",
    "\n",
    "- All VGGNet variants available in PyTorch: [docs.pytorch.org/vision/stable/models/vgg.html](https://docs.pytorch.org/vision/stable/models/vgg.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[VGG11](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_1_1_'></a>[Initialize the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_2 = vgg11()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_1_2_'></a>[Model Summary](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg11_2, (1, 3, 227, 227), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[VGG13](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_2_1_'></a>[Initialize the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg13_2 = vgg13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg13_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_2_2_'></a>[Model Summary](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg13_2, (1, 3, 227, 227), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[VGG16](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_3_1_'></a>[Initialize the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_2 = vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_3_2_'></a>[Model Summary](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg16_2, (1, 3, 227, 227), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_4_'></a>[VGG19](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_4_1_'></a>[Initialize the Model](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_2 = vgg19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_4_2_'></a>[Model Summary](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg19_2, (1, 3, 227, 227), device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}