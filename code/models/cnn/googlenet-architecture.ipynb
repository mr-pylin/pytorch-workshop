{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/pytorch-workshop](https://github.com/mr-pylin/pytorch-workshop)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [GoogLe Net](#toc2_)    \n",
    "  - [Custom GoogLeNet](#toc2_1_)    \n",
    "    - [Initialize the Model](#toc2_1_1_)    \n",
    "    - [Model Summary](#toc2_1_2_)    \n",
    "  - [PyTorch GoogLeNet](#toc2_2_)    \n",
    "    - [Initialize the Model](#toc2_2_1_)    \n",
    "    - [Model Summary](#toc2_2_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n",
    "\n",
    "- torchvision models:\n",
    "  - class\n",
    "    - brings in the model class directly\n",
    "    - Allows more control and customization since you are dealing directly with the class. You can override methods, customize initialization, etc.\n",
    "  - function\n",
    "    - This import brings in a function that returns an instance of the model\n",
    "    - Easier and quicker to use, especially for standard models\n",
    "- [docs.pytorch.org/vision/stable/models.html](https://docs.pytorch.org/vision/stable/models.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torchvision.models import GoogLeNet, googlenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[GoogLe Net](#toc0_)\n",
    "\n",
    "- GoogLeNet, officially known as `Inception v1`, Developed in 2014 by [Christian Szegedy](https://scholar.google.com/citations?user=bnQMuzgAAAAJ) and collaborators from [Google Research](https://research.google/)\n",
    "- It is based on the [Going Deeper with Convolutions](https://research.google/pubs/going-deeper-with-convolutions/) paper\n",
    "- It was trained on the [ImageNet](https://www.image-net.org/) dataset (first resized to 256x256 then center cropped to 224x224) [[ImageNet viewer](https://navigu.net/#imagenet)]\n",
    "- Known for its innovative Inception modules (concatenating filters of different sizes within the same module)\n",
    "- The architecture includes multiple [auxiliary classifiers](https://serp.ai/auxiliary-classifier/) to improve gradient flow and provide additional regularization\n",
    "- The losses of the auxiliary classifiers were weighted by 0.3\n",
    "- The `winner` of the ImageNet Large Scale Visual Recognition Challenge ([ILSVRC](https://image-net.org/challenges/LSVRC/2014/)) in 2014\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"../../../assets/images/original/cnn/architectures/googlenet.svg\" alt=\"googlenet-architecture.svg\" style=\"width: 100%;\">\n",
    "  <figcaption>GoogLeNet (Inception v1) Architecture</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Custom GoogLeNet](#toc0_)\n",
    "\n",
    "- `Softmax` is missing due to internal implementation of `LogSoftmax` in the `CrossEntropyLoss` function.\n",
    "- For better compatibility with various input sizes, `AveragePool2d` replaced with `AdaptiveAveragePool2d` to get the same output size.\n",
    "- Normalization:\n",
    "  - In the original GoogLeNet paper, `Local Response Normalization` (LRN) was used [`nn.LocalResponseNorm`].\n",
    "  - In many modern implementations including the PyTorch version, Batch Normalization (BN) is used instead [`nn.BatchNorm2d`].\n",
    "  - BatchNorm generally leads to better performance and is more effective at stabilizing training.\n",
    "- Approximate number of parameters\n",
    "  - without auxiliary classifiers: ~7 million\n",
    "  - with auxiliary classifieres: ~13 million\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,  # the number of input channels to the Inception module\n",
    "        n1x1,  # the number of 1x1 convolution filters in the first branch (branch 1)\n",
    "        n3x3red,  # the number of 1x1 convolution filters in the second branch (branch 2) before the 3x3 convolution\n",
    "        n3x3,  # the number of 3x3 convolution filters in the second branch (branch 2)\n",
    "        n5x5red,  # the number of 1x1 convolution filters in the third branch (branch 3) before the 5x5 convolution\n",
    "        n5x5,  # the number of 5x5 convolution filters in the third branch (branch 3)\n",
    "        pool_proj,  # the number of 1x1 convolution filters in the fourth branch (branch 4) after the max pooling\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # branch 1: 1x1 convolution\n",
    "        self.branch1 = BasicConv2d(in_channels, n1x1, kernel_size=1)\n",
    "\n",
    "        # branch 2: 1x1 convolution followed by 3x3 convolution\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, n3x3red, kernel_size=1), BasicConv2d(n3x3red, n3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # branch 3: 1x1 convolution followed by 5x5 convolution\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channels, n5x5red, kernel_size=1), BasicConv2d(n5x5red, n5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        # branch 4: 3x3 max pooling followed by 1x1 convolution\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1), BasicConv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # depth1: <in_channels> -> <n1x1>\n",
    "        branch1 = self.branch1(x)\n",
    "\n",
    "        # depth2: <in_channels> -> <n3x3>\n",
    "        branch2 = self.branch2(x)\n",
    "\n",
    "        # depth3: <in_channels> -> <n5x5>\n",
    "        branch3 = self.branch3(x)\n",
    "\n",
    "        # depth4: <in_channels> -> <pool_proj>\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        # depth concatenate: <in_channels> -> [depth1 + depth2 + depth3 + depth4]\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int = 1000):\n",
    "        super().__init__()\n",
    "        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.adaptive_avg_pool2d(x, output_size=(4, 4))\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000, use_aux: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_aux = use_aux\n",
    "\n",
    "        # 3x224x224 -> 64x112x112\n",
    "        self.conv1 = BasicConv2d(3, 64, kernel_size=7, padding=3, stride=2)\n",
    "\n",
    "        # 64x112x112 -> 64x56x56\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        # 64x56x56 -> 64x56x56\n",
    "        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n",
    "\n",
    "        # 64x56x56 -> 192x56x56\n",
    "        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n",
    "\n",
    "        # 192x56x56 -> 192x28x28\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        # 192x28x28 -> 256x28x28\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "\n",
    "        # 256x28x28 -> 480x28x28\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        # 480x28x28 -> 480x14x14\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        # 480x14x14 -> 512x14x14\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "\n",
    "        if self.use_aux:\n",
    "            # 512x14x14 -> 1000\n",
    "            self.aux1 = InceptionAux(in_channels=512, num_classes=num_classes)\n",
    "\n",
    "        # 512x14x14 -> 512x14x14\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "\n",
    "        # 512x14x14 -> 512x14x14\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "\n",
    "        # 512x14x14 -> 528x14x14\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "\n",
    "        if self.use_aux:\n",
    "            # 528x14x14 -> 1000\n",
    "            self.aux2 = InceptionAux(in_channels=528, num_classes=num_classes)\n",
    "\n",
    "        # 528x14x14 -> 832x14x14\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        # 832x14x14 -> 832x7x7\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        # 832x7x7 -> 832x7x7\n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        # 832x7x7 -> 1024x7x7\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        # 1024x7x7 -> 1024x1x1\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "\n",
    "        # flatten: 1024x1x1 -> 1024\n",
    "        # 1024 -> 1024\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "        # 1024 -> 1000\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        aux1 = aux2 = None\n",
    "\n",
    "        # feature extractor\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.inception4a(x)\n",
    "\n",
    "        if self.training and self.use_aux:\n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "\n",
    "        if self.training and self.use_aux:\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        # flatten: 1024x1x1 -> 1024\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # classifier\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, aux1, aux2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Initialize the Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = CustomGoogLeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[Model Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_1, (1, 3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[PyTorch GoogLeNet](#toc0_)\n",
    "\n",
    "- GoogLeNet is available in PyTorch: [docs.pytorch.org/vision/stable/models/googlenet.html](https://docs.pytorch.org/vision/stable/models/googlenet.html)\n",
    "- There's a bug in the `3rd branch` of the `Inception module` where the `kernel size` should be `5x5` but is `3x3` [[details](https://github.com/pytorch/vision/issues/906)]\n",
    "  - `torch v2.4.0+cu124`\n",
    "  - `torchvision v0.19.0+cu124`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[Initialize the Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = googlenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[Model Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_2, (1, 3, 224, 224), device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}