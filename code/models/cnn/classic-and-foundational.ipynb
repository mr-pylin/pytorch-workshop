{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/pytorch-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"text-align: right; flex: 1;\">\n",
    "        <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../../assets/images/pytorch/logo/pytorch-logo-dark.svg\" \n",
    "                 alt=\"PyTorch Logo\"\n",
    "                 style=\"max-height: 48px; width: auto; background-color: #ffffff; border-radius: 8px;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependensies](#toc1_)    \n",
    "- [Classic / Foundational Architectures](#toc2_)    \n",
    "  - [LeNet-5](#toc2_1_)    \n",
    "    - [Manual Implementation](#toc2_1_1_)    \n",
    "  - [AlexNet](#toc2_2_)    \n",
    "    - [Manual Implementation](#toc2_2_1_)    \n",
    "    - [Using Pytorch](#toc2_2_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependensies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torchvision.models import alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Classic and Foundational Architectures](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[LeNet-5](#toc0_)\n",
    "\n",
    "- One of the pioneering **Convolutional Neural Network (CNN)** architectures developed in 1998 by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) and his colleagues at [AT&T Bell Labs](https://en.wikipedia.org/wiki/Bell_Labs)\n",
    "- It was introduced in the landmark paper *[Gradient-based learning applied to document recognition](https://ieeexplore.ieee.org/document/726791)*, which demonstrated end-to-end learning directly from raw pixel data\n",
    "- It was designed primarily for **handwritten digit recognition**, such as ZIP code and bank check processing\n",
    "- It was trained on the *[MNIST dataset](http://yann.lecun.com/exdb/mnist/)*, where the original **28√ó28 images were padded to 32√ó32** to match the network architecture [[MNIST viewer](https://observablehq.com/@davidalber/mnist-browser)]\n",
    "- LeNet-5 demonstrated that CNNs can automatically learn **spatial hierarchies of features**, eliminating the need for manual feature engineering and establishing the foundation for modern computer vision systems\n",
    "\n",
    "<div style=\"text-align: center; padding-top: 10px;\">\n",
    "    <img src=\"../../../assets/images/original/cnn/architectures/lenet5.svg\" alt=\"lenet5.svg\" style=\"min-width: 512px; width: 100%; height: auto; border-radius: 16px;\">\n",
    "    <p><em>Figure 1: LeNet-5 Architecture</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Manual Implementation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learnable subsampling layer (original LeNet-5 S2, S4)\n",
    "class Subsampling(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.scale = nn.Parameter(torch.ones(channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.scale.view(1, -1, 1, 1) * x + self.bias.view(1, -1, 1, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian RBF output layer (original LeNet-5)\n",
    "class RBFLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.sigma = nn.Parameter(torch.ones(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        centers = self.centers.unsqueeze(0)  # (1, classes, features)\n",
    "        dist = torch.sum((x - centers) ** 2, dim=2)\n",
    "        return torch.exp(-dist / (2 * self.sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # C1\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "\n",
    "        # S2\n",
    "        self.sub2 = Subsampling(6)\n",
    "\n",
    "        # C3 (full connection approximation)\n",
    "        self.conv3 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "\n",
    "        # S4\n",
    "        self.sub4 = Subsampling(16)\n",
    "\n",
    "        # C5\n",
    "        self.conv5 = nn.Conv2d(16, 120, kernel_size=5)\n",
    "\n",
    "        # F6\n",
    "        self.fc6 = nn.Linear(120, 84)\n",
    "\n",
    "        # output (RBF)\n",
    "        self.rbf = RBFLayer(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # C1\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "\n",
    "        # S2\n",
    "        x = torch.tanh(self.sub2(x))\n",
    "\n",
    "        # C3\n",
    "        x = torch.tanh(self.conv3(x))\n",
    "\n",
    "        # S4\n",
    "        x = torch.tanh(self.sub4(x))\n",
    "\n",
    "        # C5\n",
    "        x = torch.tanh(self.conv5(x))\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # F6\n",
    "        x = torch.tanh(self.fc6(x))\n",
    "\n",
    "        # output\n",
    "        x = self.rbf(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "lenet5_manual = LeNet5()\n",
    "lenet5_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary report\n",
    "summary(lenet5_manual, input_size=(1, 1, 32, 32), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[AlexNet](#toc0_)\n",
    "\n",
    "- A groundbreaking **deep Convolutional Neural Network (CNN)** architecture developed in 2012 by [Alex Krizhevsky](https://en.wikipedia.org/wiki/Alex_Krizhevsky), [Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever), and [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) at the [University of Toronto](https://en.wikipedia.org/wiki/University_of_Toronto)\n",
    "- It was introduced in the landmark paper *[ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)*, which revolutionized computer vision\n",
    "- It achieved a breakthrough performance in the *[ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://image-net.org/challenges/LSVRC/2012/)*, reducing the top-5 error rate from **26.2% to 15.3%**, far outperforming traditional methods\n",
    "- It was trained on the *[ImageNet dataset](https://image-net.org/)*, which contains over **1.2 million high-resolution images across 1000 classes** [[ImageNet viewer](https://navigu.net/#imagenet)]\n",
    "- AlexNet demonstrated that **deep CNNs trained on GPUs** can learn highly discriminative and hierarchical image representations, making deep learning the dominant approach in computer vision\n",
    "\n",
    "**It introduced several key innovations that are still widely used today**\n",
    "  - **ReLU activation function**, enabling faster training compared to sigmoid or tanh\n",
    "  - **Dropout regularization**, reducing overfitting in fully connected layers\n",
    "  - **GPU acceleration**, making large-scale deep network training practical\n",
    "  - **Data augmentation**, improving generalization performance\n",
    "  - **Overlapping max pooling**, improving feature robustness\n",
    "\n",
    "> AlexNet marked the beginning of the **modern deep learning era in computer vision**, influencing nearly all subsequent CNN architectures such as VGG, GoogLeNet, and ResNet\n",
    "\n",
    "<!-- <div style=\"text-align: center; padding-top: 10px;\">\n",
    "    <img src=\"../../../assets/images/original/cnn/architectures/alexnet.svg\" alt=\"alexnet.svg\" style=\"min-width: 512px; width: 100%; height: auto; border-radius: 16px;\">\n",
    "    <p><em>Figure 1: AlexNet Architecture</em></p>\n",
    "</div> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[Manual Implementation](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetOriginal(nn.Module):\n",
    "    \"\"\"\n",
    "    Original AlexNet implementation from the 2012 paper:\n",
    "    \"ImageNet Classification with Deep Convolutional Neural Networks\"\n",
    "\n",
    "    Key characteristics:\n",
    "    - Input size: 227 x 227 x 3\n",
    "    - Uses Local Response Normalization (LRN)\n",
    "    - Uses grouped convolutions (GPU split in original paper)\n",
    "    - Uses Dropout in classifier\n",
    "    - Uses ReLU activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 1000) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv1\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Conv2 (grouped)\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5, alpha=1e-4, beta=0.75, k=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # Conv3\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Conv4 (grouped)\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Conv5 (grouped)\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "\n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=256 * 6 * 6, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "alexnet_manual = AlexNetOriginal()\n",
    "alexnet_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary report\n",
    "summary(alexnet_manual, input_size=(1, 3, 227, 227), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[Using Pytorch](#toc0_)\n",
    "\n",
    "- AlexNet is available in PyTorch: [docs.pytorch.org/vision/stable/models/alexnet.html](https://docs.pytorch.org/vision/stable/models/alexnet.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "alexnet_pytorch = alexnet(weights=None)\n",
    "alexnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary report\n",
    "summary(alexnet_pytorch, input_size=(1, 3, 224, 224), device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
