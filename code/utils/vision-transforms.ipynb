{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/pytorch-workshop](https://github.com/mr-pylin/pytorch-workshop)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Load Dataset](#toc2_)    \n",
    "- [Transforms](#toc3_)    \n",
    "  - [Common Transformations](#toc3_1_)    \n",
    "    - [v2.ToImage](#toc3_1_1_)    \n",
    "    - [v2.ToDtype](#toc3_1_2_)    \n",
    "    - [v2.Normalize](#toc3_1_3_)    \n",
    "    - [plot](#toc3_1_4_)    \n",
    "  - [Data Augmentation Techniques](#toc3_2_)    \n",
    "    - [v2.RandomCrop](#toc3_2_1_)    \n",
    "    - [v2.Resize](#toc3_2_2_)    \n",
    "    - [v2.RandomVerticalFlip](#toc3_2_3_)    \n",
    "    - [v2.RandomHorizontalFlip](#toc3_2_4_)    \n",
    "    - [v2.RandomRotation](#toc3_2_5_)    \n",
    "    - [v2.ColorJitter](#toc3_2_6_)    \n",
    "    - [v2.RandomAffine](#toc3_2_7_)    \n",
    "  - [Mix transforms](#toc3_3_)    \n",
    "- [Effect of transforms](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for deterministic results\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update paths as needed based on your project structure\n",
    "DATASET_DIR = r\"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Load Dataset](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CIFAR10(DATASET_DIR, train=True, transform=None, download=False)\n",
    "\n",
    "x = trainset.data[:3]\n",
    "y = trainset.targets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4), layout=\"compressed\")\n",
    "for i, (img, label) in enumerate(zip(x, y)):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].set(title=label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Transforms](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/transforms.html](https://pytorch.org/vision/stable/transforms.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Common Transformations](#toc0_)\n",
    "\n",
    "- v2.ToImage\n",
    "- v2.ToDtype\n",
    "- v2.Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x[{i}].shape : {x[i].shape}\")\n",
    "    print(f\"x[{i}].dtype : {x[i].dtype}\")\n",
    "    print(f\"type(x[{i}]) : {type(x[i])}\")\n",
    "    print(f\"x[{i}].min() : {x[i].min()}\")\n",
    "    print(f\"x[{i}].max() : {x[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[v2.ToImage](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToImage.html](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToImage.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_image_transform = v2.ToImage()\n",
    "x_2 = [to_image_transform(img) for img in x]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_2[{i}].shape : {x_2[i].shape}\")\n",
    "    print(f\"x_2[{i}].dtype : {x_2[i].dtype}\")\n",
    "    print(f\"type(x_2[{i}]) : {type(x_2[i])}\")\n",
    "    print(f\"x_2[{i}].min() : {x_2[i].min()}\")\n",
    "    print(f\"x_2[{i}].max() : {x_2[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[v2.ToDtype](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html#torchvision.transforms.v2.ToDtype](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html#torchvision.transforms.v2.ToDtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dtype_transform = v2.ToDtype(dtype=torch.float32, scale=True)\n",
    "x_3 = [to_dtype_transform(img) for img in x_2]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_3[{i}].shape: {x_3[i].shape}\")\n",
    "    print(f\"x_3[{i}].dtype: {x_3[i].dtype}\")\n",
    "    print(f\"type(x_3[{i}]): {type(x_3[i])}\")\n",
    "    print(f\"x_3[{i}].min(): {x_3[i].min()}\")\n",
    "    print(f\"x_3[{i}].max(): {x_3[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[v2.Normalize](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.v2.Normalize.html#torchvision.transforms.v2.Normalize](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Normalize.html#torchvision.transforms.v2.Normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = np.array(x_3).mean(axis=(0, 2, 3))\n",
    "stds = np.array(x_3).std(axis=(0, 2, 3))\n",
    "\n",
    "normalize_transform = v2.Normalize(mean=mus, std=stds)\n",
    "x_4 = [normalize_transform(img) for img in x_3]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_4[{i}].shape: {x_4[i].shape}\")\n",
    "    print(f\"x_4[{i}].dtype: {x_4[i].dtype}\")\n",
    "    print(f\"type(x_4[{i}]): {type(x_4[i])}\")\n",
    "    print(f\"x_4[{i}].min(): {x_4[i].min()}\")\n",
    "    print(f\"x_4[{i}].max(): {x_4[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_4_'></a>[plot](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x, x_4, y)):\n",
    "    axs[0, i].imshow(img1)\n",
    "    axs[0, i].set(title=\"Original\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"Normalize(ToDtype(ToImage()))\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Data Augmentation Techniques](#toc0_)\n",
    "\n",
    "- v2.RandomCrop\n",
    "- v2.Resize\n",
    "- v2.RandomVerticalFlip\n",
    "- v2.RandomHorizontalFlip\n",
    "- v2.RandomRotation\n",
    "- v2.ColorJitter\n",
    "- v2.RandomAffine\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[v2.RandomCrop](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_crop_transform = v2.RandomCrop(size=(int(x_4[0].shape[1] / 4 * 3), int(x_4[0].shape[2] / 4 * 3)))\n",
    "x_5 = [random_crop_transform(img) for img in x_4]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_5[{i}].shape: {x_5[i].shape}\")\n",
    "    print(f\"x_5[{i}].dtype: {x_5[i].dtype}\")\n",
    "    print(f\"type(x_5[{i}]): {type(x_5[i])}\")\n",
    "    print(f\"x_5[{i}].min(): {x_5[i].min()}\")\n",
    "    print(f\"x_5[{i}].max(): {x_5[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x_4, x_5, y)):\n",
    "    axs[0, i].imshow(img1.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[0, i].set(title=\"x_4\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"v2.RandomCrop\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[v2.Resize](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.v2.Resize.html#torchvision.transforms.v2.Resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Resize.html#torchvision.transforms.v2.Resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform = v2.Resize(size=(x[0].shape[0], x[0].shape[1]))\n",
    "x_6 = [resize_transform(img) for img in x_5]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_6[{i}].shape: {x_6[i].shape}\")\n",
    "    print(f\"x_6[{i}].dtype: {x_6[i].dtype}\")\n",
    "    print(f\"type(x_6[{i}]): {type(x_6[i])}\")\n",
    "    print(f\"x_6[{i}].min(): {x_6[i].min()}\")\n",
    "    print(f\"x_6[{i}].max(): {x_6[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x_5, x_6, y)):\n",
    "    axs[0, i].imshow(img1.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[0, i].set(title=\"x_5\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"v2.Resize\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_3_'></a>[v2.RandomVerticalFlip](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomVerticalFlip.html#torchvision.transforms.v2.RandomVerticalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomVerticalFlip.html#torchvision.transforms.v2.RandomVerticalFlip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_verical_flip_transform = v2.RandomVerticalFlip(p=0.6)\n",
    "x_7 = [random_verical_flip_transform(img) for img in x_6]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_7[{i}].shape: {x_7[i].shape}\")\n",
    "    print(f\"x_7[{i}].dtype: {x_7[i].dtype}\")\n",
    "    print(f\"type(x_7[{i}]): {type(x_7[i])}\")\n",
    "    print(f\"x_7[{i}].min(): {x_7[i].min()}\")\n",
    "    print(f\"x_7[{i}].max(): {x_7[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x_6, x_7, y)):\n",
    "    axs[0, i].imshow(img1.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[0, i].set(title=\"x_6\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"v2.RandomVerticalFlip\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_4_'></a>[v2.RandomHorizontalFlip](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomHorizontalFlip.html#torchvision.transforms.RandomHorizontalFlip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_horizontal_flip_transform = v2.RandomHorizontalFlip(p=0.7)\n",
    "x_8 = [random_horizontal_flip_transform(img) for img in x_7]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_8[{i}].shape: {x_8[i].shape}\")\n",
    "    print(f\"x_8[{i}].dtype: {x_8[i].dtype}\")\n",
    "    print(f\"type(x_8[{i}]): {type(x_8[i])}\")\n",
    "    print(f\"x_8[{i}].min(): {x_8[i].min()}\")\n",
    "    print(f\"x_8[{i}].max(): {x_8[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x_7, x_8, y)):\n",
    "    axs[0, i].imshow(img1.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[0, i].set(title=\"x_7\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"v2.RandomHorizontalFlip\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_5_'></a>[v2.RandomRotation](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomRotation.html#torchvision.transforms.v2.RandomRotation](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomRotation.html#torchvision.transforms.v2.RandomRotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_rotation_transform = v2.RandomRotation(degrees=[0, 45])\n",
    "x_9 = [random_rotation_transform(img) for img in x_8]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_9[{i}].shape: {x_9[i].shape}\")\n",
    "    print(f\"x_9[{i}].dtype: {x_9[i].dtype}\")\n",
    "    print(f\"type(x_9[{i}]): {type(x_9[i])}\")\n",
    "    print(f\"x_9[{i}].min(): {x_9[i].min()}\")\n",
    "    print(f\"x_9[{i}].max(): {x_9[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x_8, x_9, y)):\n",
    "    axs[0, i].imshow(img1.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[0, i].set(title=\"x_8\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"v2.RandomRotation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_6_'></a>[v2.ColorJitter](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html#torchvision.transforms.ColorJitter](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html#torchvision.transforms.ColorJitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_jitter_transform = v2.ColorJitter(brightness=0.7, contrast=0.5, saturation=0.9, hue=0.3)\n",
    "x_10 = [color_jitter_transform(img) for img in x_9]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_10[{i}].shape: {x_10[i].shape}\")\n",
    "    print(f\"x_10[{i}].dtype: {x_10[i].dtype}\")\n",
    "    print(f\"type(x_10[{i}]): {type(x_10[i])}\")\n",
    "    print(f\"x_10[{i}].min(): {x_10[i].min()}\")\n",
    "    print(f\"x_10[{i}].max(): {x_10[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x_9, x_10, y)):\n",
    "    axs[0, i].imshow(img1.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[0, i].set(title=\"x_9\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"v2.ColorJitter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_7_'></a>[v2.RandomAffine](#toc0_)\n",
    "\n",
    "- [pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomAffine.html#torchvision.transforms.v2.RandomAffine](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.RandomAffine.html#torchvision.transforms.v2.RandomAffine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_affine_transform = v2.RandomAffine(degrees=0, shear=0.5, scale=[0.5, 1.5])\n",
    "x_11 = [random_affine_transform(img) for img in x_10]\n",
    "\n",
    "# log\n",
    "for i in range(len(x)):\n",
    "    print(f\"x_11[{i}].shape: {x_11[i].shape}\")\n",
    "    print(f\"x_11[{i}].dtype: {x_11[i].dtype}\")\n",
    "    print(f\"type(x_11[{i}]): {type(x_11[i])}\")\n",
    "    print(f\"x_11[{i}].min(): {x_11[i].min()}\")\n",
    "    print(f\"x_11[{i}].max(): {x_11[i].max()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x_10, x_11, y)):\n",
    "    axs[0, i].imshow(img1.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[0, i].set(title=\"x_10\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"v2.RandomAffine\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Mix transforms](#toc0_)\n",
    "\n",
    "- v2.Compose\n",
    "  - [pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(dtype=torch.float32, scale=True),\n",
    "        v2.Normalize(mean=mus, std=stds),\n",
    "        v2.RandomCrop(size=(int(x_4[0].shape[1] / 4 * 3), int(x_4[0].shape[2] / 4 * 3))),\n",
    "        v2.Resize(size=(x[0].shape[0], x[0].shape[1])),\n",
    "        v2.RandomVerticalFlip(p=0.6),\n",
    "        v2.RandomHorizontalFlip(p=0.7),\n",
    "        v2.RandomRotation(degrees=[0, 45]),\n",
    "        v2.ColorJitter(brightness=0.7, contrast=0.5, saturation=0.9, hue=0.3),\n",
    "        v2.RandomAffine(degrees=0, shear=0.5, scale=[0.5, 1.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_12 = [transforms(img) for img in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12, 8), layout=\"compressed\")\n",
    "for i, (img1, img2, label) in enumerate(zip(x, x_12, y)):\n",
    "    axs[0, i].imshow(img1)\n",
    "    axs[0, i].set(title=\"x\")\n",
    "    axs[1, i].imshow(img2.permute(1, 2, 0).clip(0, 1))\n",
    "    axs[1, i].set(title=\"x_12\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Effect of transforms](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataset\n",
    "trainset = CIFAR10(DATASET_DIR, train=True, transform=None, download=False)\n",
    "\n",
    "# pytorch subset\n",
    "num_samples = 10\n",
    "trainsubset = Subset(trainset, indices=range(num_samples))\n",
    "\n",
    "# log\n",
    "print(\"trainset:\")\n",
    "print(f\"\\tlen(trainset)          : {len(trainset)}\")\n",
    "print(f\"\\ttrainset.transform     : {trainset.transform}\")\n",
    "print(f\"\\ttype(trainset[0][0])   : {type(trainset[0][0])}\")\n",
    "print(f\"\\ttype(trainset[0][1])   : {type(trainset[0][1])}\")\n",
    "print(f\"\\ttype(trainset.data[0]) : {type(trainset.data[0])}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"trainsubset:\")\n",
    "print(f\"\\tlen(trainsubset)             : {len(trainsubset)}\")\n",
    "print(f\"\\trainsubset.dataset.transform : {trainsubset.dataset.transform}\")\n",
    "print(f\"\\ttype(trainsubset[0][0])      : {type(trainsubset[0][0])}\")\n",
    "print(f\"\\ttype(trainsubset[0][1])      : {type(trainsubset[0][1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# add transforms to the dataset\n",
    "trainset.transform = transforms\n",
    "\n",
    "# log\n",
    "print(\"trainset:\")\n",
    "print(f\"\\tlen(trainset): {len(trainset)}\")\n",
    "print(f\"\\ttrainset.transform:\\n{trainset.transform}\")\n",
    "print(f\"\\ttype(trainset[0][0])   : {type(trainset[0][0])}\")\n",
    "print(f\"\\ttrainset[0][0].dtype   : {trainset[0][0].dtype}\")\n",
    "print(f\"\\ttype(trainset[0][1])   : {type(trainset[0][1])}\")\n",
    "print(f\"\\ttype(trainset.data[0]) : {type(trainset.data[0])}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"trainsubset:\")\n",
    "print(f\"\\tlen(trainsubset): {len(trainsubset)}\")\n",
    "print(f\"\\ttrainsubset.dataset.transform:\\n{trainsubset.dataset.transform}\")\n",
    "print(f\"\\ttype(trainsubset[0][0]) : {type(trainsubset[0][0])}\")\n",
    "print(f\"\\ttrainsubset[0][0].dtype : {trainsubset[0][0].dtype}\")\n",
    "print(f\"\\ttype(trainsubset[0][1]) : {type(trainsubset[0][1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataloader\n",
    "trainloader = DataLoader(trainsubset, batch_size=num_samples, shuffle=False)\n",
    "next_iter_trainloader = next(iter(trainloader))\n",
    "\n",
    "print(\"trainloader:\")\n",
    "print(f\"\\ttype(next_iter_trainloader[0]) : {type(next_iter_trainloader[0])}\")\n",
    "print(f\"\\tnext_iter_trainloader[0].dtype : {next_iter_trainloader[0].dtype}\")\n",
    "print(f\"\\ttype(next_iter_trainloader[1]) : {type(next_iter_trainloader[1])}\")\n",
    "print(f\"\\tnext_iter_trainloader[1].dtype : {next_iter_trainloader[1].dtype}\")"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop-U_zYfVTd-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
