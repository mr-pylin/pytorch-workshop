{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/pytorch-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"text-align: right; flex: 1;\">\n",
    "        <a href=\"https://pytorch.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/pytorch/logo/pytorch-logo-dark.svg\" \n",
    "                 alt=\"PyTorch Logo\"\n",
    "                 style=\"max-height: 48px; width: auto; background-color: #ffffff; border-radius: 8px;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Generate Artificial Inputs](#toc2_)    \n",
    "- [Activation Functions](#toc3_)    \n",
    "  - [Built-in Activations](#toc3_1_)    \n",
    "    - [Elementwise](#toc3_1_1_)    \n",
    "      - [Linear](#toc3_1_1_1_)    \n",
    "      - [Sigmoid](#toc3_1_1_2_)    \n",
    "      - [LogSigmoid](#toc3_1_1_3_)    \n",
    "      - [Hyperbolic Tangent (Tanh)](#toc3_1_1_4_)    \n",
    "      - [Softplus](#toc3_1_1_5_)    \n",
    "      - [Rectified Linear Unit (ReLU)](#toc3_1_1_6_)    \n",
    "      - [LeakyReLU](#toc3_1_1_7_)    \n",
    "      - [Exponential Linear Unit (ELU)](#toc3_1_1_8_)    \n",
    "      - [Sigmoid Linear Unit (SiLU)](#toc3_1_1_9_)    \n",
    "      - [Mish](#toc3_1_1_10_)    \n",
    "      - [Gaussian Error Linear Units (GeLU)](#toc3_1_1_11_)    \n",
    "    - [Non-Elementwise](#toc3_1_2_)    \n",
    "      - [Softmax](#toc3_1_2_1_)    \n",
    "      - [LogSoftmax](#toc3_1_2_2_)    \n",
    "  - [Custom Activations](#toc3_2_)    \n",
    "    - [Example 1: Custom Sigmoid](#toc3_2_1_)    \n",
    "    - [Example 2: Custom Softmax](#toc3_2_2_)    \n",
    "  - [Plot Activations](#toc3_3_)    \n",
    "- [Threshold Functions](#toc4_)    \n",
    "  - [Step](#toc4_1_)    \n",
    "  - [Sign](#toc4_2_)    \n",
    "  - [Plot Thresholds](#toc4_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Generate Artificial Inputs](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_features = 2, 4\n",
    "input_values = torch.randn(batch_size, num_features) * 5\n",
    "\n",
    "# log\n",
    "print(f\"input_values:\\n{input_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features, out_features = input_values.shape[1], 5\n",
    "hidden_layer = nn.Linear(in_features, out_features, bias=False).requires_grad_(False)\n",
    "hidden_values = hidden_layer(input_values)\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features, out_features = hidden_values.shape[1], 4\n",
    "output_layer = nn.Linear(in_features, out_features, bias=False).requires_grad_(False)\n",
    "logits = output_layer(hidden_values)\n",
    "\n",
    "# log\n",
    "print(f\"logits:\\n{logits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Activation Functions](#toc0_)\n",
    "\n",
    "- Activation functions are used to introduce non-linearity into the neural network.\n",
    "- Without an activation function, a neural network would behave like a linear regression model, no matter how many layers it has!\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"../../assets/images/original/mlp/no-activation-network.svg\" alt=\"no-activation-network.svg\" style=\"width: 100%;\">\n",
    "  <figcaption style=\"text-align: center;\">Neural Network without Any Activation Functions is just a Linear Transformation of Input to the Output</figcaption>\n",
    "</figure>\n",
    "\n",
    "üì• **Importing Activation Functions**:\n",
    "\n",
    "- `torch`: Some activation functions, such as `torch.sigmoid` and `torch.tanh`, are available directly under the `torch` namespace.\n",
    "- `torch.nn`: Many activation functions are available as **classes** under `torch.nn`, such as `nn.ReLU`, `nn.Sigmoid`, and `nn.Tanh`.\n",
    "- `torch.nn.functional`: The functional API provides activation functions that can be applied **directly** in the forward pass, like `F.relu`, `F.sigmoid`, and `F.leaky_relu`.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- Non-linear Activations (weighted sum, nonlinearity): [docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "- Non-linear Activations (other): [docs.pytorch.org/docs/stable/nn.html#non-linear-activations-other](https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-other)\n",
    "- Non-linear activation functions: [docs.pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions](https://docs.pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)\n",
    "\n",
    "‚úçÔ∏è **Notes**:\n",
    "\n",
    "- Using Python functions is not a correct implementation of an activation function for Pytorch\n",
    "- The correct implementation is covered in the future notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Built-in Activations](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[Elementwise](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_1_'></a>[Linear](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Identity()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"linear(hidden_values):\\n{linear(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_2_'></a>[Sigmoid](#toc0_)\n",
    "\n",
    "- Historically used for `binary classification`, but less common now due to [vanishing gradient](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484) issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"sigmoid(hidden_values):\\n{sigmoid(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_3_'></a>[LogSigmoid](#toc0_)\n",
    "\n",
    "- Logarithm of `sigmoid`, less common but used in specific applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"log_sigmoid(hidden_values):\\n{log_sigmoid(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_4_'></a>[Hyperbolic Tangent (Tanh)](#toc0_)\n",
    "\n",
    "- Similar to `sigmoid` but centered around 0, used in [recurrent neural networks (RNNs)](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) and older architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = nn.Tanh()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"tanh(hidden_values):\\n{tanh(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_5_'></a>[Softplus](#toc0_)\n",
    "\n",
    "- Smooth approximation of `ReLU`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = nn.Softplus()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"softplus(hidden_values):\\n{softplus(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_6_'></a>[Rectified Linear Unit (ReLU)](#toc0_)\n",
    "\n",
    "- Most commonly used, computationally efficient, but suffers from the [dying ReLU](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks) ([vanishing gradient](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)) problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"relu(hidden_values):\\n{relu(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_7_'></a>[LeakyReLU](#toc0_)\n",
    "\n",
    "- Addresses the `dying ReLU` problem by allowing a small, non-zero gradient for negative inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"leaky_relu(hidden_values):\\n{leaky_relu(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_8_'></a>[Exponential Linear Unit (ELU)](#toc0_)\n",
    "\n",
    "- Similar to `LeakyReLU` but uses an exponential function for negative inputs, often providing better performance than `ReLU`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu = nn.ELU()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"elu(hidden_values):\\n{elu(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_9_'></a>[Sigmoid Linear Unit (SiLU)](#toc0_)\n",
    "\n",
    "- Combines ReLU-like behavior with a smooth curve, often yielding better results than `ReLU` (also known as **Swish**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silu = nn.SiLU()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"silu(hidden_values):\\n{silu(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_10_'></a>[Mish](#toc0_)\n",
    "\n",
    "- Self-regularized activation function, generally performs better than `ReLU` and its variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mish = nn.Mish()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"mish(hidden_values):\\n{mish(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_11_'></a>[Gaussian Error Linear Units (GeLU)](#toc0_)\n",
    "\n",
    "- Approximates the expected value of `ReLU` with a Gaussian input, often used in `transformer-based` models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu = nn.GELU()\n",
    "\n",
    "# log\n",
    "print(f\"hidden_values:\\n{hidden_values}\\n\")\n",
    "print(f\"gelu(hidden_values):\\n{gelu(hidden_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Non-Elementwise](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_1_'></a>[Softmax](#toc0_)\n",
    "\n",
    "- Used for `multi-class classification`, outputs probabilities [[mutually exclusive](https://en.wikipedia.org/wiki/Softmax_function)] for each class, often used `internally` in `CrossEntropyLoss`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# log\n",
    "print(f\"logits:\\n{logits}\\n\")\n",
    "print(f\"softmax(logits):\\n{softmax(logits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_2_'></a>[LogSoftmax](#toc0_)\n",
    "\n",
    "- Logarithm of softmax, often used in `NLLLoss`.\n",
    "- Reducing the risk of numerical issues and ensuring more reliable calculations rather than `Softmax`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "# log\n",
    "print(f\"logits:\\n{logits}\\n\")\n",
    "print(f\"log_softmax(logits):\\n{log_softmax(logits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Custom Activations](#toc0_)\n",
    "\n",
    "- You can define **custom** activation functions in PyTorch using `torch.nn.Module` or simple **Python functions**.\n",
    "- To create a custom activation, extend `torch.nn.Module` and implement the `forward` method, or define a function using PyTorch operations.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `nn.Module`: [docs.pytorch.org/docs/stable/generated/torch.nn.Module.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[Example 1: Custom Sigmoid](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sigmoid(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "# compute the activations\n",
    "activations = custom_sigmoid(hidden_values)\n",
    "\n",
    "# log\n",
    "print(f\"activations:\\n{activations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "# compute the activations\n",
    "sigmoid = CustomSigmoid()\n",
    "activations = sigmoid(hidden_values)\n",
    "\n",
    "# log\n",
    "print(f\"activations:\\n{activations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[Example 2: Custom Softmax](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_softmax(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    exp_tensor = torch.exp(x)\n",
    "    sum_exp_tensor = torch.sum(exp_tensor, dim=dim, keepdim=True)\n",
    "    return exp_tensor / sum_exp_tensor\n",
    "\n",
    "\n",
    "# compute the activations\n",
    "probs = custom_softmax(logits, dim=1)\n",
    "\n",
    "# log\n",
    "print(f\"probs:\\n{probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSoftmax(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        exp_tensor = torch.exp(x)\n",
    "        sum_exp_tensor = torch.sum(exp_tensor, dim=self.dim, keepdim=True)\n",
    "        return exp_tensor / sum_exp_tensor\n",
    "\n",
    "\n",
    "# compute the activations\n",
    "softmax = CustomSoftmax(dim=1)\n",
    "probs = softmax(logits)\n",
    "\n",
    "# log\n",
    "print(f\"probs:\\n{probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Plot Activations](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain [-10, +10]\n",
    "x = torch.linspace(-10, +10, 1001)\n",
    "\n",
    "# log\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(12, 8), layout=\"compressed\")\n",
    "fig.suptitle(\"Activation Functions\")\n",
    "axs[0, 0].plot(x, nn.ReLU()(x))\n",
    "axs[0, 0].set(title=\"Rectified Linear Unit (ReLU)\", xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[0, 1].plot(x, nn.LeakyReLU()(x))\n",
    "axs[0, 1].set(title=\"LeakyReLU\", xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[0, 2].plot(x, nn.ELU()(x))\n",
    "axs[0, 2].set(title=\"Exponential Linear Unit (ELU)\", xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[0, 3].plot(x, nn.SiLU()(x))\n",
    "axs[0, 3].set(title=\"Sigmoid Linear Unit (SiLU)\", xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[1, 0].plot(x, nn.Mish()(x))\n",
    "axs[1, 0].set(title=\"Mish\", xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[1, 1].plot(x, nn.Sigmoid()(x))\n",
    "axs[1, 1].set(title=\"Sigmoid\", xlim=[-10, 10], ylim=[-4, 4])\n",
    "axs[1, 2].plot(x, nn.LogSigmoid()(x))\n",
    "axs[1, 2].set(title=\"LogSigmoid\", xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[1, 3].plot(x, nn.Tanh()(x))\n",
    "axs[1, 3].set(title=\"Hyperbolic Tangent (Tanh)\", xlim=[-10, 10], ylim=[-4, 4])\n",
    "axs[2, 0].plot(x, nn.Softplus()(x))\n",
    "axs[2, 0].set(title=\"Softplus\", xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[2, 1].plot(x, nn.GELU()(x))\n",
    "axs[2, 1].set(title=\"Gaussian Error Linear Units (GeLU)\", xlim=[-10, 10], ylim=[-10, 10])\n",
    "axs[2, 2].plot(x, nn.Softmax(dim=0)(x))\n",
    "axs[2, 2].set(title=\"Softmax\", xlim=[-10, 10], ylim=[-0.05, 0.05])\n",
    "axs[2, 3].plot(x, nn.LogSoftmax(dim=0)(x))\n",
    "axs[2, 3].set(title=\"LogSoftmax\", xlim=[-10, 10], ylim=[-25, 0])\n",
    "for ax in fig.axes:\n",
    "    ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Threshold Functions](#toc0_)\n",
    "\n",
    "- Threshold functions are a simpler type of activation function primarily used in the early development of neural networks\n",
    "- These functions decide whether a neuron should be activated or not based on whether the input surpasses a certain threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Step](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.where(x >= 0, torch.ones_like(x), torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Sign](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.where(x > 0, torch.ones_like(x), torch.where(x < 0, torch.ones_like(x) * -1, torch.zeros_like(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Plot Thresholds](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain [-10, +10]\n",
    "x = torch.linspace(-10, +10, 1001)\n",
    "\n",
    "# log\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), layout=\"compressed\")\n",
    "fig.suptitle(\"Threshold Functions\")\n",
    "axs[0].plot(x, step(x))\n",
    "axs[0].grid(True)\n",
    "axs[0].set(title=\"step\", xlim=[-10, 10], ylim=[-2, 2])\n",
    "axs[1].plot(x, sign(x))\n",
    "axs[1].grid(True)\n",
    "axs[1].set(title=\"sign\", xlim=[-10, 10], ylim=[-2, 2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
