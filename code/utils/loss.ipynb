{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/pytorch-workshop](https://github.com/mr-pylin/pytorch-workshop)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Generate Artificial Outputs](#toc2_)    \n",
    "- [Loss Function](#toc3_)    \n",
    "  - [Built-in Losses](#toc3_1_)    \n",
    "    - [Regression tasks](#toc3_1_1_)    \n",
    "    - [Classification tasks](#toc3_1_2_)    \n",
    "      - [Binary Classification](#toc3_1_2_1_)    \n",
    "      - [Multiclass Classification](#toc3_1_2_2_)    \n",
    "    - [Specialized Classification](#toc3_1_3_)    \n",
    "    - [Metric Learning / Ranking Losses](#toc3_1_4_)    \n",
    "    - [Other](#toc3_1_5_)    \n",
    "  - [Custom Losses](#toc3_2_)    \n",
    "    - [Example 1: Mean Squared Error [Regression]](#toc3_2_1_)    \n",
    "    - [Example 2: Cross Entropy Loss [binary classification]](#toc3_2_2_)    \n",
    "    - [Example 3: Cross Entropy Loss [multiclass classification]](#toc3_2_3_)    \n",
    "- [Comparison](#toc4_)    \n",
    "  - [BCELoss vs MSELoss](#toc4_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Generate Artificial Outputs](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "y_true_reg = torch.randn(10, 1)  # ground truth values\n",
    "y_pred_reg = torch.randn(10, 1)  # predicted values\n",
    "\n",
    "# log\n",
    "print(f\"y_true_reg : {y_true_reg}\")\n",
    "print(f\"y_pred_reg : {y_pred_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification\n",
    "num_classes = 2\n",
    "batch_size = 10\n",
    "\n",
    "y_true_cls_bin = torch.randint(0, num_classes, (batch_size,), dtype=torch.float32)  # true class indices\n",
    "y_pred_cls_bin = torch.randn(batch_size)  # logits (before sigmoid)\n",
    "\n",
    "# log\n",
    "print(f\"y_true_cls_bin : {y_true_cls_bin}\")\n",
    "print(f\"y_pred_cls_bin : {y_pred_cls_bin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass classification\n",
    "num_classes = 5\n",
    "batch_size = 10\n",
    "\n",
    "y_true_cls_multi = torch.randint(0, num_classes, (batch_size,))  # true class indices\n",
    "y_pred_cls_multi = torch.randn(batch_size, num_classes)  # logits (before softmax)\n",
    "\n",
    "# log\n",
    "print(f\"y_true_cls_multi : {y_true_cls_multi}\")\n",
    "print(f\"y_pred_cls_multi : {y_pred_cls_multi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Loss Function](#toc0_)\n",
    "\n",
    "- A function that quantifies the difference between the **predicted** output of a model and the **true** target values.\n",
    "- It serves as a **measure** of how well (or poorly) the model's predictions align with the actual outcomes, guiding the optimization process during training.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"../../assets/images/third_party/loss-function.png\" alt=\"loss-function.png\" style=\"width: 100%;\">\n",
    "  <figcaption style=\"text-align: center;\">¬©Ô∏è Image: <a href= \"https://www.offconvex.org/2016/03/22/saddlepoints\">offconvex.org/2016/03/22/saddlepoints</a></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Built-in Losses](#toc0_)\n",
    "\n",
    "- PyTorch provides a variety of built-in **loss functions** to simplify training and evaluation, covering classification, regression, and other tasks.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- Loss Functions: [docs.pytorch.org/docs/stable/nn.html#loss-functions](https://docs.pytorch.org/docs/stable/nn.html#loss-functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[Regression tasks](#toc0_)\n",
    "\n",
    "1. [Mean Absolute Error](https://docs.pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) (`torch.nn.L1Loss`)\n",
    "    - Measures the mean absolute error (MAE) between each element in the input and target tensors (aka L1 norm)\n",
    "    - Robust to outliers BUT does not provide gradients for large errors, leading to slower convergence\n",
    "    - **Formula**:\n",
    "      - $\\text{L1Loss} = \\frac{1}{N} \\sum_{i=1}^{N} |\\hat{y}_i - y_i|$\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $\\hat{y}_i$: Predicted value for the $i_{th}$ sample\n",
    "      - $y_i$: True value for the $i_{th}$ sample\n",
    "\n",
    "1. [Mean Squared Error](https://docs.pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) (`torch.nn.MSELoss`)\n",
    "    - Measures the mean squared error (MSE) between each element in the input and target tensors (aka L2 norm)\n",
    "    - Sensitive to outliers because it penalizes large errors quadratically (due to squaring)\n",
    "    - **Formula**:\n",
    "      - $\\text{MSELoss} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$\n",
    "    - **Notations**:\n",
    "      - \\(N\\): Number of samples\n",
    "      - \\(\\hat{y}_i\\): Predicted value for the \\(i\\)-th sample\n",
    "      - \\(y_i\\): True value for the \\(i\\)-th sample\n",
    "\n",
    "1. [Huber](https://docs.pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html) (`torch.nn.HuberLoss`)\n",
    "    - Combines the best properties of MAE and MSE losses by being less sensitive to outliers than MSE and more stable than MAE\n",
    "    - It acts `quadratic` for small errors (similar to MSE) and `linear` for large errors (similar to MAE)\n",
    "    - **Formula**:\n",
    "      - $\n",
    "        \\text{HuberLoss} = \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "        \\begin{cases}\n",
    "        0.5 (\\hat{y}_i - y_i)^2 & \\text{if } |\\hat{y}_i - y_i| < \\delta \\\\\n",
    "        \\delta \\cdot (|\\hat{y}_i - y_i| - 0.5 \\cdot \\delta) & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "        $\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $\\hat{y}_i$: Predicted value for the $i_{th}$ sample\n",
    "      - $y_i$: True value for the $i_{th}$ sample\n",
    "      - $\\delta$: Threshold parameter\n",
    "\n",
    "1. [Smooth L1](https://docs.pytorch.org/docs/main/generated/torch.nn.SmoothL1Loss.html) (`torch.nn.SmoothL1Loss`)\n",
    "    - It provides a smooth transition between quadratic and linear behavior, unlike Huber Loss which has a sharp cutoff\n",
    "    - Often used in object detection tasks\n",
    "    - **Formula**:\n",
    "      - $\n",
    "        \\text{SmoothL1Loss} = \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "        \\begin{cases}\n",
    "        0.5 \\cdot \\frac{(\\hat{y}_i - y_i)^2}{\\beta} & \\text{if } |\\hat{y}_i - y_i| < \\beta \\\\\n",
    "        |\\hat{y}_i - y_i| - 0.5 \\cdot \\beta & \\text{otherwise}\n",
    "        \\end{cases}\n",
    "        $\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $\\hat{y}_i$: Predicted value for the \\hat{y}_i sample\n",
    "      - $y_i$: True value for the \\hat{y}_i sample\n",
    "      - $\\beta$: Threshold parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y_pred_reg, y_true_reg)\n",
    "\n",
    "# log\n",
    "print(f\"loss.item(): {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Classification tasks](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_1_'></a>[Binary Classification](#toc0_)\n",
    "\n",
    "1. [Binary Cross-Entropy](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) (`torch.nn.BCELoss`)\n",
    "    - Measures the binary cross-entropy loss between the target and the input probabilities (`Sigmoid`).\n",
    "    - Penalizes incorrect predictions more heavily, especially when the predicted probability is far from the actual class.\n",
    "    - **Formula**:\n",
    "      - $\\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]$\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $y_i$: True label for the $i_{th}$ sample (0 or 1)\n",
    "      - $p_i$: Predicted probability for the $i_{th}$ sample\n",
    "\n",
    "1. [Binary Cross-Entropy with Logits](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) (`torch.nn.BCEWithLogitsLoss`)\n",
    "    - Measures the binary cross-entropy loss between the target and the input logits.\n",
    "    - Combines a `torch.nn.Sigmoid` and `torch.nn.BCELoss` in one single class.\n",
    "    - More numerically stable than using a plain sigmoid followed by a binary cross-entropy loss.\n",
    "    - **Formula**:\n",
    "      - $\\text{BCEWithLogits} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\sigma(\\hat{y}_i)) + (1 - y_i) \\log(1 - \\sigma(\\hat{y}_i)) \\right]$\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $y_i$: True label for the $i_{th}$ sample (0 or 1)\n",
    "      - $\\hat{y}_i$: Logit (raw model output) for the $i_{th}$ sample\n",
    "      - $\\sigma(\\hat{y}_i)$: Sigmoid function applied to $\\hat{y}_i$, i.e., $\\sigma(\\hat{y}_i) = \\frac{1}{1 + e^{- \\hat{y}_i}}$\n",
    "\n",
    "1. [Soft Margin](https://docs.pytorch.org/docs/main/generated/torch.nn.SoftMarginLoss.html) (`torch.nn.SoftMarginLoss`)\n",
    "    - Measures the logistic loss between the target and the input.\n",
    "    - Expects target values to be either 1 or -1.\n",
    "    - **Formula**:\n",
    "      - $\\text{SoftMarginLoss} = \\frac{1}{N} \\sum_{i=1}^{N} \\log(1 + \\exp(-y_i \\hat{y}_i))$\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $y_i$: True label for the $i_{th}$ sample (1 or -1)\n",
    "      - $\\hat{y}_i$: Logit (raw model output) for the $i_{th}$ sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(y_pred_cls_bin, y_true_cls_bin)\n",
    "\n",
    "# log\n",
    "print(f\"loss.item(): {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_2_'></a>[Multiclass Classification](#toc0_)\n",
    "\n",
    "1. [Negative Log Likelihood](https://docs.pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) (`torch.nn.NLLLoss`)\n",
    "    - Measures the negative log likelihood loss between the target and the input log-probabilities (`LogSoftmax`).\n",
    "    - Directly applying `LogSoftmax` to logits can lead to numerical instability (issues of overflow and underflow in computational systems).\n",
    "    - **Formula**:\n",
    "      - $\\text{NLLLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(\\hat{y}_{i, y_i})$\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $\\hat{y}_{i, y_i}$: Log-probability of the true class $y_i$ for the $i_{th}$ sample\n",
    "\n",
    "1. [Cross-Entropy Loss](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) (`torch.nn.CrossEntropyLoss`)\n",
    "    - Measures the cross-entropy loss between the target and the input logits.\n",
    "    - Combines a `torch.nn.LogSoftmax` and `torch.nn.NLLLoss` in one single class.\n",
    "    - It reduces the number of operations required compared to applying sigmoid and BCELoss separately\n",
    "    - **Formula**:\n",
    "      - $\\text{CrossEntropyLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left(\\frac{\\exp(\\hat{y}_{i, y_i})}{\\sum_{j=1}^{C} \\exp(\\hat{y}_{i, j})}\\right)$\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $C$: Number of classes\n",
    "      - $\\hat{y}_{i, y_i}$: Log-probability of the true class $y_i$ for the $i_{th}$ sample\n",
    "      - $\\hat{y}_{i, j}$: Logit (raw model output) for class $j$ of the $i_{th}$ sample\n",
    "      - $y_i$: True class for the $i_{th}$ sample\n",
    "\n",
    "1. [Multi-Label Soft Margin](https://docs.pytorch.org/docs/main/generated/torch.nn.MultiLabelSoftMarginLoss.html) (`torch.nn.MultiLabelSoftMarginLoss`)\n",
    "    - Measures the multi-label one-versus-all loss based on max-entropy between the target and the input.\n",
    "    - Useful for multi-label classification tasks where each sample can belong to multiple classes.\n",
    "    - **Formula**:\n",
    "      - $\\text{MultiLabelSoftMarginLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{C} \\sum_{j=1}^{C} \\left[ y_{i, j} \\log(\\sigma(\\hat{y}_{i, j})) + (1 - y_{i, j}) \\log(1 - \\sigma(\\hat{y}_{i, j})) \\right]$\n",
    "    - **Notations**:\n",
    "      - $N$: Number of samples\n",
    "      - $C$: Number of classes\n",
    "      - $y_{i, j}$: True label for class $j$ of the $i_{th}$ sample (0 or 1)\n",
    "      - $\\hat{y}_{i, j}$: Logit (raw model output) for class $j$ of the $i_{th}$ sample\n",
    "      - $\\sigma(\\hat{y}_{i, j})$: Sigmoid function applied to $\\hat{y}_{i, j}$, i.e., $\\sigma(\\hat{y}_{i, j}) = \\frac{1}{1 + e^{-\\hat{y}_{i, j}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(y_pred_cls_multi, y_true_cls_multi)\n",
    "\n",
    "# log\n",
    "print(f\"loss.item(): {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[Specialized Classification](#toc0_)\n",
    "\n",
    "1. [Connectionist Temporal Classification](https://docs.pytorch.org/docs/main/generated/torch.nn.CTCLoss.html) (`torch.nn.CTCLoss`)\n",
    "1. [Poisson Negative log likelihood](https://docs.pytorch.org/docs/main/generated/torch.nn.PoissonNLLLoss.html) (`torch.nn.PoissonNLLLoss`)\n",
    "1. [Gaussian negative log likelihood](https://docs.pytorch.org/docs/main/generated/torch.nn.GaussianNLLLoss.html) (`torch.nn.GaussianNLLLoss`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_4_'></a>[Metric Learning / Ranking Losses](#toc0_)\n",
    "\n",
    "1. [Margin Ranking](https://docs.pytorch.org/docs/main/generated/torch.nn.MarginRankingLoss.html) (`torch.nn.MarginRankingLoss`)\n",
    "1. [Hinge Embedding](https://docs.pytorch.org/docs/main/generated/torch.nn.HingeEmbeddingLoss.html) (`torch.nn.HingeEmbeddingLoss`)\n",
    "1. [Cosine Embedding](https://docs.pytorch.org/docs/main/generated/torch.nn.CosineEmbeddingLoss.html) (`torch.nn.CosineEmbeddingLoss`)\n",
    "1. [Multi Margin](https://docs.pytorch.org/docs/main/generated/torch.nn.MultiMarginLoss.html) (`torch.nn.MultiMarginLoss`)\n",
    "1. [Triplet Margin](https://docs.pytorch.org/docs/main/generated/torch.nn.TripletMarginLoss.html) (`torch.nn.TripletMarginLoss`)\n",
    "1. [Triplet Margin With Distance](https://docs.pytorch.org/docs/main/generated/torch.nn.TripletMarginWithDistanceLoss.html) (`torch.nn.TripletMarginWithDistanceLoss`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_5_'></a>[Other](#toc0_)\n",
    "\n",
    "1. [Kullback-Leibler divergence](https://docs.pytorch.org/docs/main/generated/torch.nn.KLDivLoss.html) (`torch.nn.KLDivLoss`)\n",
    "1. [Multi-Label Margin](https://docs.pytorch.org/docs/main/generated/torch.nn.MultiLabelMarginLoss.html) (`torch.nn.MultiLabelMarginLoss`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Custom Losses](#toc0_)\n",
    "\n",
    "- PyTorch lets you define **custom** loss functions using `torch.nn.Module` or simple **Python functions**.\n",
    "- To create a custom loss, extend `torch.nn.Module` and implement the `forward` method, or define a function that operates on tensors directly.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `nn.Module`: [docs.pytorch.org/docs/stable/generated/torch.nn.Module.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[Example 1: Mean Squared Error [Regression]](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "\n",
    "# compute the loss\n",
    "loss = custom_mse(y_pred_reg, y_true_reg)\n",
    "\n",
    "# log\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        loss = torch.mean((y_pred - y_true) ** 2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# compute the loss\n",
    "criterion = CustomMSE()\n",
    "loss = criterion(y_pred_reg, y_true_reg)\n",
    "\n",
    "# log\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[Example 2: Cross Entropy Loss [binary classification]](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_binary_cross_entropy(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    # numerically stable computation\n",
    "    loss = torch.clamp(y_pred, min=0) - y_pred * y_true + torch.log1p(torch.exp(-torch.abs(y_pred)))\n",
    "\n",
    "    # normal computation\n",
    "    # y_pred_sigmoid = torch.sigmoid(y_pred)\n",
    "    # loss = - (y_true * torch.log(y_pred_sigmoid) + (1 - y_true) * torch.log(1 - y_pred_sigmoid))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# compute the loss\n",
    "loss = custom_binary_cross_entropy(y_pred_cls_bin, y_true_cls_bin)\n",
    "\n",
    "# log\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBinaryCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        # numerically stable computation\n",
    "        loss = torch.clamp(y_pred, min=0) - y_pred * y_true + torch.log1p(torch.exp(-torch.abs(y_pred)))\n",
    "\n",
    "        # normal computation\n",
    "        # y_pred_sigmoid = torch.sigmoid(y_pred)\n",
    "        # loss = - (y_true * torch.log(y_pred_sigmoid) + (1 - y_true) * torch.log(1 - y_pred_sigmoid))\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# compute the loss\n",
    "criterion = CustomBinaryCrossEntropy()\n",
    "loss = criterion(y_pred_cls_bin, y_true_cls_bin)\n",
    "\n",
    "# log\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_3_'></a>[Example 3: Cross Entropy Loss [multiclass classification]](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cross_entropy(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    log_probs = F.log_softmax(y_pred, dim=1)\n",
    "    loss = -log_probs[torch.arange(y_true.shape[0]), y_true]\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# compute the loss\n",
    "loss = custom_cross_entropy(y_pred_cls_multi, y_true_cls_multi)\n",
    "\n",
    "# log\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        log_probs = F.log_softmax(y_pred, dim=1)  # apply log softmax for numerical stability\n",
    "        loss = -log_probs[torch.arange(y_true.shape[0]), y_true]\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# compute the loss\n",
    "criterion = CustomCrossEntropy()\n",
    "loss = criterion(y_pred_cls_multi, y_true_cls_multi)\n",
    "\n",
    "# log\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Comparison](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[BCELoss vs MSELoss](#toc0_)\n",
    "\n",
    "- `BCELoss` is more **sensitive** to the amount of error (grows **faster** if the distance between `y_true` & `y_pred` is high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 3 samples for a binary classification\n",
    "y_true = torch.tensor([[0], [0], [0]], dtype=torch.float32)\n",
    "\n",
    "# output of model\n",
    "output = torch.tensor([[0], [1.09864], [10]], dtype=torch.float32)\n",
    "y_pred = torch.sigmoid(output)\n",
    "\n",
    "mse_1 = nn.MSELoss(reduction=\"none\")(y_pred, y_true).squeeze()\n",
    "mse_2 = nn.MSELoss()(y_pred, y_true)\n",
    "bce_1 = nn.BCELoss(reduction=\"none\")(y_pred, y_true).squeeze()\n",
    "bce_2 = nn.BCELoss()(y_pred, y_true)\n",
    "\n",
    "# log\n",
    "print(f\"y_true: {y_true.squeeze()}\")\n",
    "print(f\"y_pred: {y_pred.squeeze()}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"MSELoss [per sample]: {mse_1}\")\n",
    "print(f\"MSELoss             : {mse_2:.5f}\")\n",
    "print(f\"BCELoss [per sample]: {bce_1}\")\n",
    "print(f\"BCELoss             : {bce_2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "y_true = torch.zeros(size=(100, 1))\n",
    "y_pred = torch.sigmoid(torch.linspace(-10, +10, 100).reshape(-1, 1))\n",
    "bce_loss = nn.BCELoss(reduction=\"none\")(y_pred, y_true)\n",
    "mse_loss = nn.MSELoss(reduction=\"none\")(y_pred, y_true)\n",
    "\n",
    "plt.plot(y_pred, bce_loss, label=\"BCELoss\")\n",
    "plt.plot(y_pred, mse_loss, label=\"MSELoss\")\n",
    "plt.title(f\"y_true = {y_true[0, 0]}   |   {y_pred.min().round()} <= y_pred <= {y_pred.max().round()}\")\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}