{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/pytorch-workshop](https://github.com/mr-pylin/pytorch-workshop)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Dataset](#toc2_)    \n",
    "  - [Load Iris Dataset](#toc2_1_)    \n",
    "- [Torch Dataset](#toc3_)    \n",
    "  - [Built-in Datasets](#toc3_1_)    \n",
    "    - [General-Purpose Dataset Wrappers](#toc3_1_1_)    \n",
    "      - [TensorDataset](#toc3_1_1_1_)    \n",
    "      - [Subset](#toc3_1_1_2_)    \n",
    "      - [RandomSplit](#toc3_1_1_3_)    \n",
    "      - [ConcatDataset](#toc3_1_1_4_)    \n",
    "      - [ChainDataset](#toc3_1_1_5_)    \n",
    "      - [ImageFolder](#toc3_1_1_6_)    \n",
    "      - [DatasetFolder](#toc3_1_1_7_)    \n",
    "    - [Predefined Datasets](#toc3_1_2_)    \n",
    "      - [Torchvision Built-in Datasets](#toc3_1_2_1_)    \n",
    "      - [Torchaudio Built-in Datasets](#toc3_1_2_2_)    \n",
    "  - [Custom Datasets](#toc3_2_)    \n",
    "    - [Base Class: Dataset](#toc3_2_1_)    \n",
    "    - [Base Class: VisionDataset](#toc3_2_2_)    \n",
    "    - [Base Class: VisionDataset's Derived Class](#toc3_2_3_)    \n",
    "- [Torch DataLoader](#toc4_)    \n",
    "  - [Strategies for updating weights](#toc4_1_)    \n",
    "    - [Batch Gradient Descent](#toc4_1_1_)    \n",
    "    - [Stochastic Gradient Descent](#toc4_1_2_)    \n",
    "    - [Mini-Batch Gradient Descent](#toc4_1_3_)    \n",
    "- [Torch DataParallel](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import ChainDataset, ConcatDataset, DataLoader, Dataset, Subset, TensorDataset, random_split\n",
    "from torchaudio.datasets import YESNO\n",
    "from torchvision.datasets import MNIST, DatasetFolder, ImageFolder, VisionDataset\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update paths as needed based on your project structure\n",
    "MNIST_DIR = r\"../../datasets\"\n",
    "YESNO_DIR = r\"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Dataset](#toc0_)\n",
    "\n",
    "$\n",
    "X = \\begin{bmatrix}\n",
    "        x_{1}^1 & x_{1}^2 & \\cdots & x_{1}^n \\\\\n",
    "        x_{2}^1 & x_{2}^2 & \\cdots & x_{2}^n \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        x_{m}^1 & x_{m}^2 & \\cdots & x_{m}^n \\\\\n",
    "    \\end{bmatrix}_{m \\times n} \\quad \\text{(m: number of samples, n: number of features)}\n",
    "$\n",
    "\n",
    "$\n",
    "Y = \\begin{bmatrix}\n",
    "        y_{1} \\\\\n",
    "        y_{2} \\\\\n",
    "        \\vdots \\\\\n",
    "        y_{m} \\\\\n",
    "    \\end{bmatrix}_{m \\times 1} \\quad \\text{(m: number of samples)}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Load Iris Dataset](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset_url = (\n",
    "    r\"https://raw.githubusercontent.com/mr-pylin/datasets/refs/heads/main/data/tabular-data/iris/dataset.csv\"\n",
    ")\n",
    "\n",
    "# pandas data-frame\n",
    "df = pd.read_csv(iris_dataset_url, encoding=\"utf-8\")\n",
    "\n",
    "# log\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df[\"class\"].unique()\n",
    "class_to_idx = {l: i for i, l in enumerate(classes)}\n",
    "\n",
    "# split dataset into features and labels\n",
    "X, y = df.iloc[:, :4].values, df.iloc[:, 4].values\n",
    "\n",
    "# convert categorical labels into indices\n",
    "y = np.array([class_to_idx[l] for l in y])\n",
    "\n",
    "# properties of the dataset\n",
    "num_samples, num_features = X.shape\n",
    "classes, samples_per_class = np.unique(y, return_counts=True)\n",
    "\n",
    "# log\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "print(f\"X.dtype: {X.dtype}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"y.dtype: {y.dtype}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"classes          : {classes}\")\n",
    "print(f\"samples per class: {samples_per_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy.ndarray to torch.Tensor\n",
    "X = torch.from_numpy(X.astype(np.float32))\n",
    "y = torch.from_numpy(y.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "# log\n",
    "print(f\"x.shape: {X.shape}\")\n",
    "print(f\"x.dtype: {X.dtype}\")\n",
    "print(f\"x.ndim : {X.ndim}\\n\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"y.dtype: {y.dtype}\")\n",
    "print(f\"y.ndim : {y.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Torch Dataset](#toc0_)\n",
    "\n",
    "- a **Dataset** is an abstraction that represents a collection of data samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Built-in Datasets](#toc0_)\n",
    "\n",
    "- It provides a standardized way to **access** and **manipulate** data, making it easier to work with different types of datasets.\n",
    "\n",
    "üìù **Docs & Tutorials** üìö:\n",
    "\n",
    "- `torch.utils.data`: [pytorch.org/docs/stable/data.html](https://pytorch.org/docs/stable/data.html)\n",
    "- Datasets & DataLoaders: [pytorch.org/tutorials/beginner/basics/data_tutorial.html](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "- ImageFolder: [pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html)\n",
    "- DatasetFolder: [pytorch.org/vision/stable/generated/torchvision.datasets.DatasetFolder.html](https://pytorch.org/vision/stable/generated/torchvision.datasets.DatasetFolder.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[General-Purpose Dataset Wrappers](#toc0_)\n",
    "\n",
    "- They provide flexible tools to efficiently structure, manipulate, and process data.\n",
    "- These classes are particularly useful when dealing with existing datasets, combining multiple datasets without writing custom classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_1_'></a>[TensorDataset](#toc0_)\n",
    "\n",
    "- The `TensorDataset` class is part of the `torch.utils.data` module and allows you to **create** a dataset from one or more **tensors**.\n",
    "- It assumes that the **first dimension** of each tensor corresponds to the **number of samples**, and it pairs the tensors together to form **samples**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = TensorDataset(X, y)\n",
    "\n",
    "# log\n",
    "print(f\"type(dataset_1) : {type(dataset_1)}\")\n",
    "print(f\"len(dataset_1)  : {len(dataset_1)}\")\n",
    "print(f\"dataset_1[0][0] : {dataset_1[0][0]}\")\n",
    "print(f\"dataset_1[0][1] : {dataset_1[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_2_'></a>[Subset](#toc0_)\n",
    "\n",
    "- The `Subset` class is part of the `torch.utils.data` module and allows you to **create** a dataset from a selected **subset** of a larger dataset.\n",
    "- It requires a list of **indices** that specify which samples to include, enabling **train-validation-test** splits or **selecting** specific data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris-setosa\n",
    "subset_1 = Subset(dataset_1, indices=range(50))\n",
    "\n",
    "# Iris-versicolor\n",
    "subset_2 = Subset(dataset_1, indices=range(50, 100))\n",
    "\n",
    "# Iris-virginica\n",
    "subset_3 = Subset(dataset_1, indices=range(100, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log\n",
    "print(f\"type(subset_1)                        : {type(subset_1)}\")\n",
    "print(f\"len(subset_1)                         : {len(subset_1)}\")\n",
    "print(f\"subset_1.indices                      : {subset_1.indices}\")\n",
    "print(f\"subset_1.dataset                      : {subset_1.dataset}\")\n",
    "print(f\"subset_1[0]                           : {subset_1[0]}\")\n",
    "print(f\"subset_1.dataset[subset_1.indices[0]] : {subset_1.dataset[subset_1.indices[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_3_'></a>[RandomSplit](#toc0_)\n",
    "\n",
    "- The `random_split` function is part of the `torch.utils.data` module and allows you to **randomly** divide a dataset into **non-overlapping** subsets.\n",
    "- It takes a dataset and a list of **split sizes**, ensuring a randomized but consistent division across multiple runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio, val_ratio, test_ratio = (0.8, 0.1, 0.1)\n",
    "train_set_1, val_set_1, test_set_1 = random_split(dataset_1, [train_ratio, val_ratio, test_ratio])\n",
    "\n",
    "# log\n",
    "print(f\"type(train_set_1)   : {type(train_set_1)}\")\n",
    "print(f\"len(train_set_1)    : {len(train_set_1)}\")\n",
    "print(f\"train_set_1.indices : {train_set_1.indices}\")\n",
    "print(f\"train_set_1.dataset : {train_set_1.dataset}\")\n",
    "print(f\"train_set_1[0]      : {train_set_1[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length, val_length, test_length = (120, 15, 15)\n",
    "train_set_2, val_set_2, test_set_2 = random_split(dataset_1, [train_ratio, val_ratio, test_ratio])\n",
    "\n",
    "# log\n",
    "print(f\"type(test_set_2)   : {type(test_set_2)}\")\n",
    "print(f\"len(test_set_2)    : {len(test_set_2)}\")\n",
    "print(f\"test_set_2.indices : {test_set_2.indices}\")\n",
    "print(f\"test_set_2.dataset : {test_set_2.dataset}\")\n",
    "print(f\"test_set_2[0]      : {test_set_2[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_4_'></a>[ConcatDataset](#toc0_)\n",
    "\n",
    "- The `ConcatDataset` class is part of the `torch.utils.data` module and allows you to **merge** multiple datasets into a single dataset.\n",
    "- It assumes that all datasets have the same **structure**, making it useful for combining different datasets for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = ConcatDataset([train_set_1, val_set_1, test_set_1])\n",
    "\n",
    "# log\n",
    "print(f\"type(dataset_2)            : {type(dataset_2)}\")\n",
    "print(f\"len(dataset_2)             : {len(dataset_2)}\")\n",
    "print(f\"dataset_2.cumulative_sizes : {dataset_2.cumulative_sizes}\")\n",
    "print(f\"dataset_2.datasets         : {dataset_2.datasets}\")\n",
    "print(f\"dataset_2[0]               : {dataset_2[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_5_'></a>[ChainDataset](#toc0_)\n",
    "\n",
    "- The `ChainDataset` class is part of the `torch.utils.data` module and allows you to **iterate** sequentially over multiple datasets **without** merging them.\n",
    "- It doesn't index elements but instead iterates over datasets in sequence (datasets must be iterables).\n",
    "- This is efficient for streaming **large** datasets when merging them into memory is impractical.\n",
    "\n",
    "‚úçÔ∏è **Note**:\n",
    "\n",
    "- `ChainDataset` in PyTorch only supports datasets that are instances of `IterableDataset`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_1, val_set_1, test_set_1 are instances from `Dataset` instead of `IterableDataset`\n",
    "# so this code is not practical and it's just for demonstration purpose\n",
    "dataset_3 = ChainDataset([train_set_1, val_set_1, test_set_1])\n",
    "\n",
    "# log\n",
    "print(f\"type(dataset_3)    : {type(dataset_3)}\")\n",
    "print(f\"dataset_3.datasets : {dataset_3.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_6_'></a>[ImageFolder](#toc0_)\n",
    "\n",
    "- The `ImageFolder` class is a subclass of `DatasetFolder` and part of the `torchvision.datasets` that allows you to load **image** datasets organized in **folders**.\n",
    "- The **folder** structure is expected to have **subfolders**, each representing a class. Each subfolder should contain images of the corresponding class.\n",
    "- This is commonly used for loading datasets in image classification tasks.\n",
    "\n",
    "‚úçÔ∏è **Note**:\n",
    "\n",
    "- `ImageFolder` automatically labels images based on the **folder names**, making it convenient for supervised learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    image_dataset_1 = ImageFolder(root=\"/path/to/data\", transform=None)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_7_'></a>[DatasetFolder](#toc0_)\n",
    "\n",
    "- The `DatasetFolder` class is a subclass of `VisionDataset` and part of the `torchvision.datasets` designed to load a dataset where the images are organized in a **custom** manner.\n",
    "- Unlike `ImageFolder`, it allows for more **flexibility** in terms of dataset organization but still requires that images be placed in a **folder-based** structure.\n",
    "\n",
    "‚úçÔ∏è **Note**:\n",
    "\n",
    "- `DatasetFolder` requires a **custom loader** function, which can be used to define how images are **read**, **processed**, and **returned**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loader(path):\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "try:\n",
    "    image_dataset_2 = DatasetFolder(root=\"/path/to/data\", loader=custom_loader, extensions=\".jpg\", transform=None)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Predefined Datasets](#toc0_)\n",
    "\n",
    "- datasets designed for specific benchmarks, with automatic downloading and processing.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- Torchvision Built-in Datasets: [pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)\n",
    "- Torchaudio Built-in Datasets: [pytorch.org/audio/stable/datasets.html](https://pytorch.org/audio/stable/datasets.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_1_'></a>[Torchvision Built-in Datasets](#toc0_)\n",
    "\n",
    "- Torchvision provides many built-in datasets in the `torchvision.datasets` module, as well as utility classes for building your own datasets.\n",
    "- **Categories of built-in datasets**:\n",
    "  - Image classification\n",
    "  - Image Detection or Segmentation\n",
    "  - Optical Flow\n",
    "  - Stereo Matching\n",
    "  - Image pairs\n",
    "  - Image captioning\n",
    "  - Video classification\n",
    "  - Video prediction\n",
    "\n",
    "‚ÑπÔ∏è **Learn more**:\n",
    "\n",
    "- details about transforms: [**vision-transform.ipynb**](./vision-transform.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = MNIST(MNIST_DIR, train=True, transform=v2.ToImage(), download=False)\n",
    "\n",
    "# log\n",
    "print(f\"len(mnist_dataset)        : {len(mnist_dataset)}\")\n",
    "print(f\"mnist_dataset[0][0].shape : {mnist_dataset[0][0].shape}\")\n",
    "print(f\"mnist_dataset[0][0].dtype : {mnist_dataset[0][0].dtype}\")\n",
    "print(f\"classes                   : {mnist_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_2_'></a>[Torchaudio Built-in Datasets](#toc0_)\n",
    "\n",
    "- Torchaudio provides several built-in datasets in the `torchaudio.datasets` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesno_dataset = YESNO(YESNO_DIR, download=False)\n",
    "\n",
    "# log\n",
    "print(f\"len(yesno_dataset)        : {len(yesno_dataset)}\")\n",
    "print(f\"yesno_dataset[0][0].shape : {yesno_dataset[0][0].shape}\")\n",
    "print(f\"yesno_dataset[0][0].dtype : {yesno_dataset[0][0].dtype}\")\n",
    "print(f\"sample_rate               : {yesno_dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Custom Datasets](#toc0_)\n",
    "\n",
    "-  A custom dataset is useful when you need to handle non-standard data formats, apply specific preprocessing, or create new dataset structures beyond what built-in datasets provide (e.g., ImageFolder, MNIST).\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `torch.utils.data.Dataset`: [pytorch.org/docs/stable/data.html#torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n",
    "- `torch.utils.data.IterableDataset`: [pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)\n",
    "- `torchvision.datasets.VisionDataset`: [pytorch.org/vision/main/generated/torchvision.datasets.VisionDataset.html](https://pytorch.org/vision/main/generated/torchvision.datasets.VisionDataset.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[Base Class: Dataset](#toc0_)\n",
    "\n",
    "- The `Dataset` class is part of the `torch.utils.data` module and serves as the foundation for **loading data** in PyTorch.\n",
    "- Use `Dataset` as the **parent** class and **implement** two key methods:\n",
    "  - `__len__`: Returns the number of samples in the dataset.\n",
    "  - `__getitem__`: Retrieves a **single** sample from the dataset at a given **index**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, labels: torch.Tensor):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = BaseDataset(X, y)\n",
    "\n",
    "# log\n",
    "print(f\"type(dataset_4) : {type(dataset_4)}\")\n",
    "print(f\"len(dataset_4)  : {len(dataset_4)}\")\n",
    "print(f\"dataset_4[0][0] : {dataset_4[0][0]}\")\n",
    "print(f\"dataset_4[0][1] : {dataset_4[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[Base Class: VisionDataset](#toc0_)\n",
    "\n",
    "- The `VisionDataset` class, part of the `torchvision.datasets` module, extends `Dataset` and provides additional functionality for **image-based datasets**.\n",
    "- It is designed for **loading, processing, and transforming images** efficiently.\n",
    "- Use `VisionDataset` as the **parent** class and **implement** two key methods:\n",
    "  - `__len__`: Returns the number of samples in the dataset.\n",
    "  - `__getitem__`: Retrieves a **single** sample from the dataset at a given **index**.\n",
    "\n",
    "‚úçÔ∏è **Note:**  \n",
    "- `VisionDataset` provides a `loader` function for image reading, making it preferable for vision tasks.\n",
    "- It is the base class for standard datasets like `ImageFolder` and `DatasetFolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_loader(path: Path) -> torch.Tensor:\n",
    "    try:\n",
    "        image = decode_image(path)\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {path}: {e}\")\n",
    "        return torch.zeros((3, 224, 224))  # return a blank image as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(VisionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        loader: Callable,\n",
    "        transform: v2.Compose | None = None,\n",
    "        cache_images: bool = False,\n",
    "    ):\n",
    "        super().__init__(root, transform=transform)\n",
    "\n",
    "        self.root = Path(root)\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "        self.cache_images = cache_images\n",
    "        self.classes = [cls.name for cls in self.root.iterdir() if cls.is_dir()]\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.img_paths: list[tuple[Path, int]] = []\n",
    "\n",
    "        # fetch and store (image_path, class_index) pairs\n",
    "        for cls_name in self.classes:\n",
    "            cls_dir = self.root / cls_name\n",
    "            for img_path in cls_dir.glob(\"*.jpg\"):\n",
    "                self.img_paths.append((img_path, self.class_to_idx[cls_name]))\n",
    "\n",
    "        # cache for storing images in memory\n",
    "        self.image_cache = {}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        img_path, label = self.img_paths[idx]\n",
    "\n",
    "        # check if the image is already cached\n",
    "        if self.cache_images and idx in self.image_cache:\n",
    "            # fetch image from cache\n",
    "            image = self.image_cache[idx]\n",
    "        else:\n",
    "            # load image\n",
    "            image = self.loader(img_path)\n",
    "\n",
    "            # cache the image if caching is enabled\n",
    "            if self.cache_images:\n",
    "                self.image_cache[idx] = image\n",
    "\n",
    "        # apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # convert label to tensor of type torch.long\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_3_'></a>[Base Class: VisionDataset's Derived Class](#toc0_)\n",
    "\n",
    "- A derived class from an existing dataset (e.g., `MNIST` from `torchvision.datasets`) extends a predefined dataset while retaining its functionalities.\n",
    "- Use an existing dataset as the **parent** class and **override** key methods as needed:\n",
    "  - `__getitem__`: Modifies the way a **single** sample is retrieved (e.g., changing labels, applying custom transformations).\n",
    "  - Optionally, override `__len__` if the dataset size needs adjustment.\n",
    "- This approach leverages built-in dataset loading while allowing for customization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMNIST(MNIST):\n",
    "    def __init__(self, root: str, train: bool = True, transform=None, target_transform=None, download: bool = True):\n",
    "        super().__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image, label = super().__getitem__(index)\n",
    "        modified_label = label if label != 9 else 0\n",
    "        return image, modified_label\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self) -> str:\n",
    "        return Path(self.root) / self.__class__.__bases__[0].__name__ / \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = CustomMNIST(MNIST_DIR, train=True, transform=None, target_transform=None, download=False)\n",
    "\n",
    "# log\n",
    "print(f\"type(dataset_5) : {type(dataset_5)}\")\n",
    "print(f\"len(dataset_5)  : {len(dataset_5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "num_images = 10\n",
    "_, axs = plt.subplots(\n",
    "    nrows=1, ncols=num_images, figsize=(num_images * 1.5, num_images / (num_images / 2)), layout=\"compressed\"\n",
    ")\n",
    "for i in range(num_images):\n",
    "    axs[i].imshow(dataset_5[i][0], cmap=\"gray\")\n",
    "    axs[i].set(title=dataset_5[i][1])\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Torch DataLoader](#toc0_)\n",
    "\n",
    "- A DataLoader(`torch.utils.data.DataLoader`) is a utility for training and evaluation in deep learning tasks that enables:\n",
    "  - efficient loading datasets\n",
    "  - handling batching\n",
    "  - shuffling\n",
    "  - parallel data loading\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `torch.utils.data.DataLoader`: [pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Strategies for updating weights](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_1_'></a>[Batch Gradient Descent](#toc0_)\n",
    "\n",
    "- Uses the **entire** dataset to compute the **gradient** of the loss function and **update** the **weights**.\n",
    "- **Pros**: Provides a stable convergence.\n",
    "- **Cons**: Can be very slow and computationally expensive for large datasets.\n",
    "\n",
    "üåü **Example**:\n",
    "\n",
    "| #Epoch | batch size | #batch per epoch                    | #iteration per epoch                |\n",
    "|:------:|:----------:|:-----------------------------------:|:-----------------------------------:|\n",
    "| $ 2 $  | $ 150 $    | $ \\lceil\\frac{150}{150}\\rceil = 1 $ | $ \\lceil\\frac{150}{150}\\rceil = 1 $ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = len(dataset_1)\n",
    "dataloader = DataLoader(dataset_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# log\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch+1:0{len(str(epochs))}}/{epochs}\")\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        print(f\"    iteration {i+1}/{ceil(len(dataset_1)/batch_size)}\")\n",
    "        print(f\"        x.shape: {x.shape}\")\n",
    "        print(f\"        y.shape: {y.shape}\")\n",
    "        print(\"    weights are updated.\")\n",
    "    print(f\"model saw the entire dataset.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_2_'></a>[Stochastic Gradient Descent](#toc0_)\n",
    "\n",
    "- the model **updates** the **weights** using only **one data point** at a time.\n",
    "- **Pros**: Faster updates and can escape local minima.\n",
    "- **Cons**: Can be noisy and may not converge as smoothly as batch gradient descent.\n",
    "\n",
    "üåü **Example**:\n",
    "\n",
    "| #Epoch | batch size | #batch per epoch                    | #iteration per epoch                |\n",
    "|:------:|:----------:|:-----------------------------------:|:-----------------------------------:|\n",
    "| $ 2 $  | $ 1 $      | $ \\lceil\\frac{150}{1}\\rceil = 150 $ | $ \\lceil\\frac{150}{1}\\rceil = 150 $ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# log\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch+1:0{len(str(epochs))}}/{epochs}\")\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        if i % 50 == 0 or i == len(X) - 1:\n",
    "            print(f\"    iteration {i+1}/{ceil(len(dataset_1)/batch_size)}\")\n",
    "            print(f\"        x.shape: {x.shape}\")\n",
    "            print(f\"        y.shape: {y.shape}\")\n",
    "            print(\"    weights are updated.\")\n",
    "    print(f\"model saw the entire dataset.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_3_'></a>[Mini-Batch Gradient Descent](#toc0_)\n",
    "\n",
    "- the model updates its weights after processing a small batch of 'm' samples from the training dataset.\n",
    "- this method combines the advantages of both SGD and Batch Gradient Descent by providing a balance between efficiency and stability during training.\n",
    "\n",
    "üåü **Example**:\n",
    "\n",
    "| #Epoch | batch size | #batch                             | #iteration per epoch               |\n",
    "|:------:|:----------:|:----------------------------------:|:----------------------------------:|\n",
    "| $ 2 $  | $ 32 $     | $ \\lceil\\frac{150}{32}\\rceil = 5 $ | $ \\lceil\\frac{150}{32}\\rceil = 5 $ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# log\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch {epoch+1:0{len(str(epochs))}}/{epochs}\")\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        print(f\"    iteration {i+1}/{ceil(len(dataset_1)/batch_size)}\")\n",
    "        print(f\"        x.shape: {x.shape}\")\n",
    "        print(f\"        y.shape: {y.shape}\")\n",
    "        print(\"    weights are updated.\")\n",
    "    print(f\"model saw the entire dataset.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Torch DataParallel](#toc0_)\n",
    "\n",
    "- A DataParallel(`torch.nn.DataParallel`), enables data-level parallelism by distributing input data across multiple GPUs and aggregating the results.\n",
    "  - Splits the input batch across GPUs.\n",
    "  - Each GPU processes its split independently.\n",
    "  - Results from all GPUs are combined on the primary (default) GPU.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `nn.DataParallel`: [pytorch.org/docs/stable/generated/torch.nn.DataParallel.html](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)\n",
    "\n",
    "üìö **Tutorials**:\n",
    "\n",
    "- Data Parallelism: [pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 30\n",
    "dataloader = DataLoader(dataset_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        print(f\"  inside model -> input size: {input.size()} | output size: {output.size()}\")\n",
    "        return output\n",
    "\n",
    "\n",
    "model = CustomModel(4, 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    # [32, xxx] -> [15, ...], [15, ...] on 2 GPUs\n",
    "    # [32, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    # ...\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log\n",
    "print(f\"number of GPUs: {torch.cuda.device_count()}\")\n",
    "for epoch in range(epochs):\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "pytorch-workshop-U_zYfVTd-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "origin_repo": "https://github.com/mr-pylin/pytorch-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
